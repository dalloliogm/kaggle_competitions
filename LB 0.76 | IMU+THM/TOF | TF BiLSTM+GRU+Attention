{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"},{"sourceId":242954653,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This solution was developed by building upon the foundational work of two key contributors:**\n\n- **CMI25 | IMU+THM/TOF |TF BiLSTM+GRU+Attention|LB.75** by [yukiZ](https://www.kaggle.com/code/hideyukizushi/cmi25-imu-thm-tof-tf-bilstm-gru-attention-lb-75)\n- **imu-tof** by [Erich von Mainstein](https://www.kaggle.com/code/vonmainstein/imu-tof)\n\n**Special recognition for the exceptional exploratory data analysis:**\n\n- **Sensor PulseðŸ§ | Viz & EDA for BFRB Detection** by [Tarun Mishra](https://www.kaggle.com/code/tarundirector/sensor-pulse-viz-eda-for-bfrb-detection)\n\n### Implemented Enhancements\n\n#### 1. IMU Signal Processing Optimization (Core Improvement)\n\n**Implemented Features:**\n- Derived kinematic metrics:\n  - Acceleration vector magnitude (`acc_mag`)\n  - Scalar rotation angle (`rot_angle`)\n  - Acceleration derivative (`acc_mag_jerk`)\n  - Angular velocity (`rot_angle_vel`)\n- Initial learning rate reduction from `1e-3` to `5e-4`\n- Implemented cosine decay scheduling:\n  - Parameters: `first_decay_steps = 15 Ã— steps_per_epoch`\n\n**Unsuccessful Experiments (Reverted):**\n- Further LR Reduction: `LR_INIT = 2-e4` worsened score.\n- Dropout Rate Adjustments: No improvement over baseline dropout.\n- TOF/THEM Branch Architecture Change: Replacing simpler CNNs with `residual_se_cnn_blocks` was detrimental.\n\n**If you support the notebook, please upvoteâ€”Iâ€™d really appreciate it!**  \n**Also, if you have any questions, feel free to askâ€”Iâ€™ll be happy to answer. ðŸ˜Š**","metadata":{}},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Importing the necessary Libraries**","metadata":{}},{"cell_type":"code","source":"import os, json, joblib, numpy as np, pandas as pd\nfrom pathlib import Path\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n    Lambda, Concatenate, GRU, GaussianNoise\n)\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport polars as pl\nfrom sklearn.model_selection import StratifiedGroupKFold\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:41:49.607009Z","iopub.execute_input":"2025-06-14T12:41:49.607177Z","iopub.status.idle":"2025-06-14T12:42:05.490391Z","shell.execute_reply.started":"2025-06-14T12:41:49.607162Z","shell.execute_reply":"2025-06-14T12:42:05.489591Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Fix Seed**","metadata":{}},{"cell_type":"code","source":"import random\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    tf.experimental.numpy.random.seed(seed)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nseed_everything(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:42:05.49126Z","iopub.execute_input":"2025-06-14T12:42:05.491682Z","iopub.status.idle":"2025-06-14T12:42:05.496221Z","shell.execute_reply.started":"2025-06-14T12:42:05.491663Z","shell.execute_reply":"2025-06-14T12:42:05.495491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Configuration**","metadata":{}},{"cell_type":"code","source":"# (Competition metric will only be imported when TRAINing)\nTRAIN = True                     # â† set to True when you want to train\nRAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\nPRETRAINED_DIR = Path(\"/kaggle/input/pretrained-model\")  # used when TRAIN=False\nEXPORT_DIR = Path(\"./\")                                    # artefacts will be saved here\nBATCH_SIZE = 64\nPAD_PERCENTILE = 95\nLR_INIT = 5e-4\nWD = 3e-3\nMIXUP_ALPHA = 0.4\nEPOCHS = 160\nPATIENCE = 40\n\n\nprint(\"â–¶ imports ready Â· tensorflow\", tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:42:05.498111Z","iopub.execute_input":"2025-06-14T12:42:05.49858Z","iopub.status.idle":"2025-06-14T12:42:05.52054Z","shell.execute_reply.started":"2025-06-14T12:42:05.498562Z","shell.execute_reply":"2025-06-14T12:42:05.519841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Utility Functions**","metadata":{}},{"cell_type":"code","source":"#Tensor Manipulations\ndef time_sum(x):\n    return K.sum(x, axis=1)\n\ndef squeeze_last_axis(x):\n    return tf.squeeze(x, axis=-1)\n\ndef expand_last_axis(x):\n    return tf.expand_dims(x, axis=-1)\n\ndef se_block(x, reduction=8):\n    ch = x.shape[-1]\n    se = GlobalAveragePooling1D()(x)\n    se = Dense(ch // reduction, activation='relu')(se)\n    se = Dense(ch, activation='sigmoid')(se)\n    se = Reshape((1, ch))(se)\n    return Multiply()([x, se])\n\n# Residual CNN Block with SE\ndef residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n    shortcut = x\n    for _ in range(2):\n        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n                   kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n    x = se_block(x)\n    if shortcut.shape[-1] != filters:\n        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n                          kernel_regularizer=l2(wd))(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n    x = add([x, shortcut])\n    x = Activation('relu')(x)\n    x = MaxPooling1D(pool_size)(x)\n    x = Dropout(drop)(x)\n    return x\n\ndef attention_layer(inputs):\n    score = Dense(1, activation='tanh')(inputs)\n    score = Lambda(squeeze_last_axis)(score)\n    weights = Activation('softmax')(score)\n    weights = Lambda(expand_last_axis)(weights)\n    context = Multiply()([inputs, weights])\n    context = Lambda(time_sum)(context)\n    return context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:42:05.521107Z","iopub.execute_input":"2025-06-14T12:42:05.521282Z","iopub.status.idle":"2025-06-14T12:42:05.53231Z","shell.execute_reply.started":"2025-06-14T12:42:05.521267Z","shell.execute_reply":"2025-06-14T12:42:05.531639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Data Helpers**","metadata":{}},{"cell_type":"code","source":"# Normalizes and cleans the time series sequence. \n\ndef preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n    return scaler.transform(mat).astype('float32')\n\n# MixUp the data argumentation in order to regularize the neural network. \n\nclass MixupGenerator(Sequence):\n    def __init__(self, X, y, batch_size, alpha=0.2):\n        self.X, self.y = X, y\n        self.batch = batch_size\n        self.alpha = alpha\n        self.indices = np.arange(len(X))\n    def __len__(self):\n        return int(np.ceil(len(self.X) / self.batch))\n    def __getitem__(self, i):\n        idx = self.indices[i*self.batch:(i+1)*self.batch]\n        Xb, yb = self.X[idx], self.y[idx]\n        lam = np.random.beta(self.alpha, self.alpha)\n        perm = np.random.permutation(len(Xb))\n        X_mix = lam * Xb + (1-lam) * Xb[perm]\n        y_mix = lam * yb + (1-lam) * yb[perm]\n        return X_mix, y_mix\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:42:05.533033Z","iopub.execute_input":"2025-06-14T12:42:05.533251Z","iopub.status.idle":"2025-06-14T12:42:05.552186Z","shell.execute_reply.started":"2025-06-14T12:42:05.533227Z","shell.execute_reply":"2025-06-14T12:42:05.551451Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Model Definition - Two Branch Architecture**","metadata":{}},{"cell_type":"code","source":"def build_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n\n    # IMU deep branch\n    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n\n    # TOF/Thermal lighter branch\n    x2 = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n    x2 = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2)\n    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n\n    merged = Concatenate()([x1, x2])\n\n    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n    xc = GaussianNoise(0.09)(merged)\n    xc = Dense(16, activation='elu')(xc)\n    \n    x = Concatenate()([xa, xb, xc])\n    x = Dropout(0.4)(x)\n    x = attention_layer(x)\n\n    for units, drop in [(256, 0.5), (128, 0.3)]:\n        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x); x = Activation('relu')(x)\n        x = Dropout(drop)(x)\n\n    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n    return Model(inp, out)\n\ntmp_model = build_two_branch_model(127,7,325,18)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:42:05.552907Z","iopub.execute_input":"2025-06-14T12:42:05.553097Z","iopub.status.idle":"2025-06-14T12:42:08.493646Z","shell.execute_reply.started":"2025-06-14T12:42:05.553083Z","shell.execute_reply":"2025-06-14T12:42:08.493117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Training / Inference Pipeline**","metadata":{}},{"cell_type":"code","source":"if TRAIN:\n    print(\"â–¶ TRAIN MODE â€“ loading dataset â€¦\")\n    df = pd.read_csv(RAW_DIR / \"train.csv\")\n\n    train_dem_df = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n    df_for_groups = pd.merge(df.copy(), train_dem_df, on='subject', how='left')\n\n    le = LabelEncoder()\n    df['gesture_int'] = le.fit_transform(df['gesture'])\n    np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n    gesture_classes = le.classes_\n\n    print(\"  Calculating engineered IMU features (magnitude, angle)...\")\n    df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n    df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n    \n    print(\"  Calculating engineered IMU derivatives (jerk, angular velocity)...\")\n    df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n    df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n\n    meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n                 'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n\n    imu_cols = [c for c in df.columns if c.startswith('acc_') and c not in ['acc_mag', 'acc_mag_jerk']]\n    imu_cols.extend([c for c in df.columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n    imu_cols.extend(['acc_mag', 'rot_angle', 'acc_mag_jerk', 'rot_angle_vel'])\n\n    thm_cols_original = [c for c in df.columns if c.startswith('thm_')]\n    \n    tof_aggregated_cols_template = []\n    for i in range(1, 6):\n        tof_aggregated_cols_template.extend([f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'])\n\n    final_feature_cols = imu_cols + thm_cols_original + tof_aggregated_cols_template\n    imu_dim_final = len(imu_cols)\n    tof_thm_aggregated_dim_final = len(thm_cols_original) + len(tof_aggregated_cols_template)\n    \n    print(f\"  IMU (incl. engineered & derivatives) {imu_dim_final} | THM + Aggregated TOF {tof_thm_aggregated_dim_final} | total {len(final_feature_cols)} features\")\n    np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))\n\n    print(\"  Building sequences with aggregated TOF and preparing data for scaler...\")\n    seq_gp = df.groupby('sequence_id') \n    \n    all_steps_for_scaler_list = []\n    X_list_unscaled, y_list_int_for_stratify, lens = [], [], [] \n\n    for seq_id, seq_df_orig in seq_gp:\n        seq_df = seq_df_orig.copy()\n\n        for i in range(1, 6):\n            pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n            tof_sensor_data = seq_df[pixel_cols_tof].replace(-1, np.nan)\n            seq_df[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n            seq_df[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n            seq_df[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n            seq_df[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n        \n        mat_unscaled = seq_df[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n        \n        all_steps_for_scaler_list.append(mat_unscaled)\n        X_list_unscaled.append(mat_unscaled)\n        y_list_int_for_stratify.append(seq_df['gesture_int'].iloc[0])\n        lens.append(len(mat_unscaled))\n\n    print(\"  Fitting StandardScaler...\")\n    all_steps_concatenated = np.concatenate(all_steps_for_scaler_list, axis=0)\n    scaler = StandardScaler().fit(all_steps_concatenated)\n    joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n    del all_steps_for_scaler_list, all_steps_concatenated\n\n    print(\"  Scaling and padding sequences...\")\n    X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n    del X_list_unscaled\n\n    pad_len = int(np.percentile(lens, PAD_PERCENTILE))\n    np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n    \n    X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n    del X_scaled_list\n    \n    y_int_for_stratify = np.array(y_list_int_for_stratify)\n    y = to_categorical(y_int_for_stratify, num_classes=len(le.classes_))\n\n    print(\"  Splitting data and preparing for training...\")\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=82, stratify=y_int_for_stratify)\n\n    cw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_int_for_stratify)\n    class_weight = dict(enumerate(cw_vals))\n\n    model = build_two_branch_model(pad_len, imu_dim_final, tof_thm_aggregated_dim_final, len(le.classes_), wd=WD)\n    \n    steps = len(X_tr) // BATCH_SIZE\n    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(5e-4, first_decay_steps=15 * steps) \n    \n    model.compile(optimizer=Adam(lr_sched),\n                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n                  metrics=['accuracy'])\n\n    train_gen = MixupGenerator(X_tr, y_tr, batch_size=BATCH_SIZE, alpha=MIXUP_ALPHA)\n    cb = EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1, monitor='val_accuracy', mode='max')\n    \n    print(\"  Starting model training...\")\n    model.fit(train_gen, epochs=EPOCHS, validation_data=(X_val, y_val),\n              class_weight=class_weight, callbacks=[cb], verbose=1)\n\n    model.save(EXPORT_DIR / \"gesture_two_branch_mixup.h5\")\n    print(\"âœ” Training done â€“ artefacts saved in\", EXPORT_DIR)\n\n    from cmi_2025_metric_copy_for_import import CompetitionMetric\n    preds_val = model.predict(X_val).argmax(1)\n    true_val_int  = y_val.argmax(1)\n    \n    h_f1 = CompetitionMetric().calculate_hierarchical_f1(\n        pd.DataFrame({'gesture': le.classes_[true_val_int]}),\n        pd.DataFrame({'gesture': le.classes_[preds_val]}))\n    print(\"Holdâ€‘out Hâ€‘F1 =\", round(h_f1, 4))\n\nelse:\n    print(\"â–¶ INFERENCE MODE â€“ loading artefacts from\", PRETRAINED_DIR)\n    final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n    pad_len        = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n    scaler         = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n\n    temp_imu_cols = [c for c in final_feature_cols if c.startswith('acc_') or c.startswith('rot_')]\n    imu_dim_final = len(temp_imu_cols)\n    tof_thm_aggregated_dim_final = len(final_feature_cols) - imu_dim_final\n\n    custom_objs = {\n        'time_sum': time_sum,\n        'squeeze_last_axis': squeeze_last_axis,\n        'expand_last_axis': expand_last_axis,\n        'se_block': se_block,\n        'residual_se_cnn_block': residual_se_cnn_block,\n        'attention_layer': attention_layer,\n    }\n    model = load_model(PRETRAINED_DIR / \"gesture_two_branch_mixup.h5\",\n                       compile=False, custom_objects=custom_objs)\n    print(\"  Model, scaler, feature_cols, pad_len loaded â€“ ready for evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:42:08.494381Z","iopub.execute_input":"2025-06-14T12:42:08.49456Z","iopub.status.idle":"2025-06-14T12:55:14.76622Z","shell.execute_reply.started":"2025-06-14T12:42:08.494545Z","shell.execute_reply":"2025-06-14T12:55:14.76556Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Predict**","metadata":{}},{"cell_type":"code","source":"def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    df_seq = sequence.to_pandas()\n\n    df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n    df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n    df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n    df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n    \n    for i in range(1, 6): \n        pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n        tof_sensor_data = df_seq[pixel_cols_tof].replace(-1, np.nan)\n        \n        df_seq[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n        df_seq[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n        df_seq[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n        df_seq[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n        \n    df_seq_reordered = pd.DataFrame(columns=final_feature_cols)\n    for col in final_feature_cols:\n        if col in df_seq.columns:\n            df_seq_reordered[col] = df_seq[col]\n\n    mat_unscaled = df_seq_reordered.ffill().bfill().fillna(0).values.astype('float32')\n    \n    mat_scaled = scaler.transform(mat_unscaled)\n    \n    pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n    \n    idx = int(model.predict(pad_input, verbose=0).argmax(1)[0])\n    return str(gesture_classes[idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:55:14.767101Z","iopub.execute_input":"2025-06-14T12:55:14.767365Z","iopub.status.idle":"2025-06-14T12:55:14.775856Z","shell.execute_reply.started":"2025-06-14T12:55:14.767345Z","shell.execute_reply":"2025-06-14T12:55:14.775081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ã€‹ã€‹ã€‹**Submit Inference server**","metadata":{}},{"cell_type":"code","source":"import kaggle_evaluation.cmi_inference_server\ninference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:55:14.777453Z","iopub.execute_input":"2025-06-14T12:55:14.777634Z","iopub.status.idle":"2025-06-14T12:55:16.607366Z","shell.execute_reply.started":"2025-06-14T12:55:14.77762Z","shell.execute_reply":"2025-06-14T12:55:16.60677Z"}},"outputs":[],"execution_count":null}]}