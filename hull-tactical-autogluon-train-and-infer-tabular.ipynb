{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ebbc3f",
   "metadata": {
    "papermill": {
     "duration": 0.00241,
     "end_time": "2025-12-12T18:06:51.501837",
     "exception": false,
     "start_time": "2025-12-12T18:06:51.499427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ§  Hull Tactical Market Prediction â€” AutoGluon Baseline\n",
    "\n",
    "This notebook builds a baseline model for the [**Hull Tactical Market Prediction**](https://www.kaggle.com/competitions/hull-tactical-market-prediction) competition using **AutoGluon Tabular**. The goal is to predict trading positions that maximize a Sharpe-like performance metric.  \n",
    "\n",
    "## Overview\n",
    "- **Task:** Predict next-period trading positions (long / flat) using engineered financial features.\n",
    "- **Approach:** Train an AutoGluon model on historical data to predict *forward returns*, then post-process those predictions into positions for scoring and submission.\n",
    "- **Metric:** Custom approximation of the competitionâ€™s adjusted Sharpe ratio, which penalizes volatility and underperformance.\n",
    "- **Post-processing:** A unified `post_process_signal()` function ensures parity between local validation and leaderboard logic by converting model predictions into bounded investment positions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b52778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:06:51.506952Z",
     "iopub.status.busy": "2025-12-12T18:06:51.506603Z",
     "iopub.status.idle": "2025-12-12T18:09:20.854954Z",
     "shell.execute_reply": "2025-12-12T18:09:20.853615Z"
    },
    "papermill": {
     "duration": 149.353365,
     "end_time": "2025-12-12T18:09:20.857342",
     "exception": false,
     "start_time": "2025-12-12T18:06:51.503977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "WHEELS = Path(\"/kaggle/input/autogluon-1-4-0-offline\")  # <- your dataset\n",
    "\n",
    "!pip install --no-index --quiet --find-links=\"{WHEELS}\" \\\n",
    "  \"torch==2.5.1\" \"torchvision==0.20.1\" \"torchaudio==2.5.1\" \"bitsandbytes>=0.46.1\" \"mlforecast==0.14.0\" \"optuna==4.3.0\"\n",
    "\n",
    "!pip install --no-index --quiet --find-links=\"{WHEELS}\" \\\n",
    "    \"autogluon.tabular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349ba34a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:09:20.863453Z",
     "iopub.status.busy": "2025-12-12T18:09:20.862826Z",
     "iopub.status.idle": "2025-12-12T18:09:26.599818Z",
     "shell.execute_reply": "2025-12-12T18:09:26.598870Z"
    },
    "papermill": {
     "duration": 5.742199,
     "end_time": "2025-12-12T18:09:26.601572",
     "exception": false,
     "start_time": "2025-12-12T18:09:20.859373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# === Metric constants (define to avoid NameError in custom scorer) ===\n",
    "ALPHA_FOR_SCORER = 0.600132\n",
    "TAU_ABS_FOR_SCORER = 9.43717e-05\n",
    "MIN_INVESTMENT, MAX_INVESTMENT = 0.0, 2.0\n",
    "TRADING_DAYS = 252\n",
    "\n",
    "\n",
    "# === Single source of truth for post-processing predictions ===\n",
    "# Use this both in the custom scorer and at inference to ensure parity with leaderboard logic.\n",
    "# Current behavior: long-only; open a position of size ALPHA when prediction > TAU; else 0; then clip to [MIN, MAX].\n",
    "# If you later want symmetric long/short, add a flag and branch here.\n",
    "\n",
    "def post_process_signal(y_pred,\n",
    "                        *,\n",
    "                        tau: float = TAU_ABS_FOR_SCORER,\n",
    "                        alpha: float = ALPHA_FOR_SCORER,\n",
    "                        min_investment: float = MIN_INVESTMENT,\n",
    "                        max_investment: float = MAX_INVESTMENT):\n",
    "    sig = np.asarray(y_pred, dtype=float).ravel()\n",
    "    pos = np.where(sig > tau, alpha, 0.0)\n",
    "    return np.clip(pos, min_investment, max_investment)\n",
    "\n",
    "\n",
    "DATA_PATH=\"/kaggle/input/hull-tactical-market-prediction/\"\n",
    "\n",
    "train = pd.read_csv(f\"{DATA_PATH}train.csv\")\n",
    "\n",
    "TARGET = \"forward_returns\"\n",
    "# simple guard to surface a clear error if the label is missing\n",
    "if TARGET not in train.columns:\n",
    "    raise ValueError(f\"Expected target column '{TARGET}' in train.csv; found: {list(train.columns)}\")\n",
    "\n",
    "DROP_IF_EXISTS = [\"row_id\", \"id\", \"risk_free_rate\", \"market_forward_excess_returns\"]\n",
    "use_cols = [c for c in train.columns if c not in DROP_IF_EXISTS]\n",
    "train = train[use_cols]\n",
    "\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"Return a single post-processed position for a **single-row** Polars DataFrame.\n",
    "\n",
    "    This mirrors the scorer's post-processing to keep CV â†” leaderboard parity.\n",
    "    - Drops leak-prone / non-feature columns if present.\n",
    "    - Aligns columns to the predictor's feature set.\n",
    "    - Converts Polars â†’ Pandas for AutoGluon.\n",
    "    \"\"\"\n",
    "    if not isinstance(test, pl.DataFrame):\n",
    "        raise TypeError(\"predict(test): expected a Polars DataFrame input\")\n",
    "\n",
    "    if test.height != 1:\n",
    "        raise ValueError(f\"predict(test): expected a single-row Polars DataFrame, got {test.height} rows\")\n",
    "\n",
    "    # Drop known non-feature columns if present\n",
    "    drop_cols = [c for c in DROP_IF_EXISTS if c in test.columns]\n",
    "    test_pl = test.drop(drop_cols) if drop_cols else test\n",
    "\n",
    "    # Ensure target isn't present at inference\n",
    "    if TARGET in test_pl.columns:\n",
    "        test_pl = test_pl.drop(TARGET)\n",
    "\n",
    "    # Convert to pandas for AutoGluon\n",
    "    test_pd = test_pl.to_pandas()\n",
    "\n",
    "    # Align to the model's feature set (adds missing as 0, drops extras)\n",
    "    feats = predictor.feature_metadata.get_features() if 'predictor' in globals() else [c for c in train.columns if c != TARGET]\n",
    "    #test_pd = test_pd.reindex(columns=feats, fill_value=0)\n",
    "\n",
    "    # Raw prediction â†’ post-processed trading position\n",
    "    raw = predictor.predict(test_pd)\n",
    "    pos = post_process_signal(raw)\n",
    "    return float(np.asarray(pos).ravel()[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8720d19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:09:26.606613Z",
     "iopub.status.busy": "2025-12-12T18:09:26.606319Z",
     "iopub.status.idle": "2025-12-12T18:18:33.741010Z",
     "shell.execute_reply": "2025-12-12T18:18:33.740014Z"
    },
    "papermill": {
     "duration": 547.139159,
     "end_time": "2025-12-12T18:18:33.742783",
     "exception": false,
     "start_time": "2025-12-12T18:09:26.603624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251212_180926\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Sat Sep 27 10:16:09 UTC 2025\n",
      "CPU Count:          4\n",
      "Memory Avail:       30.23 GB / 31.35 GB (96.4%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['high_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 90s of the 360.0s of remaining time (25%).\n",
      "/usr/local/lib/python3.11/dist-packages/autogluon/tabular/predictor/predictor.py:1444: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.49.2 detected. 2.10.0 <= ray < 2.45.0 is required. You can use pip to install certain version of ray `pip install \"ray>=2.10.0,<2.45.0\"`')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20251212_180926/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 83s\n",
      "AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20251212_180926/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    8042\n",
      "Train Data Columns: 95\n",
      "Label Column:       forward_returns\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    30898.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.83 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 9 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['D2']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['D2']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 85 | ['E1', 'E10', 'E11', 'E12', 'E13', ...]\n",
      "\t\t('int', [])   :  9 | ['date_id', 'D1', 'D3', 'D4', 'D5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 85 | ['E1', 'E10', 'E11', 'E12', 'E13', ...]\n",
      "\t\t('int', [])       :  1 | ['date_id']\n",
      "\t\t('int', ['bool']) :  8 | ['D1', 'D3', 'D4', 'D5', 'D6', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t94 features in original data used to generate 94 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.34 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 54.99s of the 82.49s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray==2.49.2 detected. 2.10.0 <= ray < 2.45.0 is required. You can use pip to install certain version of ray `pip install \"ray>=2.10.0,<2.45.0\"`\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-0.0106\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.95s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 32.50s of the 60.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-0.0106\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 12.60s of the 40.10s of remaining time.\n",
      "\t-0.0109\t = Validation score   (-root_mean_squared_error)\n",
      "\t208.87s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 82.50s of the -171.00s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.444, 'LightGBM_BAG_L1': 0.444, 'RandomForestMSE_BAG_L1': 0.111}\n",
      "\t-0.0105\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 82.50s of the -171.10s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.444, 'LightGBM_BAG_L1': 0.444, 'RandomForestMSE_BAG_L1': 0.111}\n",
      "\t-0.0105\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 254.13s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7727.3 rows/s (1006 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t0.44s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t0.45s\t = Training   runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t208.87s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.444, 'LightGBM_BAG_L1': 0.444, 'RandomForestMSE_BAG_L1': 0.111}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.444, 'LightGBM_BAG_L1': 0.444, 'RandomForestMSE_BAG_L1': 0.111}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 2.41s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20251212_180926/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "                         model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L3_FULL      -0.010264  -0.010543  root_mean_squared_error        0.498153            NaN  209.774824                 0.002616                     NaN           0.009646            3       True          5\n",
      "1     WeightedEnsemble_L2_FULL      -0.010264  -0.010543  root_mean_squared_error        0.498833            NaN  209.778741                 0.003296                     NaN           0.013564            2       True          4\n",
      "2         LightGBM_BAG_L1_FULL      -0.010268  -0.010555  root_mean_squared_error        0.002618            NaN    0.453602                 0.002618                     NaN           0.453602            1       True          2\n",
      "3       LightGBMXT_BAG_L1_FULL      -0.010272  -0.010552  root_mean_squared_error        0.003246            NaN    0.440589                 0.003246                     NaN           0.440589            1       True          1\n",
      "4  RandomForestMSE_BAG_L1_FULL      -0.010592  -0.010877  root_mean_squared_error        0.489672       0.799004  208.870986                 0.489672                0.799004         208.870986            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t266s\t = DyStack   runtime |\t94s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 94s\n",
      "AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20251212_180926\"\n",
      "Train Data Rows:    9048\n",
      "Train Data Columns: 95\n",
      "Label Column:       forward_returns\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    30602.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 9 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['D2']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['D2']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 85 | ['E1', 'E10', 'E11', 'E12', 'E13', ...]\n",
      "\t\t('int', [])   :  9 | ['date_id', 'D1', 'D3', 'D4', 'D5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 85 | ['E1', 'E10', 'E11', 'E12', 'E13', ...]\n",
      "\t\t('int', [])       :  1 | ['date_id']\n",
      "\t\t('int', ['bool']) :  8 | ['D1', 'D3', 'D4', 'D5', 'D6', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t94 features in original data used to generate 94 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 62.65s of the 93.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-0.0105\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.96s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 45.57s of the 76.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-0.0105\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.41s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 25.06s of the 56.40s of remaining time.\n",
      "\t-0.0109\t = Validation score   (-root_mean_squared_error)\n",
      "\t238.62s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 94.00s of the -183.56s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.75, 'LightGBM_BAG_L1': 0.25}\n",
      "\t-0.0105\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 94.00s of the -183.66s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.75, 'LightGBM_BAG_L1': 0.25}\n",
      "\t-0.0105\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 278.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 27774.6 rows/s (1131 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t0.58s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t0.44s\t = Training   runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t238.62s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.75, 'LightGBM_BAG_L1': 0.25}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.75, 'LightGBM_BAG_L1': 0.25}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 2.14s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20251212_180926\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x7ed2e1c36550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=TARGET,\n",
    "    eval_metric=\"rmse\",\n",
    "    problem_type=\"regression\"  # predicting returns is typically regression\n",
    ")\n",
    "\n",
    "predictor.fit(\n",
    "    train_data=train,\n",
    "    presets=\"high_quality\",  # good speed/quality tradeoff to start\n",
    "    time_limit= 60 * 60 * 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c230a90d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:18:33.769797Z",
     "iopub.status.busy": "2025-12-12T18:18:33.768484Z",
     "iopub.status.idle": "2025-12-12T18:18:34.545180Z",
     "shell.execute_reply": "2025-12-12T18:18:34.544298Z"
    },
    "papermill": {
     "duration": 0.792179,
     "end_time": "2025-12-12T18:18:34.547121",
     "exception": false,
     "start_time": "2025-12-12T18:18:33.754942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kaggle_evaluation.default_inference_server as kis\n",
    "import os\n",
    "\n",
    "# ---------- KAGGLE SERVER BOOTSTRAP ----------\n",
    "inference_server = kis.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14861981,
     "sourceId": 111543,
     "sourceType": "competition"
    },
    {
     "datasetId": 8353464,
     "sourceId": 13183252,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 710.677472,
   "end_time": "2025-12-12T18:18:37.504793",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-12T18:06:46.827321",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
