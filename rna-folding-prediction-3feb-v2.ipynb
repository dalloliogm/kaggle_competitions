{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b1b814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T20:07:48.841831Z",
     "iopub.status.busy": "2026-02-08T20:07:48.841560Z",
     "iopub.status.idle": "2026-02-08T20:07:55.594950Z",
     "shell.execute_reply": "2026-02-08T20:07:55.594009Z"
    },
    "papermill": {
     "duration": 6.758094,
     "end_time": "2026-02-08T20:07:55.596741",
     "exception": false,
     "start_time": "2026-02-08T20:07:48.838647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/biopython-cp312/biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython==1.86) (2.0.2)\r\n",
      "Installing collected packages: biopython\r\n",
      "Successfully installed biopython-1.86\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index /kaggle/input/biopython-cp312/biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bd6c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T20:07:55.601683Z",
     "iopub.status.busy": "2026-02-08T20:07:55.601450Z",
     "iopub.status.idle": "2026-02-08T20:10:29.235307Z",
     "shell.execute_reply": "2026-02-08T20:10:29.234443Z"
    },
    "papermill": {
     "duration": 153.638419,
     "end_time": "2026-02-08T20:10:29.236842",
     "exception": false,
     "start_time": "2026-02-08T20:07:55.598423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 | 0.0s\n",
      "Processing 10 | 117.7s\n",
      "Processing 20 | 120.5s\n",
      "submission.csv! saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import os, sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/kaggle/input/stanford-rna-3d-folding-2/'\n",
    "train_seqs = pd.read_csv(DATA_PATH + 'train_sequences.csv')\n",
    "test_seqs = pd.read_csv(DATA_PATH + 'test_sequences.csv')\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train_labels.csv')\n",
    "\n",
    "sys.path.append(os.path.join(DATA_PATH, \"extra\"))\n",
    "\n",
    "# --- Robust import for Kaggle's extra/parse_fasta_py.py (it may miss typing imports) ---\n",
    "try:\n",
    "    import typing as _typing\n",
    "    import builtins as _builtins\n",
    "\n",
    "    # Make these names available during module import-time annotation evaluation\n",
    "    _builtins.Dict  = getattr(_typing, \"Dict\")\n",
    "    _builtins.Tuple = getattr(_typing, \"Tuple\")\n",
    "    _builtins.List  = getattr(_typing, \"List\")\n",
    "\n",
    "    from parse_fasta_py import parse_fasta as _parse_fasta_raw\n",
    "\n",
    "    # Normalize output to: {chain_id: sequence_string}\n",
    "    def parse_fasta(fasta_content: str):\n",
    "        d = _parse_fasta_raw(fasta_content)\n",
    "        out = {}\n",
    "        for k, v in d.items():\n",
    "            # some variants return (sequence, headers/lines) or similar\n",
    "            out[k] = v[0] if isinstance(v, tuple) else v\n",
    "        return out\n",
    "\n",
    "except Exception:\n",
    "    # Fallback FASTA parser: {chain_id: sequence_string}\n",
    "    def parse_fasta(fasta_content: str):\n",
    "        out = {}\n",
    "        cur = None\n",
    "        seq_parts = []\n",
    "        for line in str(fasta_content).splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                if cur is not None:\n",
    "                    out[cur] = \"\".join(seq_parts)\n",
    "                header = line[1:]\n",
    "                # First token is usually chain id in this dataset\n",
    "                cur = header.split()[0]\n",
    "                seq_parts = []\n",
    "            else:\n",
    "                seq_parts.append(line.replace(\" \", \"\"))\n",
    "        if cur is not None:\n",
    "            out[cur] = \"\".join(seq_parts)\n",
    "        return out\n",
    "\n",
    "def parse_stoichiometry(stoich: str):\n",
    "    if pd.isna(stoich) or str(stoich).strip() == \"\":\n",
    "        return []\n",
    "    out = []\n",
    "    for part in str(stoich).split(';'):\n",
    "        ch, cnt = part.split(':')\n",
    "        out.append((ch.strip(), int(cnt)))\n",
    "    return out\n",
    "\n",
    "def get_chain_segments(row):\n",
    "    \"\"\"\n",
    "    Returns list of (start,end) segments in row['sequence'] corresponding to chain copies in stoichiometry order.\n",
    "    Falls back to single segment if parsing fails.\n",
    "    \"\"\"\n",
    "    seq = row['sequence']\n",
    "    stoich = row.get('stoichiometry', '')\n",
    "    all_seq = row.get('all_sequences', '')\n",
    "\n",
    "    if pd.isna(stoich) or pd.isna(all_seq) or str(stoich).strip()==\"\" or str(all_seq).strip()==\"\":\n",
    "        return [(0, len(seq))]\n",
    "\n",
    "    try:\n",
    "        chain_dict = parse_fasta(all_seq)  # dict: chain_id -> sequence\n",
    "        order = parse_stoichiometry(stoich)\n",
    "        segs = []\n",
    "        pos = 0\n",
    "        for ch, cnt in order:\n",
    "            base = chain_dict.get(ch)\n",
    "            if base is None:\n",
    "                return [(0, len(seq))]\n",
    "            for _ in range(cnt):\n",
    "                L = len(base)\n",
    "                segs.append((pos, pos + L))\n",
    "                pos += L\n",
    "        if pos != len(seq):\n",
    "            return [(0, len(seq))]\n",
    "        return segs\n",
    "    except Exception:\n",
    "        return [(0, len(seq))]\n",
    "\n",
    "def build_segments_map(df):\n",
    "    seg_map = {}\n",
    "    stoich_map = {}\n",
    "    for _, r in df.iterrows():\n",
    "        tid = r['target_id']\n",
    "        seg_map[tid] = get_chain_segments(r)\n",
    "        stoich_map[tid] = str(r.get('stoichiometry', '') if not pd.isna(r.get('stoichiometry', '')) else '')\n",
    "    return seg_map, stoich_map\n",
    "\n",
    "train_segs_map, train_stoich_map = build_segments_map(train_seqs)\n",
    "test_segs_map,  test_stoich_map  = build_segments_map(test_seqs)\n",
    "\n",
    "def process_labels(labels_df):\n",
    "    coords_dict = {}\n",
    "    # Faster + safer prefix extraction\n",
    "    prefixes = labels_df['ID'].str.rsplit('_', n=1).str[0]\n",
    "    for id_prefix, group in labels_df.groupby(prefixes):\n",
    "        coords_dict[id_prefix] = group.sort_values('resid')[['x_1', 'y_1', 'z_1']].values\n",
    "    return coords_dict\n",
    "\n",
    "train_coords_dict = process_labels(train_labels)\n",
    "\n",
    "from Bio.Align import PairwiseAligner\n",
    "\n",
    "aligner = PairwiseAligner()\n",
    "aligner.mode = 'global'\n",
    "aligner.match_score = 2\n",
    "aligner.mismatch_score = -1.5\n",
    "\n",
    "# Stronger gap penalties discourage \"sliding\" (critical: residue numbering must match)\n",
    "aligner.open_gap_score   = -8\n",
    "aligner.extend_gap_score = -0.4\n",
    "\n",
    "# Also penalize terminal gaps (prevents end-gap semi-global behavior)\n",
    "aligner.query_left_open_gap_score  = -8\n",
    "aligner.query_left_extend_gap_score = -0.4\n",
    "aligner.query_right_open_gap_score = -8\n",
    "aligner.query_right_extend_gap_score = -0.4\n",
    "aligner.target_left_open_gap_score = -8\n",
    "aligner.target_left_extend_gap_score = -0.4\n",
    "aligner.target_right_open_gap_score = -8\n",
    "aligner.target_right_extend_gap_score = -0.4\n",
    "\n",
    "def find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n",
    "    similar_seqs = []\n",
    "    \n",
    "    # Pre-filter: Iterate only valid targets\n",
    "    # Note: aligner.score is much faster than generating full alignments\n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        target_id, train_seq = row['target_id'], row['sequence']\n",
    "        if target_id not in train_coords_dict: continue\n",
    "        \n",
    "        # Length filter (keep your original logic)\n",
    "        if abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq)) > 0.3: continue\n",
    "        \n",
    "        # FAST SCORE: Calculates score without traceback overhead\n",
    "        raw_score = aligner.score(query_seq, train_seq)\n",
    "        \n",
    "        normalized_score = raw_score / (2 * min(len(query_seq), len(train_seq)))\n",
    "        similar_seqs.append((target_id, train_seq, normalized_score, train_coords_dict[target_id]))\n",
    "    \n",
    "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return similar_seqs[:top_n]\n",
    "\n",
    "def adapt_template_to_query(query_seq, template_seq, template_coords):\n",
    "    # Generate the alignment object\n",
    "    # aligner.align returns an iterator; we take the first optimal alignment\n",
    "    alignment = next(iter(aligner.align(query_seq, template_seq)))\n",
    "    \n",
    "    new_coords = np.full((len(query_seq), 3), np.nan)\n",
    "    \n",
    "    # VECTORIZED MAPPING:\n",
    "    # alignment.aligned returns lists of (start, end) tuples for matched segments.\n",
    "    # This avoids the slow python loop \"for char_q, char_t in zip...\"\n",
    "    for (q_start, q_end), (t_start, t_end) in zip(*alignment.aligned):\n",
    "        # Map the coordinate chunk directly\n",
    "        t_chunk = template_coords[t_start:t_end]\n",
    "        \n",
    "        # Safety check to ensure shapes match (handles edge cases)\n",
    "        if len(t_chunk) == (q_end - q_start):\n",
    "            new_coords[q_start:q_end] = t_chunk\n",
    "\n",
    "    # --- Interpolation Logic (Unchanged) ---\n",
    "    for i in range(len(new_coords)):\n",
    "        if np.isnan(new_coords[i, 0]):\n",
    "            prev_v = next((j for j in range(i-1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            next_v = next((j for j in range(i+1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            if prev_v >= 0 and next_v >= 0:\n",
    "                w = (i - prev_v) / (next_v - prev_v)\n",
    "                new_coords[i] = (1-w)*new_coords[prev_v] + w*new_coords[next_v]\n",
    "            elif prev_v >= 0: new_coords[i] = new_coords[prev_v] + [3, 0, 0]\n",
    "            elif next_v >= 0: new_coords[i] = new_coords[next_v] + [3, 0, 0]\n",
    "            else: new_coords[i] = [i*3, 0, 0]\n",
    "            \n",
    "    return np.nan_to_num(new_coords)\n",
    "\n",
    "def adaptive_rna_constraints(coordinates, target_id, confidence=1.0, passes=2):\n",
    "    \"\"\"\n",
    "    Evaluation-driven constraints:\n",
    "    - US-align is show-only rigid body => internal geometry errors are fatal\n",
    "    - apply within each chain segment (no fake bond across chain breaks)\n",
    "    \"\"\"\n",
    "    coords = coordinates.copy()\n",
    "    segments = test_segs_map.get(target_id, [(0, len(coords))])\n",
    "\n",
    "    # stronger corrections when confidence is low\n",
    "    strength = 0.75 * (1.0 - min(confidence, 0.97))\n",
    "    strength = max(strength, 0.02)\n",
    "\n",
    "    for _ in range(passes):\n",
    "        for (s, e) in segments:\n",
    "            X = coords[s:e]\n",
    "            L = e - s\n",
    "            if L < 3:\n",
    "                coords[s:e] = X\n",
    "                continue\n",
    "\n",
    "            # (1) bond i,i+1 to ~5.95Å (vectorized, symmetric)\n",
    "            d = X[1:] - X[:-1]\n",
    "            dist = np.linalg.norm(d, axis=1) + 1e-6\n",
    "            target = 5.95\n",
    "            scale = (target - dist) / dist\n",
    "            adj = (d * scale[:, None]) * (0.22 * strength)\n",
    "            X[:-1] -= adj\n",
    "            X[1:]  += adj\n",
    "\n",
    "            # (2) soft i,i+2 to ~10.2Å (vectorized, symmetric)\n",
    "            d2 = X[2:] - X[:-2]\n",
    "            dist2 = np.linalg.norm(d2, axis=1) + 1e-6\n",
    "            target2 = 10.2\n",
    "            scale2 = (target2 - dist2) / dist2\n",
    "            adj2 = (d2 * scale2[:, None]) * (0.10 * strength)\n",
    "            X[:-2] -= adj2\n",
    "            X[2:]  += adj2\n",
    "\n",
    "            # (3) Laplacian smoothing (removes kinks US-align cannot fix)\n",
    "            lap = 0.5 * (X[:-2] + X[2:]) - X[1:-1]\n",
    "            X[1:-1] += (0.06 * strength) * lap\n",
    "\n",
    "            # (4) light self-avoidance (prevents steric collapse)\n",
    "            if L >= 25:\n",
    "                k = min(L, 160) if L > 220 else L\n",
    "                if k < L:\n",
    "                    idx = np.linspace(0, L - 1, k).astype(int)\n",
    "                else:\n",
    "                    idx = np.arange(L)\n",
    "\n",
    "                P = X[idx]\n",
    "                diff = P[:, None, :] - P[None, :, :]\n",
    "                distm = np.linalg.norm(diff, axis=2) + 1e-6\n",
    "                sep = np.abs(idx[:, None] - idx[None, :])\n",
    "\n",
    "                mask = (sep > 2) & (distm < 3.2)\n",
    "                if np.any(mask):\n",
    "                    force = (3.2 - distm) / distm\n",
    "                    vec = (diff * force[:, :, None] * mask[:, :, None]).sum(axis=1)\n",
    "                    X[idx] += (0.015 * strength) * vec\n",
    "\n",
    "            coords[s:e] = X\n",
    "\n",
    "    return coords\n",
    "\n",
    "def _rotmat(axis, ang):\n",
    "    axis = np.asarray(axis, float)\n",
    "    axis = axis / (np.linalg.norm(axis) + 1e-12)\n",
    "    x, y, z = axis\n",
    "    c, s = np.cos(ang), np.sin(ang)\n",
    "    C = 1.0 - c\n",
    "    return np.array([\n",
    "        [c + x*x*C,     x*y*C - z*s, x*z*C + y*s],\n",
    "        [y*x*C + z*s,   c + y*y*C,   y*z*C - x*s],\n",
    "        [z*x*C - y*s,   z*y*C + x*s, c + z*z*C]\n",
    "    ], dtype=float)\n",
    "\n",
    "def apply_hinge(coords, seg, rng, max_angle_deg=25):\n",
    "    s, e = seg\n",
    "    L = e - s\n",
    "    if L < 30:\n",
    "        return coords\n",
    "    pivot = s + int(rng.integers(10, L - 10))\n",
    "    axis = rng.normal(size=3)\n",
    "    ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n",
    "    R = _rotmat(axis, ang)\n",
    "    X = coords.copy()\n",
    "    p0 = X[pivot].copy()\n",
    "    X[pivot+1:e] = (X[pivot+1:e] - p0) @ R.T + p0\n",
    "    return X\n",
    "\n",
    "def jitter_chains(coords, segments, rng, max_angle_deg=12, max_trans=1.5):\n",
    "    X = coords.copy()\n",
    "    global_center = X.mean(axis=0, keepdims=True)\n",
    "    for (s, e) in segments:\n",
    "        axis = rng.normal(size=3)\n",
    "        ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n",
    "        R = _rotmat(axis, ang)\n",
    "        shift = rng.normal(size=3)\n",
    "        shift = shift / (np.linalg.norm(shift) + 1e-12) * float(rng.uniform(0.0, max_trans))\n",
    "        c = X[s:e].mean(axis=0, keepdims=True)\n",
    "        X[s:e] = (X[s:e] - c) @ R.T + c + shift\n",
    "    # recenter\n",
    "    X -= X.mean(axis=0, keepdims=True) - global_center\n",
    "    return X\n",
    "\n",
    "def smooth_wiggle(coords, segments, rng, amp=0.8):\n",
    "    X = coords.copy()\n",
    "    for (s, e) in segments:\n",
    "        L = e - s\n",
    "        if L < 20:\n",
    "            continue\n",
    "        n_ctrl = 6\n",
    "        ctrl_x = np.linspace(0, L - 1, n_ctrl)\n",
    "        ctrl_disp = rng.normal(0, amp, size=(n_ctrl, 3))\n",
    "        t = np.arange(L)\n",
    "        disp = np.vstack([np.interp(t, ctrl_x, ctrl_disp[:, k]) for k in range(3)]).T\n",
    "        X[s:e] += disp\n",
    "    return X\n",
    "\n",
    "def predict_rna_structures(row, train_seqs_df, train_coords_dict, n_predictions=5):\n",
    "    tid = row['target_id']\n",
    "    seq = row['sequence']\n",
    "\n",
    "    # Data constraint: should already be canonical A/C/G/U\n",
    "    assert set(seq).issubset(set(\"ACGU\")), f\"Non-ACGU in {tid}; do not remap here.\"\n",
    "\n",
    "    segments = test_segs_map.get(tid, [(0, len(seq))])\n",
    "\n",
    "    # Grab a larger candidate pool, then sample for diversity (best-of-5)\n",
    "    cands = find_similar_sequences(query_seq=seq, train_seqs_df=train_seqs_df, train_coords_dict=train_coords_dict, top_n=30)\n",
    "    assert all(len(c[3]) == len(c[1]) for c in cands), \"Template coords/seq length mismatch\"\n",
    "    predictions = []\n",
    "    used = set()\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        seed = (abs(hash(tid)) + i * 10007) % (2**32)\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        if not cands:\n",
    "            # hard fallback (straight line per chain)\n",
    "            coords = np.zeros((len(seq), 3), dtype=float)\n",
    "            for (s, e) in segments:\n",
    "                for j in range(s+1, e):\n",
    "                    coords[j] = coords[j-1] + [5.95, 0, 0]\n",
    "            predictions.append(coords)\n",
    "            continue\n",
    "\n",
    "        # Choose template:\n",
    "        # i=0 => best template; others => sample among top-K with weights, avoid duplicates\n",
    "        if i == 0:\n",
    "            t_id, t_seq, sim, t_coords = cands[0]\n",
    "        else:\n",
    "            K = min(12, len(cands))\n",
    "            sims = np.array([cands[k][2] for k in range(K)], float)\n",
    "            w = np.exp((sims - sims.max()) / 0.08)\n",
    "            # penalize already used templates\n",
    "            for k in range(K):\n",
    "                if cands[k][0] in used:\n",
    "                    w[k] *= 0.10\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            k = int(rng.choice(np.arange(K), p=w))\n",
    "            t_id, t_seq, sim, t_coords = cands[k]\n",
    "\n",
    "        used.add(t_id)\n",
    "\n",
    "        # Transfer coords with diagonal-guard mapping (no sliding)\n",
    "        adapted = adapt_template_to_query(query_seq=seq, template_seq=t_seq, template_coords=t_coords)\n",
    "\n",
    "        # Diversity transforms (then re-refine constraints)\n",
    "        if i == 0:\n",
    "            X = adapted\n",
    "        elif i == 1:\n",
    "            # mild noise\n",
    "            X = adapted + rng.normal(0, max(0.01, (0.40 - sim) * 0.06), adapted.shape)\n",
    "        elif i == 2:\n",
    "            # hinge within the longest chain\n",
    "            longest = max(segments, key=lambda se: se[1] - se[0])\n",
    "            X = apply_hinge(adapted, longest, rng, max_angle_deg=22)\n",
    "        elif i == 3:\n",
    "            # inter-chain jitter (small, safe)\n",
    "            X = jitter_chains(adapted, segments, rng, max_angle_deg=10, max_trans=1.0)\n",
    "        else:\n",
    "            # smooth low-frequency deformation\n",
    "            X = smooth_wiggle(adapted, segments, rng, amp=0.7)\n",
    "\n",
    "        refined = adaptive_rna_constraints(X, tid, confidence=sim, passes=2)\n",
    "        predictions.append(refined)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "all_predictions = []\n",
    "start_time = time.time()\n",
    "for idx, row in test_seqs.iterrows():\n",
    "    if idx % 10 == 0: print(f\"Processing {idx} | {time.time()-start_time:.1f}s\")\n",
    "    tid, seq = row['target_id'], row['sequence']\n",
    "    preds = predict_rna_structures(row, train_seqs, train_coords_dict)\n",
    "    for j in range(len(seq)):\n",
    "        res = {'ID': f\"{tid}_{j+1}\", 'resname': seq[j], 'resid': j+1}\n",
    "        for i in range(5):\n",
    "            res[f'x_{i+1}'], res[f'y_{i+1}'], res[f'z_{i+1}'] = preds[i][j]\n",
    "        all_predictions.append(res)\n",
    "\n",
    "sub = pd.DataFrame(all_predictions)\n",
    "cols = ['ID', 'resname', 'resid'] + [f'{c}_{i}' for i in range(1,6) for c in ['x','y','z']]\n",
    "\n",
    "# Safety: competition clips coords; do it explicitly to avoid out-of-range explosions\n",
    "coord_cols = [c for c in cols if c.startswith(('x_','y_','z_'))]\n",
    "sub[coord_cols] = sub[coord_cols].clip(-999.999, 9999.999)\n",
    "\n",
    "sub[cols].to_csv('submission.csv', index=False)\n",
    "print(\"submission.csv! saved\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 15231210,
     "sourceId": 118765,
     "sourceType": "competition"
    },
    {
     "datasetId": 9328538,
     "sourceId": 14604295,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 164.663827,
   "end_time": "2026-02-08T20:10:29.855629",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-08T20:07:45.191802",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
