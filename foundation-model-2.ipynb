{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664ebd24",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-28T16:58:53.902547Z",
     "iopub.status.busy": "2025-03-28T16:58:53.902042Z",
     "iopub.status.idle": "2025-03-28T17:01:54.193253Z",
     "shell.execute_reply": "2025-03-28T17:01:54.192153Z"
    },
    "papermill": {
     "duration": 180.296564,
     "end_time": "2025-03-28T17:01:54.194981",
     "exception": false,
     "start_time": "2025-03-28T16:58:53.898417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (8349, 38)\n",
      "Training Foundation Autoencoder...\n",
      "Foundation Autoencoder Epoch 1/200, Loss: 0.3422\n",
      "Foundation Autoencoder Epoch 2/200, Loss: 0.0376\n",
      "Foundation Autoencoder Epoch 3/200, Loss: 0.0228\n",
      "Foundation Autoencoder Epoch 4/200, Loss: 0.0178\n",
      "Foundation Autoencoder Epoch 5/200, Loss: 0.0143\n",
      "Foundation Autoencoder Epoch 6/200, Loss: 0.0118\n",
      "Foundation Autoencoder Epoch 7/200, Loss: 0.0097\n",
      "Foundation Autoencoder Epoch 8/200, Loss: 0.0076\n",
      "Foundation Autoencoder Epoch 9/200, Loss: 0.0060\n",
      "Foundation Autoencoder Epoch 10/200, Loss: 0.0054\n",
      "Foundation Autoencoder Epoch 11/200, Loss: 0.0050\n",
      "Foundation Autoencoder Epoch 12/200, Loss: 0.0051\n",
      "Foundation Autoencoder Epoch 13/200, Loss: 0.0042\n",
      "Foundation Autoencoder Epoch 14/200, Loss: 0.0038\n",
      "Foundation Autoencoder Epoch 15/200, Loss: 0.0038\n",
      "Foundation Autoencoder Epoch 16/200, Loss: 0.0034\n",
      "Foundation Autoencoder Epoch 17/200, Loss: 0.0035\n",
      "Foundation Autoencoder Epoch 18/200, Loss: 0.0032\n",
      "Foundation Autoencoder Epoch 19/200, Loss: 0.0033\n",
      "Foundation Autoencoder Epoch 20/200, Loss: 0.0030\n",
      "Foundation Autoencoder Epoch 21/200, Loss: 0.0030\n",
      "Foundation Autoencoder Epoch 22/200, Loss: 0.0026\n",
      "Foundation Autoencoder Epoch 23/200, Loss: 0.0027\n",
      "Foundation Autoencoder Epoch 24/200, Loss: 0.0025\n",
      "Foundation Autoencoder Epoch 25/200, Loss: 0.0024\n",
      "Foundation Autoencoder Epoch 26/200, Loss: 0.0048\n",
      "Foundation Autoencoder Epoch 27/200, Loss: 0.0021\n",
      "Foundation Autoencoder Epoch 28/200, Loss: 0.0023\n",
      "Foundation Autoencoder Epoch 29/200, Loss: 0.0022\n",
      "Foundation Autoencoder Epoch 30/200, Loss: 0.0022\n",
      "Foundation Autoencoder Epoch 31/200, Loss: 0.0021\n",
      "Foundation Autoencoder Epoch 32/200, Loss: 0.0021\n",
      "Foundation Autoencoder Epoch 33/200, Loss: 0.0021\n",
      "Foundation Autoencoder Epoch 34/200, Loss: 0.0021\n",
      "Foundation Autoencoder Epoch 35/200, Loss: 0.0022\n",
      "Foundation Autoencoder Epoch 36/200, Loss: 0.0020\n",
      "Foundation Autoencoder Epoch 37/200, Loss: 0.0031\n",
      "Foundation Autoencoder Epoch 38/200, Loss: 0.0019\n",
      "Foundation Autoencoder Epoch 39/200, Loss: 0.0019\n",
      "Foundation Autoencoder Epoch 40/200, Loss: 0.0024\n",
      "Foundation Autoencoder Epoch 41/200, Loss: 0.0017\n",
      "Foundation Autoencoder Epoch 42/200, Loss: 0.0018\n",
      "Foundation Autoencoder Epoch 43/200, Loss: 0.0018\n",
      "Foundation Autoencoder Epoch 44/200, Loss: 0.0019\n",
      "Foundation Autoencoder Epoch 45/200, Loss: 0.0020\n",
      "Foundation Autoencoder Epoch 46/200, Loss: 0.0020\n",
      "Foundation Autoencoder Epoch 47/200, Loss: 0.0018\n",
      "Foundation Autoencoder Epoch 48/200, Loss: 0.0017\n",
      "Foundation Autoencoder Epoch 49/200, Loss: 0.0022\n",
      "Foundation Autoencoder Epoch 50/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 51/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 52/200, Loss: 0.0018\n",
      "Foundation Autoencoder Epoch 53/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 54/200, Loss: 0.0017\n",
      "Foundation Autoencoder Epoch 55/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 56/200, Loss: 0.0015\n",
      "Foundation Autoencoder Epoch 57/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 58/200, Loss: 0.0018\n",
      "Foundation Autoencoder Epoch 59/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 60/200, Loss: 0.0015\n",
      "Foundation Autoencoder Epoch 61/200, Loss: 0.0015\n",
      "Foundation Autoencoder Epoch 62/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 63/200, Loss: 0.0015\n",
      "Foundation Autoencoder Epoch 64/200, Loss: 0.0020\n",
      "Foundation Autoencoder Epoch 65/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 66/200, Loss: 0.0018\n",
      "Foundation Autoencoder Epoch 67/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 68/200, Loss: 0.0015\n",
      "Foundation Autoencoder Epoch 69/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 70/200, Loss: 0.0019\n",
      "Foundation Autoencoder Epoch 71/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 72/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 73/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 74/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 75/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 76/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 77/200, Loss: 0.0019\n",
      "Foundation Autoencoder Epoch 78/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 79/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 80/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 81/200, Loss: 0.0020\n",
      "Foundation Autoencoder Epoch 82/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 83/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 84/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 85/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 86/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 87/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 88/200, Loss: 0.0025\n",
      "Foundation Autoencoder Epoch 89/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 90/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 91/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 92/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 93/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 94/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 95/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 96/200, Loss: 0.0015\n",
      "Foundation Autoencoder Epoch 97/200, Loss: 0.0017\n",
      "Foundation Autoencoder Epoch 98/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 99/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 100/200, Loss: 0.0017\n",
      "Foundation Autoencoder Epoch 101/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 102/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 103/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 104/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 105/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 106/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 107/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 108/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 109/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 110/200, Loss: 0.0015\n",
      "Foundation Autoencoder Epoch 111/200, Loss: 0.0015\n",
      "Foundation Autoencoder Epoch 112/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 113/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 114/200, Loss: 0.0018\n",
      "Foundation Autoencoder Epoch 115/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 116/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 117/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 118/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 119/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 120/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 121/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 122/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 123/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 124/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 125/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 126/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 127/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 128/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 129/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 130/200, Loss: 0.0014\n",
      "Foundation Autoencoder Epoch 131/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 132/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 133/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 134/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 135/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 136/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 137/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 138/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 139/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 140/200, Loss: 0.0017\n",
      "Foundation Autoencoder Epoch 141/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 142/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 143/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 144/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 145/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 146/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 147/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 148/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 149/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 150/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 151/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 152/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 153/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 154/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 155/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 156/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 157/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 158/200, Loss: 0.0013\n",
      "Foundation Autoencoder Epoch 159/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 160/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 161/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 162/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 163/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 164/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 165/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 166/200, Loss: 0.0017\n",
      "Foundation Autoencoder Epoch 167/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 168/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 169/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 170/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 171/200, Loss: 0.0016\n",
      "Foundation Autoencoder Epoch 172/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 173/200, Loss: 0.0008\n",
      "Foundation Autoencoder Epoch 174/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 175/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 176/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 177/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 178/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 179/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 180/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 181/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 182/200, Loss: 0.0010\n",
      "Foundation Autoencoder Epoch 183/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 184/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 185/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 186/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 187/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 188/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 189/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 190/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 191/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 192/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 193/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 194/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 195/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 196/200, Loss: 0.0011\n",
      "Foundation Autoencoder Epoch 197/200, Loss: 0.0009\n",
      "Foundation Autoencoder Epoch 198/200, Loss: 0.0012\n",
      "Foundation Autoencoder Epoch 199/200, Loss: 0.0008\n",
      "Foundation Autoencoder Epoch 200/200, Loss: 0.0008\n",
      "Training Main Mapping Model...\n",
      "Main Mapping Model Epoch 1/200, Loss: 840.8426\n",
      "Main Mapping Model Epoch 2/200, Loss: 5.0089\n",
      "Main Mapping Model Epoch 3/200, Loss: 3.0840\n",
      "Main Mapping Model Epoch 4/200, Loss: 2.3288\n",
      "Main Mapping Model Epoch 5/200, Loss: 1.8430\n",
      "Main Mapping Model Epoch 6/200, Loss: 1.6203\n",
      "Main Mapping Model Epoch 7/200, Loss: 1.5381\n",
      "Main Mapping Model Epoch 8/200, Loss: 1.4269\n",
      "Main Mapping Model Epoch 9/200, Loss: 1.3984\n",
      "Main Mapping Model Epoch 10/200, Loss: 1.3796\n",
      "Main Mapping Model Epoch 11/200, Loss: 1.3080\n",
      "Main Mapping Model Epoch 12/200, Loss: 1.3536\n",
      "Main Mapping Model Epoch 13/200, Loss: 1.3679\n",
      "Main Mapping Model Epoch 14/200, Loss: 1.3584\n",
      "Main Mapping Model Epoch 15/200, Loss: 1.3170\n",
      "Main Mapping Model Epoch 16/200, Loss: 1.3504\n",
      "Main Mapping Model Epoch 17/200, Loss: 1.3628\n",
      "Main Mapping Model Epoch 18/200, Loss: 1.3830\n",
      "Main Mapping Model Epoch 19/200, Loss: 1.3499\n",
      "Main Mapping Model Epoch 20/200, Loss: 1.4789\n",
      "Main Mapping Model Epoch 21/200, Loss: 1.3947\n",
      "Main Mapping Model Epoch 22/200, Loss: 1.4519\n",
      "Main Mapping Model Epoch 23/200, Loss: 1.4896\n",
      "Main Mapping Model Epoch 24/200, Loss: 1.4701\n",
      "Main Mapping Model Epoch 25/200, Loss: 1.4948\n",
      "Main Mapping Model Epoch 26/200, Loss: 1.5233\n",
      "Main Mapping Model Epoch 27/200, Loss: 1.4690\n",
      "Main Mapping Model Epoch 28/200, Loss: 1.4806\n",
      "Main Mapping Model Epoch 29/200, Loss: 1.4756\n",
      "Main Mapping Model Epoch 30/200, Loss: 1.5285\n",
      "Main Mapping Model Epoch 31/200, Loss: 1.5253\n",
      "Main Mapping Model Epoch 32/200, Loss: 1.5039\n",
      "Main Mapping Model Epoch 33/200, Loss: 1.5288\n",
      "Main Mapping Model Epoch 34/200, Loss: 1.5302\n",
      "Main Mapping Model Epoch 35/200, Loss: 1.5247\n",
      "Main Mapping Model Epoch 36/200, Loss: 1.4987\n",
      "Main Mapping Model Epoch 37/200, Loss: 1.6196\n",
      "Main Mapping Model Epoch 38/200, Loss: 1.5542\n",
      "Main Mapping Model Epoch 39/200, Loss: 1.4429\n",
      "Main Mapping Model Epoch 40/200, Loss: 1.5644\n",
      "Main Mapping Model Epoch 41/200, Loss: 1.4844\n",
      "Main Mapping Model Epoch 42/200, Loss: 1.5546\n",
      "Main Mapping Model Epoch 43/200, Loss: 1.5446\n",
      "Main Mapping Model Epoch 44/200, Loss: 1.5258\n",
      "Main Mapping Model Epoch 45/200, Loss: 1.4579\n",
      "Main Mapping Model Epoch 46/200, Loss: 1.5130\n",
      "Main Mapping Model Epoch 47/200, Loss: 1.5584\n",
      "Main Mapping Model Epoch 48/200, Loss: 1.4837\n",
      "Main Mapping Model Epoch 49/200, Loss: 1.5784\n",
      "Main Mapping Model Epoch 50/200, Loss: 1.5343\n",
      "Main Mapping Model Epoch 51/200, Loss: 1.4983\n",
      "Main Mapping Model Epoch 52/200, Loss: 1.6143\n",
      "Main Mapping Model Epoch 53/200, Loss: 1.4058\n",
      "Main Mapping Model Epoch 54/200, Loss: 1.5323\n",
      "Main Mapping Model Epoch 55/200, Loss: 1.5207\n",
      "Main Mapping Model Epoch 56/200, Loss: 1.5058\n",
      "Main Mapping Model Epoch 57/200, Loss: 1.5288\n",
      "Main Mapping Model Epoch 58/200, Loss: 1.4650\n",
      "Main Mapping Model Epoch 59/200, Loss: 1.5116\n",
      "Main Mapping Model Epoch 60/200, Loss: 1.4789\n",
      "Main Mapping Model Epoch 61/200, Loss: 1.5262\n",
      "Main Mapping Model Epoch 62/200, Loss: 1.4598\n",
      "Main Mapping Model Epoch 63/200, Loss: 1.5383\n",
      "Main Mapping Model Epoch 64/200, Loss: 1.4279\n",
      "Main Mapping Model Epoch 65/200, Loss: 1.5945\n",
      "Main Mapping Model Epoch 66/200, Loss: 1.4700\n",
      "Main Mapping Model Epoch 67/200, Loss: 1.5370\n",
      "Main Mapping Model Epoch 68/200, Loss: 1.5020\n",
      "Main Mapping Model Epoch 69/200, Loss: 1.5214\n",
      "Main Mapping Model Epoch 70/200, Loss: 1.4275\n",
      "Main Mapping Model Epoch 71/200, Loss: 1.5690\n",
      "Main Mapping Model Epoch 72/200, Loss: 1.3913\n",
      "Main Mapping Model Epoch 73/200, Loss: 1.5996\n",
      "Main Mapping Model Epoch 74/200, Loss: 1.3977\n",
      "Main Mapping Model Epoch 75/200, Loss: 1.4832\n",
      "Main Mapping Model Epoch 76/200, Loss: 1.4546\n",
      "Main Mapping Model Epoch 77/200, Loss: 1.4628\n",
      "Main Mapping Model Epoch 78/200, Loss: 1.5617\n",
      "Main Mapping Model Epoch 79/200, Loss: 1.4021\n",
      "Main Mapping Model Epoch 80/200, Loss: 1.5032\n",
      "Main Mapping Model Epoch 81/200, Loss: 1.3950\n",
      "Main Mapping Model Epoch 82/200, Loss: 1.3893\n",
      "Main Mapping Model Epoch 83/200, Loss: 1.4359\n",
      "Main Mapping Model Epoch 84/200, Loss: 1.4393\n",
      "Main Mapping Model Epoch 85/200, Loss: 1.4509\n",
      "Main Mapping Model Epoch 86/200, Loss: 1.4912\n",
      "Main Mapping Model Epoch 87/200, Loss: 1.4783\n",
      "Main Mapping Model Epoch 88/200, Loss: 1.4919\n",
      "Main Mapping Model Epoch 89/200, Loss: 1.4522\n",
      "Main Mapping Model Epoch 90/200, Loss: 1.5081\n",
      "Main Mapping Model Epoch 91/200, Loss: 1.4138\n",
      "Main Mapping Model Epoch 92/200, Loss: 1.4238\n",
      "Main Mapping Model Epoch 93/200, Loss: 1.4198\n",
      "Main Mapping Model Epoch 94/200, Loss: 1.4939\n",
      "Main Mapping Model Epoch 95/200, Loss: 1.3677\n",
      "Main Mapping Model Epoch 96/200, Loss: 1.4440\n",
      "Main Mapping Model Epoch 97/200, Loss: 1.4563\n",
      "Main Mapping Model Epoch 98/200, Loss: 1.4645\n",
      "Main Mapping Model Epoch 99/200, Loss: 1.4844\n",
      "Main Mapping Model Epoch 100/200, Loss: 1.4037\n",
      "Main Mapping Model Epoch 101/200, Loss: 1.4061\n",
      "Main Mapping Model Epoch 102/200, Loss: 1.4983\n",
      "Main Mapping Model Epoch 103/200, Loss: 1.4199\n",
      "Main Mapping Model Epoch 104/200, Loss: 1.4560\n",
      "Main Mapping Model Epoch 105/200, Loss: 1.4129\n",
      "Main Mapping Model Epoch 106/200, Loss: 1.4442\n",
      "Main Mapping Model Epoch 107/200, Loss: 1.3862\n",
      "Main Mapping Model Epoch 108/200, Loss: 1.4723\n",
      "Main Mapping Model Epoch 109/200, Loss: 1.4167\n",
      "Main Mapping Model Epoch 110/200, Loss: 1.4181\n",
      "Main Mapping Model Epoch 111/200, Loss: 1.3825\n",
      "Main Mapping Model Epoch 112/200, Loss: 1.4507\n",
      "Main Mapping Model Epoch 113/200, Loss: 1.3779\n",
      "Main Mapping Model Epoch 114/200, Loss: 1.4069\n",
      "Main Mapping Model Epoch 115/200, Loss: 1.3516\n",
      "Main Mapping Model Epoch 116/200, Loss: 1.3930\n",
      "Main Mapping Model Epoch 117/200, Loss: 1.4233\n",
      "Main Mapping Model Epoch 118/200, Loss: 1.3547\n",
      "Main Mapping Model Epoch 119/200, Loss: 1.4533\n",
      "Main Mapping Model Epoch 120/200, Loss: 1.3714\n",
      "Main Mapping Model Epoch 121/200, Loss: 1.4098\n",
      "Main Mapping Model Epoch 122/200, Loss: 1.4248\n",
      "Main Mapping Model Epoch 123/200, Loss: 1.3807\n",
      "Main Mapping Model Epoch 124/200, Loss: 1.4745\n",
      "Main Mapping Model Epoch 125/200, Loss: 1.4542\n",
      "Main Mapping Model Epoch 126/200, Loss: 1.3926\n",
      "Main Mapping Model Epoch 127/200, Loss: 1.3771\n",
      "Main Mapping Model Epoch 128/200, Loss: 1.3477\n",
      "Main Mapping Model Epoch 129/200, Loss: 1.3671\n",
      "Main Mapping Model Epoch 130/200, Loss: 1.5056\n",
      "Main Mapping Model Epoch 131/200, Loss: 1.4351\n",
      "Main Mapping Model Epoch 132/200, Loss: 1.4079\n",
      "Main Mapping Model Epoch 133/200, Loss: 1.4163\n",
      "Main Mapping Model Epoch 134/200, Loss: 1.4278\n",
      "Main Mapping Model Epoch 135/200, Loss: 1.2950\n",
      "Main Mapping Model Epoch 136/200, Loss: 1.4091\n",
      "Main Mapping Model Epoch 137/200, Loss: 1.4656\n",
      "Main Mapping Model Epoch 138/200, Loss: 1.3501\n",
      "Main Mapping Model Epoch 139/200, Loss: 1.3717\n",
      "Main Mapping Model Epoch 140/200, Loss: 1.3970\n",
      "Main Mapping Model Epoch 141/200, Loss: 1.3689\n",
      "Main Mapping Model Epoch 142/200, Loss: 1.3901\n",
      "Main Mapping Model Epoch 143/200, Loss: 1.3500\n",
      "Main Mapping Model Epoch 144/200, Loss: 1.3829\n",
      "Main Mapping Model Epoch 145/200, Loss: 1.4124\n",
      "Main Mapping Model Epoch 146/200, Loss: 1.3616\n",
      "Main Mapping Model Epoch 147/200, Loss: 1.3321\n",
      "Main Mapping Model Epoch 148/200, Loss: 1.4205\n",
      "Main Mapping Model Epoch 149/200, Loss: 1.4031\n",
      "Main Mapping Model Epoch 150/200, Loss: 1.3391\n",
      "Main Mapping Model Epoch 151/200, Loss: 1.3729\n",
      "Main Mapping Model Epoch 152/200, Loss: 1.3429\n",
      "Main Mapping Model Epoch 153/200, Loss: 1.4862\n",
      "Main Mapping Model Epoch 154/200, Loss: 1.3397\n",
      "Main Mapping Model Epoch 155/200, Loss: 1.3772\n",
      "Main Mapping Model Epoch 156/200, Loss: 1.3551\n",
      "Main Mapping Model Epoch 157/200, Loss: 1.3638\n",
      "Main Mapping Model Epoch 158/200, Loss: 1.3264\n",
      "Main Mapping Model Epoch 159/200, Loss: 1.3596\n",
      "Main Mapping Model Epoch 160/200, Loss: 1.3123\n",
      "Main Mapping Model Epoch 161/200, Loss: 1.3354\n",
      "Main Mapping Model Epoch 162/200, Loss: 1.3456\n",
      "Main Mapping Model Epoch 163/200, Loss: 1.3641\n",
      "Main Mapping Model Epoch 164/200, Loss: 1.3418\n",
      "Main Mapping Model Epoch 165/200, Loss: 1.3115\n",
      "Main Mapping Model Epoch 166/200, Loss: 1.3805\n",
      "Main Mapping Model Epoch 167/200, Loss: 1.4066\n",
      "Main Mapping Model Epoch 168/200, Loss: 1.3448\n",
      "Main Mapping Model Epoch 169/200, Loss: 1.3100\n",
      "Main Mapping Model Epoch 170/200, Loss: 1.3900\n",
      "Main Mapping Model Epoch 171/200, Loss: 1.3489\n",
      "Main Mapping Model Epoch 172/200, Loss: 1.3685\n",
      "Main Mapping Model Epoch 173/200, Loss: 1.3464\n",
      "Main Mapping Model Epoch 174/200, Loss: 1.3159\n",
      "Main Mapping Model Epoch 175/200, Loss: 1.3322\n",
      "Main Mapping Model Epoch 176/200, Loss: 1.3289\n",
      "Main Mapping Model Epoch 177/200, Loss: 1.3446\n",
      "Main Mapping Model Epoch 178/200, Loss: 1.2784\n",
      "Main Mapping Model Epoch 179/200, Loss: 1.3595\n",
      "Main Mapping Model Epoch 180/200, Loss: 1.4156\n",
      "Main Mapping Model Epoch 181/200, Loss: 1.3056\n",
      "Main Mapping Model Epoch 182/200, Loss: 1.4200\n",
      "Main Mapping Model Epoch 183/200, Loss: 1.2564\n",
      "Main Mapping Model Epoch 184/200, Loss: 1.2838\n",
      "Main Mapping Model Epoch 185/200, Loss: 1.3059\n",
      "Main Mapping Model Epoch 186/200, Loss: 1.3007\n",
      "Main Mapping Model Epoch 187/200, Loss: 1.2954\n",
      "Main Mapping Model Epoch 188/200, Loss: 1.3658\n",
      "Main Mapping Model Epoch 189/200, Loss: 1.3212\n",
      "Main Mapping Model Epoch 190/200, Loss: 1.2896\n",
      "Main Mapping Model Epoch 191/200, Loss: 1.3239\n",
      "Main Mapping Model Epoch 192/200, Loss: 1.2862\n",
      "Main Mapping Model Epoch 193/200, Loss: 1.2924\n",
      "Main Mapping Model Epoch 194/200, Loss: 1.3127\n",
      "Main Mapping Model Epoch 195/200, Loss: 1.3179\n",
      "Main Mapping Model Epoch 196/200, Loss: 1.2708\n",
      "Main Mapping Model Epoch 197/200, Loss: 1.2463\n",
      "Main Mapping Model Epoch 198/200, Loss: 1.3554\n",
      "Main Mapping Model Epoch 199/200, Loss: 1.2624\n",
      "Main Mapping Model Epoch 200/200, Loss: 1.3260\n",
      "Submission file 'submission.csv' created!\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Load Training Data with Slice Information\n",
    "# --------------------------------------------------------------------\n",
    "h5_file_path = \"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\"\n",
    "\n",
    "with h5py.File(h5_file_path, \"r\") as f:\n",
    "    train_spots = f[\"spots/Train\"]\n",
    "    # Each slide's data is loaded and tagged with its slide name.\n",
    "    train_spot_tables = {\n",
    "        slide: pd.DataFrame(np.array(train_spots[slide])).assign(slice_name=slide)\n",
    "        for slide in train_spots.keys()\n",
    "    }\n",
    "train_df = pd.concat(train_spot_tables.values(), ignore_index=True)\n",
    "\n",
    "# Assume the first two columns are x and y, and the remaining 35 columns are cell abundances.\n",
    "cell_types = [f\"C{i+1}\" for i in range(35)]\n",
    "train_df.columns = [\"x\", \"y\"] + cell_types + [\"slice_name\"]\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Define Datasets for Foundation Autoencoder and Main Mapping\n",
    "# --------------------------------------------------------------------\n",
    "# Dataset for the foundation autoencoder: uses the cell abundance vector.\n",
    "class FoundationDataset(Dataset):\n",
    "    def __init__(self, abundances):\n",
    "        self.data = torch.tensor(abundances, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Dataset for training the main model: maps (x, y) coordinates to the foundation embedding.\n",
    "class MainMappingDataset(Dataset):\n",
    "    def __init__(self, coords, embeddings):\n",
    "        self.coords = torch.tensor(coords, dtype=torch.float32)\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.coords)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.coords[idx], self.embeddings[idx]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Define the Foundation Autoencoder\n",
    "# --------------------------------------------------------------------\n",
    "class FoundationAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder that learns a low-dimensional embedding from cell abundance vectors.\n",
    "    The encoder should capture relationships among cell types (e.g., co-occurrence, ranking, etc.).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=35, embed_dim=16, hidden_dim=64):\n",
    "        super(FoundationAutoencoder, self).__init__()\n",
    "        # Encoder network: compresses the 35-d cell abundance vector into a lower-dimensional embedding.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        # Decoder network: reconstructs the cell abundance vector from the embedding.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)\n",
    "        recon = self.decoder(emb)\n",
    "        return emb, recon\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Define the Main Model: Mapping from Coordinates to Embedding\n",
    "# --------------------------------------------------------------------\n",
    "class MainModelMapping(nn.Module):\n",
    "    \"\"\"\n",
    "    Main model that learns to predict the foundation embedding from (x, y) coordinates.\n",
    "    During inference, the predicted embedding is passed to the foundation decoder to generate cell abundances.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, embed_dim=16, hidden_dim=64):\n",
    "        super(MainModelMapping, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Define Training Functions for Each Model\n",
    "# --------------------------------------------------------------------\n",
    "def train_foundation_autoencoder(model, dataloader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for data in dataloader:\n",
    "            data = data.to(device)\n",
    "            emb, recon = model(data)\n",
    "            loss = criterion(recon, data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * data.size(0)\n",
    "        print(f\"Foundation Autoencoder Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader.dataset):.4f}\")\n",
    "    return model\n",
    "\n",
    "def train_main_mapping(model, dataloader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for coords, target_emb in dataloader:\n",
    "            coords, target_emb = coords.to(device), target_emb.to(device)\n",
    "            pred_emb = model(coords)\n",
    "            loss = criterion(pred_emb, target_emb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * coords.size(0)\n",
    "        print(f\"Main Mapping Model Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader.dataset):.4f}\")\n",
    "    return model\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6. Train the Foundation Autoencoder\n",
    "# --------------------------------------------------------------------\n",
    "# Create dataset from the training cell abundance vectors.\n",
    "foundation_dataset = FoundationDataset(train_df[cell_types].values)\n",
    "foundation_loader = DataLoader(foundation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "foundation_model = FoundationAutoencoder(input_dim=35, embed_dim=16, hidden_dim=64)\n",
    "print(\"Training Foundation Autoencoder...\")\n",
    "foundation_model = train_foundation_autoencoder(foundation_model, foundation_loader, num_epochs=200, lr=0.001, device=device)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 7. Precompute Foundation Embeddings for Training Spots\n",
    "# --------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    foundation_model.eval()\n",
    "    train_abundances = torch.tensor(train_df[cell_types].values, dtype=torch.float32).to(device)\n",
    "    train_embeddings, _ = foundation_model(train_abundances)\n",
    "    train_embeddings = train_embeddings.cpu().numpy()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 8. Train the Main Model to Predict Embeddings from Coordinates\n",
    "# --------------------------------------------------------------------\n",
    "main_dataset = MainMappingDataset(train_df[['x','y']].values, train_embeddings)\n",
    "main_loader = DataLoader(main_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "main_model = MainModelMapping(input_dim=2, embed_dim=16, hidden_dim=64)\n",
    "print(\"Training Main Mapping Model...\")\n",
    "main_model = train_main_mapping(main_model, main_loader, num_epochs=200, lr=0.001, device=device)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 9. Inference on Test Data and Submission Creation\n",
    "# --------------------------------------------------------------------\n",
    "with h5py.File(h5_file_path, \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_array = np.array(test_spots[\"S_7\"])\n",
    "    test_df = pd.DataFrame(test_array)\n",
    "    # Test data columns are assumed to be: x, y, Test_set.\n",
    "    if test_df.shape[1] == 3:\n",
    "        test_df.columns = [\"x\", \"y\", \"Test_set\"]\n",
    "        test_df = test_df[[\"x\", \"y\"]]\n",
    "    elif test_df.shape[1] == 2:\n",
    "        test_df.columns = [\"x\", \"y\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected number of columns in test data.\")\n",
    "\n",
    "test_coords = test_df[['x','y']].values\n",
    "test_coords_tensor = torch.tensor(test_coords, dtype=torch.float32).to(device)\n",
    "\n",
    "main_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_embeddings = main_model(test_coords_tensor)\n",
    "# Use the foundation decoder to convert embeddings into cell abundance predictions.\n",
    "foundation_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_abundances = foundation_model.decoder(predicted_embeddings)\n",
    "    predicted_abundances = predicted_abundances.cpu().numpy()\n",
    "\n",
    "submission_df = pd.DataFrame(predicted_abundances, columns=cell_types)\n",
    "submission_df.insert(0, 'ID', test_df.index)\n",
    "submission_file = \"submission.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "print(f\"Submission file '{submission_file}' created!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11390004,
     "sourceId": 94147,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 185.700466,
   "end_time": "2025-03-28T17:01:56.652732",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-28T16:58:50.952266",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
