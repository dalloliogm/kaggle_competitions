{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12609125,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12191182,"sourceType":"datasetVersion","datasetId":7678913}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Autogluon Notebook\n\nThis is a modified version of this amazing notebook: [smiles-rdkit-lgbm-ftw](https://www.kaggle.com/code/richolson/smiles-rdkit-lgbm-ftw). Instead of using LGBM, we use Autogluon, an auto-ml framework.","metadata":{}},{"cell_type":"markdown","source":"# SMILES->RDKIT->LGBM->FTW  ðŸ§ªâš¡ðŸš€\n\n**SMILES** *(Simplified Molecular Input Line Entry System)*  \n**RDKIT** *(Open-source cheminformatics toolkit)*   \n**AutoGLuon** *AUtomated Machine Learning*\n**FTW** *(For The Win!)*  \n\n## 1. Use RDKit to calculate *chemical descriptors* from our SMILES molecule data\n- **Structural Counts:** Ring counts, rotatable bonds, molecular weight\n- **Calculated Properties:** LogP (oiliness), TPSA (surface area), qed (drug-likeness), complexity/shape stuff\n- We infer these for both **train** and **test** data\n- **We are using RDKit to do feature engineering**\n\n## 2. Train models using those features to predict our *targets*:\n- **Tg** - Glass transition temperature (Â°C)\n- **FFV** - Fractional free volume\n- **Tc** - Thermal conductivity (W/mÂ·K)\n- **Density** - Polymer density (g/cmÂ³)\n- **Rg** - Radius of gyration (Ã…)\n\n## We train unique Autogluon models for each target!\n- Actually 5 models per target using CV / averaging predictions (**25 models total!**)\n- **RDKit is doing the heavy-lifting here** - we just train a model to figure out how to translate the data to our targets...\n\n*Friendly Reminder:* If re-using large parts of this work in a public notebook - **please credit where you found the code**","metadata":{"_kg_hide-output":true}},{"cell_type":"markdown","source":"# Install RDKit and Autogluon\n* https://www.kaggle.com/datasets/richolson/rdkit-install-whl","metadata":{}},{"cell_type":"code","source":"# Install Autogluon for offline use\n!pip install -q autogluon --no-index --find-links=file:///kaggle/input/autogluon-install-notebook","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:01:55.902199Z","iopub.execute_input":"2025-06-18T14:01:55.902786Z","iopub.status.idle":"2025-06-18T14:02:01.442432Z","shell.execute_reply.started":"2025-06-18T14:01:55.902761Z","shell.execute_reply":"2025-06-18T14:02:01.441584Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# install RDKit for offline use\n!pip install /kaggle/input/rdkit-install-whl/rdkit_wheel/rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:01.443981Z","iopub.execute_input":"2025-06-18T14:02:01.444226Z","iopub.status.idle":"2025-06-18T14:02:04.755369Z","shell.execute_reply.started":"2025-06-18T14:02:01.444203Z","shell.execute_reply":"2025-06-18T14:02:04.754416Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/rdkit-install-whl/rdkit_wheel/rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi==2022.9.5) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi==2022.9.5) (11.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi==2022.9.5) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi==2022.9.5) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit-pypi==2022.9.5) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit-pypi==2022.9.5) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit-pypi==2022.9.5) (2024.2.0)\nrdkit-pypi is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import os\ndef is_interactive_session():\n    return os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Interactive'\n\nis_interactive_session()\n\nconfig = {\n    \"autogluon_time\": 60*60*0.2,\n    \"autogluon_presets\": \"best_quality\",\n    #\"reduce_features\": 0, # Set to >0 to use only the first n features\n    \"tail_rows\": 0 # Set to >0 to use only the last n rows in the file\n    \n}\n\nif is_interactive_session():\n    print(\"Interactive session\")\n    config[\"autogluon_time\"] = 100\n    #config[\"reduce_features\"] = 200\n    config[\"autogluon_presets\"] = \"medium_quality\"\n    config[\"tail_rows\"] = 2000\n    print(config)\nelse:\n    print(\"running as job\")\n    print(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:04.756566Z","iopub.execute_input":"2025-06-18T14:02:04.756848Z","iopub.status.idle":"2025-06-18T14:02:04.763274Z","shell.execute_reply.started":"2025-06-18T14:02:04.756819Z","shell.execute_reply":"2025-06-18T14:02:04.762751Z"}},"outputs":[{"name":"stdout","text":"Interactive session\n{'autogluon_time': 100, 'autogluon_presets': 'medium_quality', 'tail_rows': 2000}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import pandas as pd\nimport autogluon as ag\nimport numpy as np\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, rdMolDescriptors\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:04.765268Z","iopub.execute_input":"2025-06-18T14:02:04.765751Z","iopub.status.idle":"2025-06-18T14:02:04.778110Z","shell.execute_reply.started":"2025-06-18T14:02:04.765727Z","shell.execute_reply":"2025-06-18T14:02:04.777541Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Simple SMILES / RDKit Demo\n* So you can see how this works...","metadata":{}},{"cell_type":"code","source":"molecules = [\n    ('CCO', 'Ethanol - simple alcohol'),\n    ('CCCCCCCC', 'Octane - long chain'),\n    ('c1ccccc1', 'Benzene - aromatic ring'),\n    ('COO', 'CO2'),\n    (\"O\", \"Water\")\n\n]\n\nfor smiles, description in molecules:\n    mol = Chem.MolFromSmiles(smiles)\n    \n    print(f\"\\n{description}\")\n    print(f\"SMILES: {smiles}\")\n    print(f\"  Molecular Weight: {Descriptors.MolWt(mol):.1f}\")\n    print(f\"  LogP (oiliness): {Descriptors.MolLogP(mol):.2f}\")\n    print(f\"  Rotatable Bonds: {Descriptors.NumRotatableBonds(mol)}\")\n    print(f\"  Aromatic Rings: {Descriptors.NumAromaticRings(mol)}\")\n    print(f\"  Complexity (BertzCT): {Descriptors.BertzCT(mol):.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:04.778971Z","iopub.execute_input":"2025-06-18T14:02:04.779233Z","iopub.status.idle":"2025-06-18T14:02:04.799402Z","shell.execute_reply.started":"2025-06-18T14:02:04.779207Z","shell.execute_reply":"2025-06-18T14:02:04.798682Z"}},"outputs":[{"name":"stdout","text":"\nEthanol - simple alcohol\nSMILES: CCO\n  Molecular Weight: 46.1\n  LogP (oiliness): -0.00\n  Rotatable Bonds: 0\n  Aromatic Rings: 0\n  Complexity (BertzCT): 3\n\nOctane - long chain\nSMILES: CCCCCCCC\n  Molecular Weight: 114.2\n  LogP (oiliness): 3.37\n  Rotatable Bonds: 5\n  Aromatic Rings: 0\n  Complexity (BertzCT): 25\n\nBenzene - aromatic ring\nSMILES: c1ccccc1\n  Molecular Weight: 78.1\n  LogP (oiliness): 1.69\n  Rotatable Bonds: 0\n  Aromatic Rings: 1\n  Complexity (BertzCT): 72\n\nCO2\nSMILES: COO\n  Molecular Weight: 48.0\n  LogP (oiliness): 0.11\n  Rotatable Bonds: 0\n  Aromatic Rings: 0\n  Complexity (BertzCT): 3\n\nWater\nSMILES: O\n  Molecular Weight: 18.0\n  LogP (oiliness): -0.82\n  Rotatable Bonds: 0\n  Aromatic Rings: 0\n  Complexity (BertzCT): 0\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# Load data\ntrain_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\ntest_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:04.800666Z","iopub.execute_input":"2025-06-18T14:02:04.800974Z","iopub.status.idle":"2025-06-18T14:02:04.830814Z","shell.execute_reply.started":"2025-06-18T14:02:04.800957Z","shell.execute_reply":"2025-06-18T14:02:04.830239Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:04.831602Z","iopub.execute_input":"2025-06-18T14:02:04.831863Z","iopub.status.idle":"2025-06-18T14:02:04.843218Z","shell.execute_reply.started":"2025-06-18T14:02:04.831840Z","shell.execute_reply":"2025-06-18T14:02:04.842580Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"       id  \\\n0   87817   \n1  106919   \n2  388772   \n3  519416   \n4  539187   \n\n                                                                                                                                   SMILES  \\\n0                                                                                                              *CC(*)c1ccccc1C(=O)OCCCCCC   \n1                                                      *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5ccc(N*)cc5)cc4)CCC(CCCCC)CC3)cc2)cc1   \n2  *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(S(=O)(=O)c7ccc(Oc8ccc(C=C9CCCC(=Cc%10ccc(*)cc%10)C9=O)cc8)cc7)cc6)cc5)CCCCC4)cc3)cc2)cc1   \n3                                                         *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)c(-c3ccc(C)cc3)c2-c2ccc(C)cc2)cc1   \n4                  *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N+](=O)[O-])cc3)c(C(*)=O)cc2OCCCCCCCCCOCC2CCCN2c2ccc([N+](=O)[O-])cc2)cc1   \n\n   Tg       FFV        Tc  Density  Rg  \n0 NaN  0.374645  0.205667      NaN NaN  \n1 NaN  0.370410       NaN      NaN NaN  \n2 NaN  0.378860       NaN      NaN NaN  \n3 NaN  0.387324       NaN      NaN NaN  \n4 NaN  0.355470       NaN      NaN NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>SMILES</th>\n      <th>Tg</th>\n      <th>FFV</th>\n      <th>Tc</th>\n      <th>Density</th>\n      <th>Rg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>87817</td>\n      <td>*CC(*)c1ccccc1C(=O)OCCCCCC</td>\n      <td>NaN</td>\n      <td>0.374645</td>\n      <td>0.205667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>106919</td>\n      <td>*Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5ccc(N*)cc5)cc4)CCC(CCCCC)CC3)cc2)cc1</td>\n      <td>NaN</td>\n      <td>0.370410</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>388772</td>\n      <td>*Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(S(=O)(=O)c7ccc(Oc8ccc(C=C9CCCC(=Cc%10ccc(*)cc%10)C9=O)cc8)cc7)cc6)cc5)CCCCC4)cc3)cc2)cc1</td>\n      <td>NaN</td>\n      <td>0.378860</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>519416</td>\n      <td>*Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)c(-c3ccc(C)cc3)c2-c2ccc(C)cc2)cc1</td>\n      <td>NaN</td>\n      <td>0.387324</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>539187</td>\n      <td>*Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N+](=O)[O-])cc3)c(C(*)=O)cc2OCCCCCCCCCOCC2CCCN2c2ccc([N+](=O)[O-])cc2)cc1</td>\n      <td>NaN</td>\n      <td>0.355470</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"# Define molecular descriptions to be generated by RDKit\n* These are properties that RDKit can determine based on SMILES data\n* Auto-discovers 400 descriptors defined by RDKit\n* Only uses a subset of 192 AUTOCORR2D descriptors (as defined by max_autocorr) \n* We run this function on **train and test** data\n* We develop models to take this information and predict our actual targets  ('Tg', 'FFV', 'Tc', 'Density', 'Rg')","metadata":{}},{"cell_type":"code","source":"from rdkit.Chem import Descriptors\nfrom rdkit import Chem\nimport numpy as np\n\ndef get_molecular_descriptors(max_autocorr=10):\n    \"\"\"Get molecular descriptors - either hardcoded list or auto-discovered\"\"\"\n\n    descriptor_list_all = []\n    test_mol = Chem.MolFromSmiles('CCO')\n\n    # Collect all valid descriptors first\n    for name in dir(Descriptors):\n        if not name.startswith('_'):\n            try:\n                func = getattr(Descriptors, name)\n                if callable(func):\n                    result = func(test_mol)\n                    if isinstance(result, (int, float)) and not np.isnan(result):\n                        descriptor_list_all.append((name, func))\n            except:\n                pass\n\n    print(f\"ðŸ” Total discovered descriptors before filtering: {len(descriptor_list_all)}\")\n\n    # Sort AUTOCORR2D descriptors by their numeric suffix\n    autocorr_descriptors = [\n        (name, func)\n        for name, func in descriptor_list_all\n        if name.startswith('AUTOCORR2D_')\n    ]\n    autocorr_descriptors.sort(key=lambda x: int(x[0].split('_')[-1]))\n\n    # Select only the lowest-numbered ones\n    limited_autocorr = autocorr_descriptors[:max_autocorr]\n\n    # Include all other descriptors\n    other_descriptors = [\n        (name, func)\n        for name, func in descriptor_list_all\n        if not name.startswith('AUTOCORR2D_')\n    ]\n\n    # Final descriptor list\n    descriptor_list = limited_autocorr + other_descriptors\n\n    print(f\"âœ… Auto-discovered {len(descriptor_list)} descriptors (limited to {max_autocorr} AUTOCORR2D):\")\n    names = [name for name, _ in descriptor_list]\n    print(\"  \" + \", \".join(names))\n\n    feature_names = [name for name, _ in descriptor_list]\n    return descriptor_list, feature_names\n\nmolecular_descriptors =  get_molecular_descriptors(max_autocorr=10) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:04.844115Z","iopub.execute_input":"2025-06-18T14:02:04.844373Z","iopub.status.idle":"2025-06-18T14:02:04.862317Z","shell.execute_reply.started":"2025-06-18T14:02:04.844347Z","shell.execute_reply":"2025-06-18T14:02:04.861718Z"}},"outputs":[{"name":"stdout","text":"ðŸ” Total discovered descriptors before filtering: 400\nâœ… Auto-discovered 218 descriptors (limited to 10 AUTOCORR2D):\n  AUTOCORR2D_1, AUTOCORR2D_2, AUTOCORR2D_3, AUTOCORR2D_4, AUTOCORR2D_5, AUTOCORR2D_6, AUTOCORR2D_7, AUTOCORR2D_8, AUTOCORR2D_9, AUTOCORR2D_10, BCUT2D_CHGHI, BCUT2D_CHGLO, BCUT2D_LOGPHI, BCUT2D_LOGPLOW, BCUT2D_MRHI, BCUT2D_MRLOW, BCUT2D_MWHI, BCUT2D_MWLOW, BalabanJ, BertzCT, Chi0, Chi0n, Chi0v, Chi1, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, EState_VSA1, EState_VSA10, EState_VSA11, EState_VSA2, EState_VSA3, EState_VSA4, EState_VSA5, EState_VSA6, EState_VSA7, EState_VSA8, EState_VSA9, ExactMolWt, FpDensityMorgan1, FpDensityMorgan2, FpDensityMorgan3, FractionCSP3, HallKierAlpha, HeavyAtomCount, HeavyAtomMolWt, Ipc, Kappa1, Kappa2, Kappa3, LabuteASA, MaxAbsEStateIndex, MaxAbsPartialCharge, MaxEStateIndex, MaxPartialCharge, MinAbsEStateIndex, MinAbsPartialCharge, MinEStateIndex, MinPartialCharge, MolLogP, MolMR, MolWt, NHOHCount, NOCount, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumHAcceptors, NumHDonors, NumHeteroatoms, NumRadicalElectrons, NumRotatableBonds, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumValenceElectrons, PEOE_VSA1, PEOE_VSA10, PEOE_VSA11, PEOE_VSA12, PEOE_VSA13, PEOE_VSA14, PEOE_VSA2, PEOE_VSA3, PEOE_VSA4, PEOE_VSA5, PEOE_VSA6, PEOE_VSA7, PEOE_VSA8, PEOE_VSA9, RingCount, SMR_VSA1, SMR_VSA10, SMR_VSA2, SMR_VSA3, SMR_VSA4, SMR_VSA5, SMR_VSA6, SMR_VSA7, SMR_VSA8, SMR_VSA9, SlogP_VSA1, SlogP_VSA10, SlogP_VSA11, SlogP_VSA12, SlogP_VSA2, SlogP_VSA3, SlogP_VSA4, SlogP_VSA5, SlogP_VSA6, SlogP_VSA7, SlogP_VSA8, SlogP_VSA9, TPSA, VSA_EState1, VSA_EState10, VSA_EState2, VSA_EState3, VSA_EState4, VSA_EState5, VSA_EState6, VSA_EState7, VSA_EState8, VSA_EState9, fr_Al_COO, fr_Al_OH, fr_Al_OH_noTert, fr_ArN, fr_Ar_COO, fr_Ar_N, fr_Ar_NH, fr_Ar_OH, fr_COO, fr_COO2, fr_C_O, fr_C_O_noCOO, fr_C_S, fr_HOCCN, fr_Imine, fr_NH0, fr_NH1, fr_NH2, fr_N_O, fr_Ndealkylation1, fr_Ndealkylation2, fr_Nhpyrrole, fr_SH, fr_aldehyde, fr_alkyl_carbamate, fr_alkyl_halide, fr_allylic_oxid, fr_amide, fr_amidine, fr_aniline, fr_aryl_methyl, fr_azide, fr_azo, fr_barbitur, fr_benzene, fr_benzodiazepine, fr_bicyclic, fr_diazo, fr_dihydropyridine, fr_epoxide, fr_ester, fr_ether, fr_furan, fr_guanido, fr_halogen, fr_hdrzine, fr_hdrzone, fr_imidazole, fr_imide, fr_isocyan, fr_isothiocyan, fr_ketone, fr_ketone_Topliss, fr_lactam, fr_lactone, fr_methoxy, fr_morpholine, fr_nitrile, fr_nitro, fr_nitro_arom, fr_nitro_arom_nonortho, fr_nitroso, fr_oxazole, fr_oxime, fr_para_hydroxylation, fr_phenol, fr_phenol_noOrthoHbond, fr_phos_acid, fr_phos_ester, fr_piperdine, fr_piperzine, fr_priamide, fr_prisulfonamd, fr_pyridine, fr_quatN, fr_sulfide, fr_sulfonamd, fr_sulfone, fr_term_acetylene, fr_tetrazole, fr_thiazole, fr_thiocyan, fr_thiophene, fr_unbrch_alkane, fr_urea, qed\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Run RDKit on SMILES Train data\n* Predicts molecular descriptors we previously defined\n* This is time intensive - so we do it once here instead of in training loop\n* This function is also used to process Test data later","metadata":{}},{"cell_type":"code","source":"def smiles_to_features(smiles_list, descriptor_functions):\n   \"\"\"Convert SMILES strings to raw feature matrix\"\"\"\n   \n   features = []\n   total = len(smiles_list)\n   \n   print(f\"Processing {total} SMILES...\", end=\"\", flush=True)\n   \n   for i, smiles in enumerate(smiles_list):\n       # Progress indicator every 1000 molecules or at milestones\n       if i > 0 and (i % 1000 == 0 or i == total - 1):\n           print(f\" {i+1}/{total}\", end=\"\", flush=True)\n       \n       mol_features = []\n       try:\n           mol = Chem.MolFromSmiles(smiles)\n           if mol is None:\n               # Invalid SMILES - fill with NaN\n               mol_features = [np.nan] * len(descriptor_functions)\n           else:\n               # Calculate each descriptor\n               for name, func in descriptor_functions:\n                   try:\n                       value = func(mol)\n                       # Handle problematic values\n                       if np.isinf(value) or abs(value) > 1e10:\n                           value = np.nan\n                       mol_features.append(value)\n                   except:\n                       # Descriptor calculation failed\n                       mol_features.append(np.nan)\n       except:\n           # Complete failure - fill entire row with NaN\n           mol_features = [np.nan] * len(descriptor_functions)\n       \n       features.append(mol_features)\n   \n   print(\" âœ…\", flush=True)\n   return np.array(features, dtype=float)\n\ndescriptor_functions, feature_names = molecular_descriptors\nX_raw = smiles_to_features(train_df['SMILES'].values, descriptor_functions)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:04.863109Z","iopub.execute_input":"2025-06-18T14:02:04.863284Z","iopub.status.idle":"2025-06-18T14:04:18.783759Z","shell.execute_reply.started":"2025-06-18T14:02:04.863262Z","shell.execute_reply":"2025-06-18T14:04:18.782944Z"}},"outputs":[{"name":"stdout","text":"Processing 7973 SMILES... 1001/7973 2001/7973 3001/7973 4001/7973 5001/7973 6001/7973 7001/7973 7973/7973 âœ…\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Clean Train Data\n* Just replaces any NaNs with Median for the column\n* This function is also used to process Test data later","metadata":{}},{"cell_type":"code","source":"def clean_features(X):\n   \"\"\"Handle NaN/inf values and impute missing data\"\"\"\n   X[np.isinf(X)] = np.nan\n   \n   # Count and report missing values\n   missing = np.isnan(X).sum()\n   print(f\"ðŸ§¹ Cleaned {missing:,} missing values ({missing/X.size*100:.1f}%)\")\n   \n   # Median imputation\n   for i in range(X.shape[1]):\n       col = X[:, i]\n       if np.isnan(col).any():\n           X[np.isnan(col), i] = np.nanmedian(col) if not np.isnan(np.nanmedian(col)) else 0\n   \n   return X\n\ntrain_features = pd.DataFrame(clean_features(X_raw))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:04:18.786239Z","iopub.execute_input":"2025-06-18T14:04:18.786443Z","iopub.status.idle":"2025-06-18T14:04:18.825866Z","shell.execute_reply.started":"2025-06-18T14:04:18.786427Z","shell.execute_reply":"2025-06-18T14:04:18.825241Z"}},"outputs":[{"name":"stdout","text":"ðŸ§¹ Cleaned 96,915 missing values (5.6%)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_features.columns = feature_names\ntrain_features.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:04:18.826458Z","iopub.execute_input":"2025-06-18T14:04:18.826736Z","iopub.status.idle":"2025-06-18T14:04:18.846746Z","shell.execute_reply.started":"2025-06-18T14:04:18.826719Z","shell.execute_reply":"2025-06-18T14:04:18.845906Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"   AUTOCORR2D_1  AUTOCORR2D_2  AUTOCORR2D_3  AUTOCORR2D_4  AUTOCORR2D_5  \\\n0         2.944         3.126         3.178         3.120         2.890   \n1         3.919         4.229         4.352         4.361         4.374   \n2         4.631         4.955         5.035         4.908         4.892   \n3         3.878         4.244         4.357         4.435         4.670   \n4         4.429         4.697         4.734         4.718         4.653   \n\n   AUTOCORR2D_6  AUTOCORR2D_7  AUTOCORR2D_8  AUTOCORR2D_9  AUTOCORR2D_10  ...  \\\n0         2.539         2.428         2.335         2.842          2.978  ...   \n1         4.258         4.174         4.248         3.902          4.205  ...   \n2         4.814         4.697         4.754         4.393          4.724  ...   \n3         4.793         4.785         4.631         3.861          4.220  ...   \n4         4.617         4.660         4.629         4.227          4.468  ...   \n\n   fr_sulfonamd  fr_sulfone  fr_term_acetylene  fr_tetrazole  fr_thiazole  \\\n0           0.0         0.0                0.0           0.0          0.0   \n1           0.0         0.0                0.0           0.0          0.0   \n2           0.0         2.0                0.0           0.0          0.0   \n3           0.0         0.0                0.0           0.0          0.0   \n4           0.0         0.0                0.0           0.0          0.0   \n\n   fr_thiocyan  fr_thiophene  fr_unbrch_alkane  fr_urea       qed  \n0          0.0           0.0               3.0      0.0  0.500278  \n1          0.0           0.0               1.0      0.0  0.125364  \n2          0.0           0.0               0.0      0.0  0.092387  \n3          0.0           0.0               0.0      0.0  0.209590  \n4          0.0           0.0              18.0      0.0  0.014164  \n\n[5 rows x 218 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AUTOCORR2D_1</th>\n      <th>AUTOCORR2D_2</th>\n      <th>AUTOCORR2D_3</th>\n      <th>AUTOCORR2D_4</th>\n      <th>AUTOCORR2D_5</th>\n      <th>AUTOCORR2D_6</th>\n      <th>AUTOCORR2D_7</th>\n      <th>AUTOCORR2D_8</th>\n      <th>AUTOCORR2D_9</th>\n      <th>AUTOCORR2D_10</th>\n      <th>...</th>\n      <th>fr_sulfonamd</th>\n      <th>fr_sulfone</th>\n      <th>fr_term_acetylene</th>\n      <th>fr_tetrazole</th>\n      <th>fr_thiazole</th>\n      <th>fr_thiocyan</th>\n      <th>fr_thiophene</th>\n      <th>fr_unbrch_alkane</th>\n      <th>fr_urea</th>\n      <th>qed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.944</td>\n      <td>3.126</td>\n      <td>3.178</td>\n      <td>3.120</td>\n      <td>2.890</td>\n      <td>2.539</td>\n      <td>2.428</td>\n      <td>2.335</td>\n      <td>2.842</td>\n      <td>2.978</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.500278</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.919</td>\n      <td>4.229</td>\n      <td>4.352</td>\n      <td>4.361</td>\n      <td>4.374</td>\n      <td>4.258</td>\n      <td>4.174</td>\n      <td>4.248</td>\n      <td>3.902</td>\n      <td>4.205</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.125364</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.631</td>\n      <td>4.955</td>\n      <td>5.035</td>\n      <td>4.908</td>\n      <td>4.892</td>\n      <td>4.814</td>\n      <td>4.697</td>\n      <td>4.754</td>\n      <td>4.393</td>\n      <td>4.724</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.092387</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.878</td>\n      <td>4.244</td>\n      <td>4.357</td>\n      <td>4.435</td>\n      <td>4.670</td>\n      <td>4.793</td>\n      <td>4.785</td>\n      <td>4.631</td>\n      <td>3.861</td>\n      <td>4.220</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.209590</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.429</td>\n      <td>4.697</td>\n      <td>4.734</td>\n      <td>4.718</td>\n      <td>4.653</td>\n      <td>4.617</td>\n      <td>4.660</td>\n      <td>4.629</td>\n      <td>4.227</td>\n      <td>4.468</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.014164</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 218 columns</p>\n</div>"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":" # Function to Train AUTOGLUON\n for a given target\n * Runs RDKit feature generation on SMILES data\n * Creates / trains a model for a specific target ('Tg', 'FFV', 'Tc', 'Density', 'Rg')\n * Uses 5x cross-validation to utilize all training data (5 models per feature)","metadata":{}},{"cell_type":"code","source":"train_targets = train_df[['Tg', 'FFV', 'Tc', 'Density', 'Rg']]  # Y targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:04:18.847612Z","iopub.execute_input":"2025-06-18T14:04:18.847865Z","iopub.status.idle":"2025-06-18T14:04:18.861163Z","shell.execute_reply.started":"2025-06-18T14:04:18.847840Z","shell.execute_reply":"2025-06-18T14:04:18.860537Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from autogluon.tabular import TabularPredictor\n\ndef train_target_property_autogluon(X, train_df, target_name, time_limit=300, presets=\"best_quality\"):\n    \"\"\"\n    Trains an AutoGluon model to predict a single target property.\n\n    Returns:\n        predictor: Trained TabularPredictor.\n        scaler: None (for compatibility with legacy unpacking).\n        feature_names: List of feature names used.\n        best_model_score: MAE on internal validation.\n        leaderboard_df: AutoGluon leaderboard DataFrame.\n    \"\"\"\n    # Filter samples with target value\n    mask = train_df[target_name].notna()\n    X_target = X.loc[mask].copy()\n    y_target = train_df.loc[mask, target_name].copy()\n\n    print(f\"ðŸ“Š Training on {len(y_target)} samples with target = '{target_name}'\")\n    print(f\"ðŸ“ˆ Target range: {y_target.min():.4f} to {y_target.max():.4f}\")\n\n    # Prepare training data\n    train_data = X_target.copy()\n    train_data[target_name] = y_target\n    feature_names = list(X_target.columns)\n\n    # Train\n    predictor = TabularPredictor(label=target_name, eval_metric='mae').fit(\n        train_data,\n        time_limit=time_limit,\n        presets=presets\n    )\n\n    # Leaderboard\n    leaderboard_df = predictor.leaderboard(silent=True)\n    best_model_score = leaderboard_df.loc[0, 'score_val']\n    print(f\"âœ… Best AutoGluon model MAE: {best_model_score:.4f}\")\n\n    return predictor, None, feature_names, best_model_score, leaderboard_df\n\n# Define all target properties\ntargets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# Store trained models and scalers\ntrained_models = {}\ntrained_scalers = {}  # will remain None\ncv_scores = []\nleaderboards = {}\n\nfor target in targets:\n    print(f\"Training {target}...\")\n    model, scaler, features, cv_score, lb = train_target_property_autogluon(\n        train_features, train_df, target,\n        time_limit=config[\"autogluon_time\"],\n        presets=config[\"autogluon_presets\"],\n        hyperparameters={\n        'GBM': {}, 'CAT': {}, 'RF': {}, 'XT': {}, # Removing XGboost due to incompatibility with scikit's version\n        'NN_TORCH': {}, 'FASTAI': {}, 'VW': {}, 'KNN': {},\n    }\n    )\n    trained_models[target] = model\n    trained_scalers[target] = scaler  # remains None\n    cv_scores.append(cv_score)\n    leaderboards[target] = lb\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:26:05.356800Z","iopub.execute_input":"2025-06-18T14:26:05.357050Z","iopub.status.idle":"2025-06-18T14:31:03.877561Z","shell.execute_reply.started":"2025-06-18T14:26:05.357034Z","shell.execute_reply":"2025-06-18T14:31:03.876954Z"}},"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20250618_142605\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.3.1\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.35 GB / 31.35 GB (93.6%)\nDisk Space Avail:   19.36 GB / 19.52 GB (99.2%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ... Time limit = 100s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250618_142605\"\nTrain Data Rows:    511\nTrain Data Columns: 218\nLabel Column:       Tg\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n\tLabel info (max, min, mean, stddev): (472.25, -148.0297376, 96.45231, 111.22828)\n\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30059.21 MB\n\tTrain Data (Original)  Memory Usage: 0.85 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 13 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n","output_type":"stream"},{"name":"stdout","text":"Training Tg...\nðŸ“Š Training on 511 samples with target = 'Tg'\nðŸ“ˆ Target range: -148.0297 to 472.2500\n","output_type":"stream"},{"name":"stderr","text":"\tUseless Original Features (Count: 32): ['BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'NumRadicalElectrons', 'SMR_VSA8', 'SlogP_VSA9', 'fr_HOCCN', 'fr_SH', 'fr_aldehyde', 'fr_barbitur', 'fr_benzodiazepine', 'fr_dihydropyridine', 'fr_epoxide', 'fr_guanido', 'fr_isocyan', 'fr_isothiocyan', 'fr_lactam', 'fr_morpholine', 'fr_nitroso', 'fr_oxime', 'fr_phos_acid', 'fr_phos_ester', 'fr_priamide', 'fr_prisulfonamd', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiocyan']\n\t\tThese features carry no predictive signal and should be manually investigated.\n\t\tThis is typically a feature which has the same value for all rows.\n\t\tThese features do not need to be present at inference time.\n\tUnused Original Features (Count: 9): ['MaxEStateIndex', 'fr_Al_OH_noTert', 'fr_COO2', 'fr_NH2', 'fr_Nhpyrrole', 'fr_benzene', 'fr_diazo', 'fr_phenol', 'fr_phenol_noOrthoHbond']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('float', []) : 9 | ['MaxEStateIndex', 'fr_Al_OH_noTert', 'fr_COO2', 'fr_NH2', 'fr_Nhpyrrole', ...]\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) : 177 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', [])     : 165 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\t\t('int', ['bool']) :  12 | ['fr_ArN', 'fr_C_S', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', ...]\n\t0.9s = Fit runtime\n\t177 features in original data used to generate 177 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.65 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.96s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 408, Val Rows: 103\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif ... Training model for up to 99.04s of the 99.04s of remaining time.\n\t-86.8898\t = Validation score   (-mean_absolute_error)\n\t0.02s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: KNeighborsDist ... Training model for up to 99.01s of the 99.01s of remaining time.\n\t-89.3356\t = Validation score   (-mean_absolute_error)\n\t0.03s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBMXT ... Training model for up to 98.98s of the 98.98s of remaining time.\n\t-52.1864\t = Validation score   (-mean_absolute_error)\n\t0.63s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 98.33s of the 98.33s of remaining time.\n\t-52.1925\t = Validation score   (-mean_absolute_error)\n\t0.91s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: RandomForestMSE ... Training model for up to 97.40s of the 97.40s of remaining time.\n\t-53.5574\t = Validation score   (-mean_absolute_error)\n\t2.76s\t = Training   runtime\n\t0.08s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 94.54s of the 94.54s of remaining time.\n\t-54.3592\t = Validation score   (-mean_absolute_error)\n\t4.84s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: ExtraTreesMSE ... Training model for up to 89.69s of the 89.69s of remaining time.\n\t-55.5089\t = Validation score   (-mean_absolute_error)\n\t1.07s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 88.51s of the 88.51s of remaining time.\n\t-57.3793\t = Validation score   (-mean_absolute_error)\n\t0.55s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 87.94s of the 87.94s of remaining time.\n\t-50.0743\t = Validation score   (-mean_absolute_error)\n\t2.93s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 85.00s of the 85.00s of remaining time.\n\t-50.3149\t = Validation score   (-mean_absolute_error)\n\t1.92s\t = Training   runtime\n\t0.07s\t = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 83.01s of the 83.01s of remaining time.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l1: 55.4612\n[2000]\tvalid_set's l1: 55.461\n","output_type":"stream"},{"name":"stderr","text":"\t-55.461\t = Validation score   (-mean_absolute_error)\n\t21.68s\t = Training   runtime\n\t0.03s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 99.04s of the 60.80s of remaining time.\n\tEnsemble Weights: {'XGBoost': 0.412, 'NeuralNetTorch': 0.412, 'NeuralNetFastAI': 0.176}\n\t-47.4217\t = Validation score   (-mean_absolute_error)\n\t0.08s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 39.31s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1331.8 rows/s (103 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250618_142605\")\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20250618_142644\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.3.1\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.32 GB / 31.35 GB (93.5%)\nDisk Space Avail:   19.32 GB / 19.52 GB (99.0%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ... Time limit = 100s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250618_142644\"\nTrain Data Rows:    7030\nTrain Data Columns: 218\nLabel Column:       FFV\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n\tLabel info (max, min, mean, stddev): (0.77709707, 0.2269924, 0.36721, 0.02961)\n\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30002.02 MB\n\tTrain Data (Original)  Memory Usage: 11.69 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 11 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n","output_type":"stream"},{"name":"stdout","text":"âœ… Best AutoGluon model MAE: -47.4217\n\nTraining FFV...\nðŸ“Š Training on 7030 samples with target = 'FFV'\nðŸ“ˆ Target range: 0.2270 to 0.7771\n","output_type":"stream"},{"name":"stderr","text":"\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tUseless Original Features (Count: 22): ['BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'NumRadicalElectrons', 'SMR_VSA8', 'SlogP_VSA9', 'fr_azide', 'fr_barbitur', 'fr_benzodiazepine', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_isothiocyan', 'fr_lactam', 'fr_nitroso', 'fr_prisulfonamd', 'fr_thiocyan']\n\t\tThese features carry no predictive signal and should be manually investigated.\n\t\tThis is typically a feature which has the same value for all rows.\n\t\tThese features do not need to be present at inference time.\n\tUnused Original Features (Count: 10): ['fr_COO2', 'fr_Nhpyrrole', 'fr_aldehyde', 'fr_benzene', 'fr_hdrzone', 'fr_morpholine', 'fr_nitro_arom', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_ester']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('float', []) : 10 | ['fr_COO2', 'fr_Nhpyrrole', 'fr_aldehyde', 'fr_benzene', 'fr_hdrzone', ...]\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) : 186 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', [])     : 177 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\t\t('int', ['bool']) :   9 | ['fr_HOCCN', 'fr_SH', 'fr_isocyan', 'fr_phos_acid', 'fr_piperzine', ...]\n\t2.7s = Fit runtime\n\t186 features in original data used to generate 186 features in processed data.\n\tTrain Data (Processed) Memory Usage: 9.55 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 2.78s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.1, Train Rows: 6327, Val Rows: 703\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif ... Training model for up to 97.22s of the 97.21s of remaining time.\n\t-0.0181\t = Validation score   (-mean_absolute_error)\n\t0.05s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitting model: KNeighborsDist ... Training model for up to 97.14s of the 97.13s of remaining time.\n\t-0.0177\t = Validation score   (-mean_absolute_error)\n\t0.05s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitting model: LightGBMXT ... Training model for up to 97.06s of the 97.06s of remaining time.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l1: 0.00547555\n[2000]\tvalid_set's l1: 0.00522952\n[3000]\tvalid_set's l1: 0.00513289\n[4000]\tvalid_set's l1: 0.00510065\n[5000]\tvalid_set's l1: 0.00508986\n[6000]\tvalid_set's l1: 0.00508876\n[7000]\tvalid_set's l1: 0.00508824\n[8000]\tvalid_set's l1: 0.00508363\n[9000]\tvalid_set's l1: 0.00508133\n[10000]\tvalid_set's l1: 0.00508124\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0051\t = Validation score   (-mean_absolute_error)\n\t36.82s\t = Training   runtime\n\t0.48s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 58.99s of the 58.99s of remaining time.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l1: 0.00579225\n[2000]\tvalid_set's l1: 0.00567151\n[3000]\tvalid_set's l1: 0.005655\n[4000]\tvalid_set's l1: 0.00564348\n[5000]\tvalid_set's l1: 0.00564302\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0056\t = Validation score   (-mean_absolute_error)\n\t33.48s\t = Training   runtime\n\t0.22s\t = Validation runtime\nFitting model: RandomForestMSE ... Training model for up to 25.03s of the 25.02s of remaining time.\n\t-0.0066\t = Validation score   (-mean_absolute_error)\n\t56.86s\t = Training   runtime\n\t0.11s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 97.22s of the -32.20s of remaining time.\n\tEnsemble Weights: {'LightGBMXT': 0.909, 'RandomForestMSE': 0.091}\n\t-0.0051\t = Validation score   (-mean_absolute_error)\n\t0.03s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 132.28s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1182.3 rows/s (703 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250618_142644\")\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20250618_142857\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.3.1\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.16 GB / 31.35 GB (93.0%)\nDisk Space Avail:   19.09 GB / 19.52 GB (97.8%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ... Time limit = 100s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250618_142857\"\nTrain Data Rows:    737\nTrain Data Columns: 218\nLabel Column:       Tc\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n\tLabel info (max, min, mean, stddev): (0.524, 0.0465, 0.25633, 0.08954)\n\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29859.82 MB\n\tTrain Data (Original)  Memory Usage: 1.23 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 31 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n","output_type":"stream"},{"name":"stdout","text":"âœ… Best AutoGluon model MAE: -0.0051\n\nTraining Tc...\nðŸ“Š Training on 737 samples with target = 'Tc'\nðŸ“ˆ Target range: 0.0465 to 0.5240\n","output_type":"stream"},{"name":"stderr","text":"\tUseless Original Features (Count: 38): ['BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'NumRadicalElectrons', 'PEOE_VSA5', 'SMR_VSA8', 'SlogP_VSA9', 'fr_ArN', 'fr_C_S', 'fr_HOCCN', 'fr_SH', 'fr_azide', 'fr_barbitur', 'fr_benzodiazepine', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_guanido', 'fr_hdrzine', 'fr_hdrzone', 'fr_isocyan', 'fr_isothiocyan', 'fr_lactam', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperzine', 'fr_prisulfonamd', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiocyan']\n\t\tThese features carry no predictive signal and should be manually investigated.\n\t\tThis is typically a feature which has the same value for all rows.\n\t\tThese features do not need to be present at inference time.\n\tUnused Original Features (Count: 8): ['MaxEStateIndex', 'fr_COO2', 'fr_Nhpyrrole', 'fr_benzene', 'fr_nitro_arom_nonortho', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_priamide']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('float', []) : 8 | ['MaxEStateIndex', 'fr_COO2', 'fr_Nhpyrrole', 'fr_benzene', 'fr_nitro_arom_nonortho', ...]\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) : 172 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', [])     : 144 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\t\t('int', ['bool']) :  28 | ['EState_VSA11', 'NumSaturatedHeterocycles', 'PEOE_VSA11', 'fr_Al_COO', 'fr_Al_OH', ...]\n\t0.9s = Fit runtime\n\t172 features in original data used to generate 172 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.83 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.89s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 589, Val Rows: 148\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif ... Training model for up to 99.11s of the 99.11s of remaining time.\n\t-0.0481\t = Validation score   (-mean_absolute_error)\n\t0.03s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: KNeighborsDist ... Training model for up to 99.07s of the 99.07s of remaining time.\n\t-0.0483\t = Validation score   (-mean_absolute_error)\n\t0.02s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBMXT ... Training model for up to 99.04s of the 99.04s of remaining time.\n\t-0.0275\t = Validation score   (-mean_absolute_error)\n\t0.73s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 98.30s of the 98.30s of remaining time.\n\t-0.0277\t = Validation score   (-mean_absolute_error)\n\t1.11s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: RandomForestMSE ... Training model for up to 97.18s of the 97.18s of remaining time.\n\t-0.0288\t = Validation score   (-mean_absolute_error)\n\t3.39s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 93.67s of the 93.67s of remaining time.\n\t-0.0295\t = Validation score   (-mean_absolute_error)\n\t7.77s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: ExtraTreesMSE ... Training model for up to 85.89s of the 85.89s of remaining time.\n\t-0.0286\t = Validation score   (-mean_absolute_error)\n\t1.36s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 84.42s of the 84.41s of remaining time.\n\t-0.0305\t = Validation score   (-mean_absolute_error)\n\t0.77s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 83.63s of the 83.63s of remaining time.\n\t-0.0321\t = Validation score   (-mean_absolute_error)\n\t1.87s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 81.74s of the 81.74s of remaining time.\n\t-0.0292\t = Validation score   (-mean_absolute_error)\n\t7.09s\t = Training   runtime\n\t0.06s\t = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 74.59s of the 74.59s of remaining time.\n\t-0.0334\t = Validation score   (-mean_absolute_error)\n\t6.22s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 99.11s of the 68.31s of remaining time.\n\tEnsemble Weights: {'LightGBMXT': 0.55, 'NeuralNetFastAI': 0.25, 'NeuralNetTorch': 0.15, 'RandomForestMSE': 0.05}\n\t-0.0264\t = Validation score   (-mean_absolute_error)\n\t0.08s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 31.8s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 941.4 rows/s (148 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250618_142857\")\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20250618_142928\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.3.1\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.16 GB / 31.35 GB (93.0%)\nDisk Space Avail:   19.05 GB / 19.52 GB (97.6%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ... Time limit = 100s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250618_142928\"\nTrain Data Rows:    613\nTrain Data Columns: 218\nLabel Column:       Density\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n\tLabel info (max, min, mean, stddev): (1.840998909, 0.748691234, 0.98548, 0.14619)\n\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29855.85 MB\n\tTrain Data (Original)  Memory Usage: 1.02 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 32 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n","output_type":"stream"},{"name":"stdout","text":"âœ… Best AutoGluon model MAE: -0.0264\n\nTraining Density...\nðŸ“Š Training on 613 samples with target = 'Density'\nðŸ“ˆ Target range: 0.7487 to 1.8410\n","output_type":"stream"},{"name":"stderr","text":"\tUseless Original Features (Count: 37): ['BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'NumRadicalElectrons', 'PEOE_VSA5', 'SMR_VSA8', 'SlogP_VSA9', 'fr_ArN', 'fr_C_S', 'fr_HOCCN', 'fr_SH', 'fr_azide', 'fr_barbitur', 'fr_benzodiazepine', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_guanido', 'fr_hdrzone', 'fr_isocyan', 'fr_isothiocyan', 'fr_lactam', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperzine', 'fr_prisulfonamd', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiocyan']\n\t\tThese features carry no predictive signal and should be manually investigated.\n\t\tThis is typically a feature which has the same value for all rows.\n\t\tThese features do not need to be present at inference time.\n\tUnused Original Features (Count: 8): ['MaxEStateIndex', 'fr_COO2', 'fr_Nhpyrrole', 'fr_benzene', 'fr_nitro_arom_nonortho', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_priamide']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('float', []) : 8 | ['MaxEStateIndex', 'fr_COO2', 'fr_Nhpyrrole', 'fr_benzene', 'fr_nitro_arom_nonortho', ...]\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) : 173 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', [])     : 144 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\t\t('int', ['bool']) :  29 | ['EState_VSA11', 'NumSaturatedHeterocycles', 'PEOE_VSA11', 'fr_Al_COO', 'fr_Al_OH', ...]\n\t1.1s = Fit runtime\n\t173 features in original data used to generate 173 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.69 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 1.14s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 490, Val Rows: 123\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif ... Training model for up to 98.86s of the 98.86s of remaining time.\n\t-0.1191\t = Validation score   (-mean_absolute_error)\n\t0.02s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: KNeighborsDist ... Training model for up to 98.83s of the 98.83s of remaining time.\n\t-0.1128\t = Validation score   (-mean_absolute_error)\n\t0.02s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBMXT ... Training model for up to 98.80s of the 98.80s of remaining time.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l1: 0.0324844\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0324\t = Validation score   (-mean_absolute_error)\n\t1.12s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 97.63s of the 97.63s of remaining time.\n\t-0.0388\t = Validation score   (-mean_absolute_error)\n\t0.94s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: RandomForestMSE ... Training model for up to 96.69s of the 96.68s of remaining time.\n\t-0.0402\t = Validation score   (-mean_absolute_error)\n\t2.75s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 93.82s of the 93.81s of remaining time.\n\t-0.0356\t = Validation score   (-mean_absolute_error)\n\t48.59s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: ExtraTreesMSE ... Training model for up to 45.20s of the 45.20s of remaining time.\n\t-0.04\t = Validation score   (-mean_absolute_error)\n\t1.14s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 43.95s of the 43.95s of remaining time.\n\t-0.0343\t = Validation score   (-mean_absolute_error)\n\t0.63s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 43.29s of the 43.29s of remaining time.\n\t-0.0414\t = Validation score   (-mean_absolute_error)\n\t1.69s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 41.59s of the 41.59s of remaining time.\n\t-0.0355\t = Validation score   (-mean_absolute_error)\n\t3.44s\t = Training   runtime\n\t0.06s\t = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 38.09s of the 38.09s of remaining time.\n\t-0.0417\t = Validation score   (-mean_absolute_error)\n\t5.33s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 98.86s of the 32.64s of remaining time.\n\tEnsemble Weights: {'LightGBMXT': 0.35, 'NeuralNetFastAI': 0.35, 'CatBoost': 0.15, 'NeuralNetTorch': 0.15}\n\t-0.0271\t = Validation score   (-mean_absolute_error)\n\t0.08s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 67.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1539.5 rows/s (123 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250618_142928\")\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20250618_143036\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.3.1\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.12 GB / 31.35 GB (92.9%)\nDisk Space Avail:   19.01 GB / 19.52 GB (97.4%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ... Time limit = 100s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250618_143036\"\nTrain Data Rows:    614\nTrain Data Columns: 218\nLabel Column:       Rg\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n\tLabel info (max, min, mean, stddev): (34.672905605, 9.7283551, 16.41979, 4.60864)\n\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29822.77 MB\n\tTrain Data (Original)  Memory Usage: 1.02 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 31 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n","output_type":"stream"},{"name":"stdout","text":"âœ… Best AutoGluon model MAE: -0.0271\n\nTraining Rg...\nðŸ“Š Training on 614 samples with target = 'Rg'\nðŸ“ˆ Target range: 9.7284 to 34.6729\n","output_type":"stream"},{"name":"stderr","text":"\tUseless Original Features (Count: 38): ['BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'NumRadicalElectrons', 'PEOE_VSA5', 'SMR_VSA8', 'SlogP_VSA9', 'fr_ArN', 'fr_C_S', 'fr_HOCCN', 'fr_SH', 'fr_azide', 'fr_barbitur', 'fr_benzodiazepine', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_guanido', 'fr_hdrzine', 'fr_hdrzone', 'fr_isocyan', 'fr_isothiocyan', 'fr_lactam', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperzine', 'fr_prisulfonamd', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiocyan']\n\t\tThese features carry no predictive signal and should be manually investigated.\n\t\tThis is typically a feature which has the same value for all rows.\n\t\tThese features do not need to be present at inference time.\n\tUnused Original Features (Count: 8): ['MaxEStateIndex', 'fr_COO2', 'fr_Nhpyrrole', 'fr_benzene', 'fr_nitro_arom_nonortho', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_priamide']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('float', []) : 8 | ['MaxEStateIndex', 'fr_COO2', 'fr_Nhpyrrole', 'fr_benzene', 'fr_nitro_arom_nonortho', ...]\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) : 172 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', [])     : 144 | ['AUTOCORR2D_1', 'AUTOCORR2D_2', 'AUTOCORR2D_3', 'AUTOCORR2D_4', 'AUTOCORR2D_5', ...]\n\t\t('int', ['bool']) :  28 | ['EState_VSA11', 'NumSaturatedHeterocycles', 'PEOE_VSA11', 'fr_Al_COO', 'fr_Al_OH', ...]\n\t0.8s = Fit runtime\n\t172 features in original data used to generate 172 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.69 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.84s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 491, Val Rows: 123\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif ... Training model for up to 99.16s of the 99.15s of remaining time.\n\t-3.2008\t = Validation score   (-mean_absolute_error)\n\t0.02s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: KNeighborsDist ... Training model for up to 99.12s of the 99.12s of remaining time.\n\t-3.166\t = Validation score   (-mean_absolute_error)\n\t0.02s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBMXT ... Training model for up to 99.09s of the 99.09s of remaining time.\n\t-1.9017\t = Validation score   (-mean_absolute_error)\n\t0.95s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 98.12s of the 98.12s of remaining time.\n\t-1.9492\t = Validation score   (-mean_absolute_error)\n\t1.07s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: RandomForestMSE ... Training model for up to 97.03s of the 97.03s of remaining time.\n\t-1.8426\t = Validation score   (-mean_absolute_error)\n\t2.79s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 94.13s of the 94.13s of remaining time.\n\t-1.7785\t = Validation score   (-mean_absolute_error)\n\t7.03s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: ExtraTreesMSE ... Training model for up to 87.09s of the 87.09s of remaining time.\n\t-1.8987\t = Validation score   (-mean_absolute_error)\n\t1.17s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 85.80s of the 85.80s of remaining time.\n\t-2.0408\t = Validation score   (-mean_absolute_error)\n\t0.55s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 85.23s of the 85.23s of remaining time.\n\t-1.9616\t = Validation score   (-mean_absolute_error)\n\t3.46s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 81.75s of the 81.75s of remaining time.\n\t-1.8551\t = Validation score   (-mean_absolute_error)\n\t5.09s\t = Training   runtime\n\t0.06s\t = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 76.58s of the 76.58s of remaining time.\n\t-2.0265\t = Validation score   (-mean_absolute_error)\n\t3.97s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 99.16s of the 72.58s of remaining time.\n\tEnsemble Weights: {'CatBoost': 0.435, 'NeuralNetTorch': 0.435, 'RandomForestMSE': 0.087, 'NeuralNetFastAI': 0.043}\n\t-1.7163\t = Validation score   (-mean_absolute_error)\n\t0.08s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 27.53s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 736.4 rows/s (123 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250618_143036\")\n","output_type":"stream"},{"name":"stdout","text":"âœ… Best AutoGluon model MAE: -1.7163\n\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor target, lb in leaderboards.items():\n    plt.figure()\n    plt.title(f\"Leaderboard: {target}\")\n    plt.bar(lb['model'], lb['score_val'])\n    plt.xticks(rotation=90)\n    plt.ylabel(\"MAE\")\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Perform Training\n* Loops through all 5 polymer target properties (Tg, FFV, Tc, Density, Rg)\n* Trains LGBM models for each target","metadata":{}},{"cell_type":"code","source":"# # Define all target properties\n# targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# # Store trained models and scalers\n# trained_models = {}\n# trained_scalers = {}\n# cv_scores = []\n\n# # Train each target - collect results for summary\n# for target in targets:\n#     print(f\"Training {target}...\")\n#     models, scaler, features, cv_score = train_target_property_autogluon(train_features, train_df, target)\n#     trained_models[target] = models\n#     trained_scalers[target] = scaler\n#     cv_scores.append(cv_score)\n#     print()\n\n# # Clean summary with average\n# print(\"=\" * 40)\n# print(f\"Trained: {len(targets)} targets Ã— 5 CV folds = {len(targets) * 5} models\")\n# print(f\"Average CV MAE across all targets: {np.mean(cv_scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:04:58.791942Z","iopub.status.idle":"2025-06-18T14:04:58.792167Z","shell.execute_reply.started":"2025-06-18T14:04:58.792056Z","shell.execute_reply":"2025-06-18T14:04:58.792066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function to Predict using trained LGBM models for a given target\n* Runs same RDKit feature generation on test SMILES data\n* Uses the 5 trained models to predict a specific target\n* Averages predictions from all 5 models for final result","metadata":{}},{"cell_type":"code","source":"def predict_target_property_autogluon(test_df, target_name, predictor):\n    print(f\"PREDICTING: {target_name}\")\n    \n    if predictor is None:\n        print(f\"âŒ No trained predictor available for {target_name}, returning zeros\")\n        return np.zeros(len(test_df))\n    \n    # Make sure test_df is processed to match training features\n    descriptor_functions, _ = molecular_descriptors\n    X_raw = smiles_to_features(test_df['SMILES'].values, descriptor_functions)\n    X = clean_features(X_raw)\n    \n    # AutoGluon works directly with DataFrames\n    predictions = predictor.predict(X).values\n    print(f\"ðŸ“Š Predictions range: {predictions.min():.4f} to {predictions.max():.4f}\")\n    \n    return predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:04:58.793715Z","iopub.status.idle":"2025-06-18T14:04:58.794052Z","shell.execute_reply.started":"2025-06-18T14:04:58.793871Z","shell.execute_reply":"2025-06-18T14:04:58.793886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict All Targets / Submit\n* Predicts on test data\n* Creates final submission CSV with all predictions","metadata":{}},{"cell_type":"code","source":"print(f\"\\nMAKING PREDICTIONS...\")\nall_predictions = {}\nfor target in targets:\n    predictions = predict_target_property_autogluon(\n        test_df, target, trained_models[target]\n    )\n    all_predictions[target] = predictions\n\n\n# Create submission\nsubmission = pd.DataFrame({'id': test_df['id']})\nfor target in targets:\n    submission[target] = all_predictions[target]\n\nsubmission.to_csv('submission.csv', index=False)\n\nprint(f\"Predicted: {len(test_df)} test samples\")\nprint(f\"Saved: submission.csv\")\n\nprint(f\"\\nðŸ‘€ SUBMISSION PREVIEW:\")\nprint(submission.head().to_string(index=False, float_format='%.4f'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:04:58.795239Z","iopub.status.idle":"2025-06-18T14:04:58.795586Z","shell.execute_reply.started":"2025-06-18T14:04:58.795393Z","shell.execute_reply":"2025-06-18T14:04:58.795408Z"}},"outputs":[],"execution_count":null}]}