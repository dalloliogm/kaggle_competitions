{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4af31dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T14:50:18.427215Z",
     "iopub.status.busy": "2025-10-05T14:50:18.426850Z",
     "iopub.status.idle": "2025-10-05T15:18:23.127786Z",
     "shell.execute_reply": "2025-10-05T15:18:23.126573Z"
    },
    "papermill": {
     "duration": 1684.709203,
     "end_time": "2025-10-05T15:18:23.131582",
     "exception": false,
     "start_time": "2025-10-05T14:50:18.422379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training LGBM]\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.654195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 142274\n",
      "[LightGBM] [Info] Number of data points in the train set: 665171, number of used features: 565\n",
      "[LightGBM] [Info] Start training from score -0.000061\n",
      "[1]\tvalid_0's rmse: 0.0300154\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's rmse: 0.0300153\n",
      "[3]\tvalid_0's rmse: 0.0300152\n",
      "[4]\tvalid_0's rmse: 0.0300152\n",
      "[5]\tvalid_0's rmse: 0.0300151\n",
      "[6]\tvalid_0's rmse: 0.0300151\n",
      "[7]\tvalid_0's rmse: 0.030015\n",
      "[8]\tvalid_0's rmse: 0.030015\n",
      "[9]\tvalid_0's rmse: 0.0300149\n",
      "[10]\tvalid_0's rmse: 0.0300149\n",
      "[11]\tvalid_0's rmse: 0.0300148\n",
      "[12]\tvalid_0's rmse: 0.0300148\n",
      "[13]\tvalid_0's rmse: 0.0300148\n",
      "[14]\tvalid_0's rmse: 0.0300148\n",
      "[15]\tvalid_0's rmse: 0.0300147\n",
      "[16]\tvalid_0's rmse: 0.0300148\n",
      "[17]\tvalid_0's rmse: 0.0300147\n",
      "[18]\tvalid_0's rmse: 0.0300147\n",
      "[19]\tvalid_0's rmse: 0.0300147\n",
      "[20]\tvalid_0's rmse: 0.0300147\n",
      "[21]\tvalid_0's rmse: 0.0300146\n",
      "[22]\tvalid_0's rmse: 0.0300147\n",
      "[23]\tvalid_0's rmse: 0.0300146\n",
      "[24]\tvalid_0's rmse: 0.0300147\n",
      "[25]\tvalid_0's rmse: 0.0300146\n",
      "[26]\tvalid_0's rmse: 0.0300146\n",
      "[27]\tvalid_0's rmse: 0.0300146\n",
      "[28]\tvalid_0's rmse: 0.0300146\n",
      "[29]\tvalid_0's rmse: 0.0300146\n",
      "[30]\tvalid_0's rmse: 0.0300146\n",
      "[31]\tvalid_0's rmse: 0.0300146\n",
      "[32]\tvalid_0's rmse: 0.0300146\n",
      "[33]\tvalid_0's rmse: 0.0300146\n",
      "[34]\tvalid_0's rmse: 0.0300146\n",
      "[35]\tvalid_0's rmse: 0.0300146\n",
      "[36]\tvalid_0's rmse: 0.0300146\n",
      "[37]\tvalid_0's rmse: 0.0300146\n",
      "[38]\tvalid_0's rmse: 0.0300146\n",
      "[39]\tvalid_0's rmse: 0.0300146\n",
      "[40]\tvalid_0's rmse: 0.0300146\n",
      "[41]\tvalid_0's rmse: 0.0300146\n",
      "[42]\tvalid_0's rmse: 0.0300146\n",
      "[43]\tvalid_0's rmse: 0.0300146\n",
      "[44]\tvalid_0's rmse: 0.0300146\n",
      "[45]\tvalid_0's rmse: 0.0300146\n",
      "[46]\tvalid_0's rmse: 0.0300146\n",
      "[47]\tvalid_0's rmse: 0.0300146\n",
      "[48]\tvalid_0's rmse: 0.0300147\n",
      "[49]\tvalid_0's rmse: 0.0300146\n",
      "[50]\tvalid_0's rmse: 0.0300147\n",
      "[51]\tvalid_0's rmse: 0.0300147\n",
      "[52]\tvalid_0's rmse: 0.0300147\n",
      "[53]\tvalid_0's rmse: 0.0300148\n",
      "[54]\tvalid_0's rmse: 0.0300147\n",
      "[55]\tvalid_0's rmse: 0.0300147\n",
      "[56]\tvalid_0's rmse: 0.0300147\n",
      "[57]\tvalid_0's rmse: 0.0300148\n",
      "[58]\tvalid_0's rmse: 0.0300147\n",
      "[59]\tvalid_0's rmse: 0.0300148\n",
      "[60]\tvalid_0's rmse: 0.0300148\n",
      "[61]\tvalid_0's rmse: 0.0300147\n",
      "[62]\tvalid_0's rmse: 0.0300147\n",
      "[63]\tvalid_0's rmse: 0.0300147\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's rmse: 0.0300146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.263041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 142273\n",
      "[LightGBM] [Info] Number of data points in the train set: 665171, number of used features: 565\n",
      "[LightGBM] [Info] Start training from score -0.000035\n",
      "[1]\tvalid_0's rmse: 0.029864\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's rmse: 0.0298638\n",
      "[3]\tvalid_0's rmse: 0.0298636\n",
      "[4]\tvalid_0's rmse: 0.0298634\n",
      "[5]\tvalid_0's rmse: 0.0298632\n",
      "[6]\tvalid_0's rmse: 0.029863\n",
      "[7]\tvalid_0's rmse: 0.0298628\n",
      "[8]\tvalid_0's rmse: 0.0298626\n",
      "[9]\tvalid_0's rmse: 0.0298625\n",
      "[10]\tvalid_0's rmse: 0.0298623\n",
      "[11]\tvalid_0's rmse: 0.0298622\n",
      "[12]\tvalid_0's rmse: 0.0298621\n",
      "[13]\tvalid_0's rmse: 0.0298619\n",
      "[14]\tvalid_0's rmse: 0.0298618\n",
      "[15]\tvalid_0's rmse: 0.0298617\n",
      "[16]\tvalid_0's rmse: 0.0298616\n",
      "[17]\tvalid_0's rmse: 0.0298614\n",
      "[18]\tvalid_0's rmse: 0.0298613\n",
      "[19]\tvalid_0's rmse: 0.0298612\n",
      "[20]\tvalid_0's rmse: 0.0298611\n",
      "[21]\tvalid_0's rmse: 0.0298609\n",
      "[22]\tvalid_0's rmse: 0.0298608\n",
      "[23]\tvalid_0's rmse: 0.0298607\n",
      "[24]\tvalid_0's rmse: 0.0298606\n",
      "[25]\tvalid_0's rmse: 0.0298605\n",
      "[26]\tvalid_0's rmse: 0.0298604\n",
      "[27]\tvalid_0's rmse: 0.0298604\n",
      "[28]\tvalid_0's rmse: 0.0298603\n",
      "[29]\tvalid_0's rmse: 0.0298602\n",
      "[30]\tvalid_0's rmse: 0.0298601\n",
      "[31]\tvalid_0's rmse: 0.02986\n",
      "[32]\tvalid_0's rmse: 0.0298599\n",
      "[33]\tvalid_0's rmse: 0.0298598\n",
      "[34]\tvalid_0's rmse: 0.0298597\n",
      "[35]\tvalid_0's rmse: 0.0298596\n",
      "[36]\tvalid_0's rmse: 0.0298595\n",
      "[37]\tvalid_0's rmse: 0.0298594\n",
      "[38]\tvalid_0's rmse: 0.0298593\n",
      "[39]\tvalid_0's rmse: 0.0298592\n",
      "[40]\tvalid_0's rmse: 0.0298592\n",
      "[41]\tvalid_0's rmse: 0.0298591\n",
      "[42]\tvalid_0's rmse: 0.0298589\n",
      "[43]\tvalid_0's rmse: 0.0298588\n",
      "[44]\tvalid_0's rmse: 0.0298588\n",
      "[45]\tvalid_0's rmse: 0.0298587\n",
      "[46]\tvalid_0's rmse: 0.0298586\n",
      "[47]\tvalid_0's rmse: 0.0298584\n",
      "[48]\tvalid_0's rmse: 0.0298583\n",
      "[49]\tvalid_0's rmse: 0.0298581\n",
      "[50]\tvalid_0's rmse: 0.0298581\n",
      "[51]\tvalid_0's rmse: 0.029858\n",
      "[52]\tvalid_0's rmse: 0.029858\n",
      "[53]\tvalid_0's rmse: 0.0298578\n",
      "[54]\tvalid_0's rmse: 0.0298577\n",
      "[55]\tvalid_0's rmse: 0.0298576\n",
      "[56]\tvalid_0's rmse: 0.0298575\n",
      "[57]\tvalid_0's rmse: 0.0298574\n",
      "[58]\tvalid_0's rmse: 0.0298574\n",
      "[59]\tvalid_0's rmse: 0.0298572\n",
      "[60]\tvalid_0's rmse: 0.0298571\n",
      "[61]\tvalid_0's rmse: 0.029857\n",
      "[62]\tvalid_0's rmse: 0.0298569\n",
      "[63]\tvalid_0's rmse: 0.0298568\n",
      "[64]\tvalid_0's rmse: 0.0298566\n",
      "[65]\tvalid_0's rmse: 0.0298565\n",
      "[66]\tvalid_0's rmse: 0.0298564\n",
      "[67]\tvalid_0's rmse: 0.0298563\n",
      "[68]\tvalid_0's rmse: 0.0298563\n",
      "[69]\tvalid_0's rmse: 0.0298561\n",
      "[70]\tvalid_0's rmse: 0.0298561\n",
      "[71]\tvalid_0's rmse: 0.029856\n",
      "[72]\tvalid_0's rmse: 0.0298558\n",
      "[73]\tvalid_0's rmse: 0.0298558\n",
      "[74]\tvalid_0's rmse: 0.0298556\n",
      "[75]\tvalid_0's rmse: 0.0298556\n",
      "[76]\tvalid_0's rmse: 0.0298555\n",
      "[77]\tvalid_0's rmse: 0.0298553\n",
      "[78]\tvalid_0's rmse: 0.0298553\n",
      "[79]\tvalid_0's rmse: 0.0298552\n",
      "[80]\tvalid_0's rmse: 0.0298551\n",
      "[81]\tvalid_0's rmse: 0.0298551\n",
      "[82]\tvalid_0's rmse: 0.0298551\n",
      "[83]\tvalid_0's rmse: 0.029855\n",
      "[84]\tvalid_0's rmse: 0.0298549\n",
      "[85]\tvalid_0's rmse: 0.0298548\n",
      "[86]\tvalid_0's rmse: 0.0298547\n",
      "[87]\tvalid_0's rmse: 0.0298546\n",
      "[88]\tvalid_0's rmse: 0.0298546\n",
      "[89]\tvalid_0's rmse: 0.0298545\n",
      "[90]\tvalid_0's rmse: 0.0298545\n",
      "[91]\tvalid_0's rmse: 0.0298544\n",
      "[92]\tvalid_0's rmse: 0.0298544\n",
      "[93]\tvalid_0's rmse: 0.0298543\n",
      "[94]\tvalid_0's rmse: 0.0298543\n",
      "[95]\tvalid_0's rmse: 0.0298542\n",
      "[96]\tvalid_0's rmse: 0.0298542\n",
      "[97]\tvalid_0's rmse: 0.0298542\n",
      "[98]\tvalid_0's rmse: 0.0298541\n",
      "[99]\tvalid_0's rmse: 0.0298541\n",
      "[100]\tvalid_0's rmse: 0.029854\n",
      "[101]\tvalid_0's rmse: 0.029854\n",
      "[102]\tvalid_0's rmse: 0.0298539\n",
      "[103]\tvalid_0's rmse: 0.0298539\n",
      "[104]\tvalid_0's rmse: 0.0298538\n",
      "[105]\tvalid_0's rmse: 0.0298538\n",
      "[106]\tvalid_0's rmse: 0.0298537\n",
      "[107]\tvalid_0's rmse: 0.0298536\n",
      "[108]\tvalid_0's rmse: 0.0298535\n",
      "[109]\tvalid_0's rmse: 0.0298534\n",
      "[110]\tvalid_0's rmse: 0.0298534\n",
      "[111]\tvalid_0's rmse: 0.0298534\n",
      "[112]\tvalid_0's rmse: 0.0298532\n",
      "[113]\tvalid_0's rmse: 0.0298532\n",
      "[114]\tvalid_0's rmse: 0.0298531\n",
      "[115]\tvalid_0's rmse: 0.029853\n",
      "[116]\tvalid_0's rmse: 0.029853\n",
      "[117]\tvalid_0's rmse: 0.0298528\n",
      "[118]\tvalid_0's rmse: 0.0298528\n",
      "[119]\tvalid_0's rmse: 0.0298527\n",
      "[120]\tvalid_0's rmse: 0.0298526\n",
      "[121]\tvalid_0's rmse: 0.0298526\n",
      "[122]\tvalid_0's rmse: 0.0298526\n",
      "[123]\tvalid_0's rmse: 0.0298524\n",
      "[124]\tvalid_0's rmse: 0.0298524\n",
      "[125]\tvalid_0's rmse: 0.0298523\n",
      "[126]\tvalid_0's rmse: 0.0298522\n",
      "[127]\tvalid_0's rmse: 0.0298522\n",
      "[128]\tvalid_0's rmse: 0.0298521\n",
      "[129]\tvalid_0's rmse: 0.0298521\n",
      "[130]\tvalid_0's rmse: 0.029852\n",
      "[131]\tvalid_0's rmse: 0.0298519\n",
      "[132]\tvalid_0's rmse: 0.0298518\n",
      "[133]\tvalid_0's rmse: 0.0298518\n",
      "[134]\tvalid_0's rmse: 0.0298517\n",
      "[135]\tvalid_0's rmse: 0.0298516\n",
      "[136]\tvalid_0's rmse: 0.0298516\n",
      "[137]\tvalid_0's rmse: 0.0298515\n",
      "[138]\tvalid_0's rmse: 0.0298514\n",
      "[139]\tvalid_0's rmse: 0.0298513\n",
      "[140]\tvalid_0's rmse: 0.0298513\n",
      "[141]\tvalid_0's rmse: 0.0298513\n",
      "[142]\tvalid_0's rmse: 0.0298512\n",
      "[143]\tvalid_0's rmse: 0.0298511\n",
      "[144]\tvalid_0's rmse: 0.0298511\n",
      "[145]\tvalid_0's rmse: 0.029851\n",
      "[146]\tvalid_0's rmse: 0.029851\n",
      "[147]\tvalid_0's rmse: 0.029851\n",
      "[148]\tvalid_0's rmse: 0.0298509\n",
      "[149]\tvalid_0's rmse: 0.0298508\n",
      "[150]\tvalid_0's rmse: 0.0298508\n",
      "[151]\tvalid_0's rmse: 0.0298508\n",
      "[152]\tvalid_0's rmse: 0.0298508\n",
      "[153]\tvalid_0's rmse: 0.0298507\n",
      "[154]\tvalid_0's rmse: 0.0298507\n",
      "[155]\tvalid_0's rmse: 0.0298506\n",
      "[156]\tvalid_0's rmse: 0.0298506\n",
      "[157]\tvalid_0's rmse: 0.0298505\n",
      "[158]\tvalid_0's rmse: 0.0298505\n",
      "[159]\tvalid_0's rmse: 0.0298505\n",
      "[160]\tvalid_0's rmse: 0.0298505\n",
      "[161]\tvalid_0's rmse: 0.0298505\n",
      "[162]\tvalid_0's rmse: 0.0298504\n",
      "[163]\tvalid_0's rmse: 0.0298504\n",
      "[164]\tvalid_0's rmse: 0.0298503\n",
      "[165]\tvalid_0's rmse: 0.0298503\n",
      "[166]\tvalid_0's rmse: 0.0298502\n",
      "[167]\tvalid_0's rmse: 0.0298502\n",
      "[168]\tvalid_0's rmse: 0.0298501\n",
      "[169]\tvalid_0's rmse: 0.0298501\n",
      "[170]\tvalid_0's rmse: 0.02985\n",
      "[171]\tvalid_0's rmse: 0.02985\n",
      "[172]\tvalid_0's rmse: 0.02985\n",
      "[173]\tvalid_0's rmse: 0.0298498\n",
      "[174]\tvalid_0's rmse: 0.0298498\n",
      "[175]\tvalid_0's rmse: 0.0298497\n",
      "[176]\tvalid_0's rmse: 0.0298496\n",
      "[177]\tvalid_0's rmse: 0.0298495\n",
      "[178]\tvalid_0's rmse: 0.0298495\n",
      "[179]\tvalid_0's rmse: 0.0298495\n",
      "[180]\tvalid_0's rmse: 0.0298493\n",
      "[181]\tvalid_0's rmse: 0.0298493\n",
      "[182]\tvalid_0's rmse: 0.0298493\n",
      "[183]\tvalid_0's rmse: 0.0298492\n",
      "[184]\tvalid_0's rmse: 0.0298492\n",
      "[185]\tvalid_0's rmse: 0.0298491\n",
      "[186]\tvalid_0's rmse: 0.029849\n",
      "[187]\tvalid_0's rmse: 0.0298489\n",
      "[188]\tvalid_0's rmse: 0.0298488\n",
      "[189]\tvalid_0's rmse: 0.0298488\n",
      "[190]\tvalid_0's rmse: 0.0298488\n",
      "[191]\tvalid_0's rmse: 0.0298487\n",
      "[192]\tvalid_0's rmse: 0.0298487\n",
      "[193]\tvalid_0's rmse: 0.0298485\n",
      "[194]\tvalid_0's rmse: 0.0298485\n",
      "[195]\tvalid_0's rmse: 0.0298485\n",
      "[196]\tvalid_0's rmse: 0.0298484\n",
      "[197]\tvalid_0's rmse: 0.0298484\n",
      "[198]\tvalid_0's rmse: 0.0298484\n",
      "[199]\tvalid_0's rmse: 0.0298484\n",
      "[200]\tvalid_0's rmse: 0.0298483\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's rmse: 0.0298483\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.455727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 142272\n",
      "[LightGBM] [Info] Number of data points in the train set: 665171, number of used features: 565\n",
      "[LightGBM] [Info] Start training from score -0.000035\n",
      "[1]\tvalid_0's rmse: 0.0301649\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's rmse: 0.030165\n",
      "[3]\tvalid_0's rmse: 0.0301651\n",
      "[4]\tvalid_0's rmse: 0.0301652\n",
      "[5]\tvalid_0's rmse: 0.0301653\n",
      "[6]\tvalid_0's rmse: 0.0301655\n",
      "[7]\tvalid_0's rmse: 0.0301656\n",
      "[8]\tvalid_0's rmse: 0.0301657\n",
      "[9]\tvalid_0's rmse: 0.0301658\n",
      "[10]\tvalid_0's rmse: 0.0301659\n",
      "[11]\tvalid_0's rmse: 0.030166\n",
      "[12]\tvalid_0's rmse: 0.0301661\n",
      "[13]\tvalid_0's rmse: 0.0301663\n",
      "[14]\tvalid_0's rmse: 0.0301664\n",
      "[15]\tvalid_0's rmse: 0.0301665\n",
      "[16]\tvalid_0's rmse: 0.0301666\n",
      "[17]\tvalid_0's rmse: 0.0301668\n",
      "[18]\tvalid_0's rmse: 0.0301669\n",
      "[19]\tvalid_0's rmse: 0.030167\n",
      "[20]\tvalid_0's rmse: 0.0301671\n",
      "[21]\tvalid_0's rmse: 0.0301672\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's rmse: 0.0301649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.218222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 142272\n",
      "[LightGBM] [Info] Number of data points in the train set: 665171, number of used features: 565\n",
      "[LightGBM] [Info] Start training from score -0.000057\n",
      "[1]\tvalid_0's rmse: 0.029875\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's rmse: 0.0298749\n",
      "[3]\tvalid_0's rmse: 0.0298748\n",
      "[4]\tvalid_0's rmse: 0.0298748\n",
      "[5]\tvalid_0's rmse: 0.0298747\n",
      "[6]\tvalid_0's rmse: 0.0298746\n",
      "[7]\tvalid_0's rmse: 0.0298746\n",
      "[8]\tvalid_0's rmse: 0.0298745\n",
      "[9]\tvalid_0's rmse: 0.0298745\n",
      "[10]\tvalid_0's rmse: 0.0298744\n",
      "[11]\tvalid_0's rmse: 0.0298744\n",
      "[12]\tvalid_0's rmse: 0.0298744\n",
      "[13]\tvalid_0's rmse: 0.0298744\n",
      "[14]\tvalid_0's rmse: 0.0298744\n",
      "[15]\tvalid_0's rmse: 0.0298744\n",
      "[16]\tvalid_0's rmse: 0.0298744\n",
      "[17]\tvalid_0's rmse: 0.0298744\n",
      "[18]\tvalid_0's rmse: 0.0298744\n",
      "[19]\tvalid_0's rmse: 0.0298744\n",
      "[20]\tvalid_0's rmse: 0.0298744\n",
      "[21]\tvalid_0's rmse: 0.0298744\n",
      "[22]\tvalid_0's rmse: 0.0298744\n",
      "[23]\tvalid_0's rmse: 0.0298744\n",
      "[24]\tvalid_0's rmse: 0.0298744\n",
      "[25]\tvalid_0's rmse: 0.0298743\n",
      "[26]\tvalid_0's rmse: 0.0298744\n",
      "[27]\tvalid_0's rmse: 0.0298743\n",
      "[28]\tvalid_0's rmse: 0.0298744\n",
      "[29]\tvalid_0's rmse: 0.0298744\n",
      "[30]\tvalid_0's rmse: 0.0298743\n",
      "[31]\tvalid_0's rmse: 0.0298744\n",
      "[32]\tvalid_0's rmse: 0.0298744\n",
      "[33]\tvalid_0's rmse: 0.0298744\n",
      "[34]\tvalid_0's rmse: 0.0298743\n",
      "[35]\tvalid_0's rmse: 0.0298743\n",
      "[36]\tvalid_0's rmse: 0.0298743\n",
      "[37]\tvalid_0's rmse: 0.0298743\n",
      "[38]\tvalid_0's rmse: 0.0298742\n",
      "[39]\tvalid_0's rmse: 0.0298742\n",
      "[40]\tvalid_0's rmse: 0.0298742\n",
      "[41]\tvalid_0's rmse: 0.0298742\n",
      "[42]\tvalid_0's rmse: 0.0298742\n",
      "[43]\tvalid_0's rmse: 0.0298743\n",
      "[44]\tvalid_0's rmse: 0.0298742\n",
      "[45]\tvalid_0's rmse: 0.0298742\n",
      "[46]\tvalid_0's rmse: 0.0298742\n",
      "[47]\tvalid_0's rmse: 0.0298742\n",
      "[48]\tvalid_0's rmse: 0.0298741\n",
      "[49]\tvalid_0's rmse: 0.0298742\n",
      "[50]\tvalid_0's rmse: 0.0298741\n",
      "[51]\tvalid_0's rmse: 0.0298742\n",
      "[52]\tvalid_0's rmse: 0.0298741\n",
      "[53]\tvalid_0's rmse: 0.0298741\n",
      "[54]\tvalid_0's rmse: 0.0298741\n",
      "[55]\tvalid_0's rmse: 0.0298741\n",
      "[56]\tvalid_0's rmse: 0.0298742\n",
      "[57]\tvalid_0's rmse: 0.0298741\n",
      "[58]\tvalid_0's rmse: 0.0298741\n",
      "[59]\tvalid_0's rmse: 0.0298741\n",
      "[60]\tvalid_0's rmse: 0.0298741\n",
      "[61]\tvalid_0's rmse: 0.029874\n",
      "[62]\tvalid_0's rmse: 0.0298741\n",
      "[63]\tvalid_0's rmse: 0.029874\n",
      "[64]\tvalid_0's rmse: 0.029874\n",
      "[65]\tvalid_0's rmse: 0.029874\n",
      "[66]\tvalid_0's rmse: 0.029874\n",
      "[67]\tvalid_0's rmse: 0.029874\n",
      "[68]\tvalid_0's rmse: 0.029874\n",
      "[69]\tvalid_0's rmse: 0.029874\n",
      "[70]\tvalid_0's rmse: 0.029874\n",
      "[71]\tvalid_0's rmse: 0.029874\n",
      "[72]\tvalid_0's rmse: 0.029874\n",
      "[73]\tvalid_0's rmse: 0.029874\n",
      "[74]\tvalid_0's rmse: 0.0298739\n",
      "[75]\tvalid_0's rmse: 0.0298739\n",
      "[76]\tvalid_0's rmse: 0.0298739\n",
      "[77]\tvalid_0's rmse: 0.0298739\n",
      "[78]\tvalid_0's rmse: 0.0298739\n",
      "[79]\tvalid_0's rmse: 0.0298739\n",
      "[80]\tvalid_0's rmse: 0.0298739\n",
      "[81]\tvalid_0's rmse: 0.0298739\n",
      "[82]\tvalid_0's rmse: 0.0298739\n",
      "[83]\tvalid_0's rmse: 0.0298739\n",
      "[84]\tvalid_0's rmse: 0.0298739\n",
      "[85]\tvalid_0's rmse: 0.0298739\n",
      "[86]\tvalid_0's rmse: 0.0298739\n",
      "[87]\tvalid_0's rmse: 0.0298739\n",
      "[88]\tvalid_0's rmse: 0.0298739\n",
      "[89]\tvalid_0's rmse: 0.0298738\n",
      "[90]\tvalid_0's rmse: 0.0298738\n",
      "[91]\tvalid_0's rmse: 0.0298738\n",
      "[92]\tvalid_0's rmse: 0.0298738\n",
      "[93]\tvalid_0's rmse: 0.0298738\n",
      "[94]\tvalid_0's rmse: 0.0298738\n",
      "[95]\tvalid_0's rmse: 0.0298737\n",
      "[96]\tvalid_0's rmse: 0.0298738\n",
      "[97]\tvalid_0's rmse: 0.0298738\n",
      "[98]\tvalid_0's rmse: 0.0298737\n",
      "[99]\tvalid_0's rmse: 0.0298737\n",
      "[100]\tvalid_0's rmse: 0.0298737\n",
      "[101]\tvalid_0's rmse: 0.0298737\n",
      "[102]\tvalid_0's rmse: 0.0298736\n",
      "[103]\tvalid_0's rmse: 0.0298736\n",
      "[104]\tvalid_0's rmse: 0.0298732\n",
      "[105]\tvalid_0's rmse: 0.0298732\n",
      "[106]\tvalid_0's rmse: 0.0298731\n",
      "[107]\tvalid_0's rmse: 0.0298731\n",
      "[108]\tvalid_0's rmse: 0.0298731\n",
      "[109]\tvalid_0's rmse: 0.0298731\n",
      "[110]\tvalid_0's rmse: 0.0298727\n",
      "[111]\tvalid_0's rmse: 0.0298726\n",
      "[112]\tvalid_0's rmse: 0.0298726\n",
      "[113]\tvalid_0's rmse: 0.0298726\n",
      "[114]\tvalid_0's rmse: 0.0298726\n",
      "[115]\tvalid_0's rmse: 0.0298726\n",
      "[116]\tvalid_0's rmse: 0.0298726\n",
      "[117]\tvalid_0's rmse: 0.0298726\n",
      "[118]\tvalid_0's rmse: 0.0298726\n",
      "[119]\tvalid_0's rmse: 0.0298725\n",
      "[120]\tvalid_0's rmse: 0.0298725\n",
      "[121]\tvalid_0's rmse: 0.0298725\n",
      "[122]\tvalid_0's rmse: 0.0298724\n",
      "[123]\tvalid_0's rmse: 0.0298724\n",
      "[124]\tvalid_0's rmse: 0.0298724\n",
      "[125]\tvalid_0's rmse: 0.0298724\n",
      "[126]\tvalid_0's rmse: 0.0298723\n",
      "[127]\tvalid_0's rmse: 0.0298723\n",
      "[128]\tvalid_0's rmse: 0.0298723\n",
      "[129]\tvalid_0's rmse: 0.0298723\n",
      "[130]\tvalid_0's rmse: 0.0298722\n",
      "[131]\tvalid_0's rmse: 0.0298722\n",
      "[132]\tvalid_0's rmse: 0.0298722\n",
      "[133]\tvalid_0's rmse: 0.0298718\n",
      "[134]\tvalid_0's rmse: 0.0298718\n",
      "[135]\tvalid_0's rmse: 0.0298717\n",
      "[136]\tvalid_0's rmse: 0.0298718\n",
      "[137]\tvalid_0's rmse: 0.0298717\n",
      "[138]\tvalid_0's rmse: 0.0298717\n",
      "[139]\tvalid_0's rmse: 0.0298717\n",
      "[140]\tvalid_0's rmse: 0.0298717\n",
      "[141]\tvalid_0's rmse: 0.0298716\n",
      "[142]\tvalid_0's rmse: 0.0298716\n",
      "[143]\tvalid_0's rmse: 0.0298716\n",
      "[144]\tvalid_0's rmse: 0.0298716\n",
      "[145]\tvalid_0's rmse: 0.0298715\n",
      "[146]\tvalid_0's rmse: 0.0298716\n",
      "[147]\tvalid_0's rmse: 0.0298715\n",
      "[148]\tvalid_0's rmse: 0.0298715\n",
      "[149]\tvalid_0's rmse: 0.0298715\n",
      "[150]\tvalid_0's rmse: 0.0298715\n",
      "[151]\tvalid_0's rmse: 0.0298714\n",
      "[152]\tvalid_0's rmse: 0.0298714\n",
      "[153]\tvalid_0's rmse: 0.0298714\n",
      "[154]\tvalid_0's rmse: 0.0298714\n",
      "[155]\tvalid_0's rmse: 0.0298713\n",
      "[156]\tvalid_0's rmse: 0.0298713\n",
      "[157]\tvalid_0's rmse: 0.0298713\n",
      "[158]\tvalid_0's rmse: 0.0298712\n",
      "[159]\tvalid_0's rmse: 0.0298708\n",
      "[160]\tvalid_0's rmse: 0.0298708\n",
      "[161]\tvalid_0's rmse: 0.0298708\n",
      "[162]\tvalid_0's rmse: 0.0298708\n",
      "[163]\tvalid_0's rmse: 0.0298708\n",
      "[164]\tvalid_0's rmse: 0.0298707\n",
      "[165]\tvalid_0's rmse: 0.0298707\n",
      "[166]\tvalid_0's rmse: 0.0298707\n",
      "[167]\tvalid_0's rmse: 0.0298707\n",
      "[168]\tvalid_0's rmse: 0.0298707\n",
      "[169]\tvalid_0's rmse: 0.0298707\n",
      "[170]\tvalid_0's rmse: 0.0298707\n",
      "[171]\tvalid_0's rmse: 0.0298707\n",
      "[172]\tvalid_0's rmse: 0.0298706\n",
      "[173]\tvalid_0's rmse: 0.0298706\n",
      "[174]\tvalid_0's rmse: 0.0298705\n",
      "[175]\tvalid_0's rmse: 0.0298705\n",
      "[176]\tvalid_0's rmse: 0.0298705\n",
      "[177]\tvalid_0's rmse: 0.0298705\n",
      "[178]\tvalid_0's rmse: 0.0298705\n",
      "[179]\tvalid_0's rmse: 0.0298705\n",
      "[180]\tvalid_0's rmse: 0.0298704\n",
      "[181]\tvalid_0's rmse: 0.0298704\n",
      "[182]\tvalid_0's rmse: 0.0298704\n",
      "[183]\tvalid_0's rmse: 0.0298704\n",
      "[184]\tvalid_0's rmse: 0.0298704\n",
      "[185]\tvalid_0's rmse: 0.0298704\n",
      "[186]\tvalid_0's rmse: 0.0298704\n",
      "[187]\tvalid_0's rmse: 0.0298703\n",
      "[188]\tvalid_0's rmse: 0.0298703\n",
      "[189]\tvalid_0's rmse: 0.0298703\n",
      "[190]\tvalid_0's rmse: 0.0298703\n",
      "[191]\tvalid_0's rmse: 0.0298703\n",
      "[192]\tvalid_0's rmse: 0.0298703\n",
      "[193]\tvalid_0's rmse: 0.0298702\n",
      "[194]\tvalid_0's rmse: 0.0298702\n",
      "[195]\tvalid_0's rmse: 0.0298702\n",
      "[196]\tvalid_0's rmse: 0.0298702\n",
      "[197]\tvalid_0's rmse: 0.0298702\n",
      "[198]\tvalid_0's rmse: 0.0298702\n",
      "[199]\tvalid_0's rmse: 0.0298701\n",
      "[200]\tvalid_0's rmse: 0.0298701\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's rmse: 0.0298701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.231189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 142274\n",
      "[LightGBM] [Info] Number of data points in the train set: 665172, number of used features: 565\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[1]\tvalid_0's rmse: 0.0298307\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's rmse: 0.0298306\n",
      "[3]\tvalid_0's rmse: 0.0298305\n",
      "[4]\tvalid_0's rmse: 0.0298304\n",
      "[5]\tvalid_0's rmse: 0.0298304\n",
      "[6]\tvalid_0's rmse: 0.0298304\n",
      "[7]\tvalid_0's rmse: 0.0298303\n",
      "[8]\tvalid_0's rmse: 0.0298302\n",
      "[9]\tvalid_0's rmse: 0.0298302\n",
      "[10]\tvalid_0's rmse: 0.0298301\n",
      "[11]\tvalid_0's rmse: 0.0298301\n",
      "[12]\tvalid_0's rmse: 0.02983\n",
      "[13]\tvalid_0's rmse: 0.0298299\n",
      "[14]\tvalid_0's rmse: 0.0298299\n",
      "[15]\tvalid_0's rmse: 0.0298299\n",
      "[16]\tvalid_0's rmse: 0.0298298\n",
      "[17]\tvalid_0's rmse: 0.0298298\n",
      "[18]\tvalid_0's rmse: 0.0298298\n",
      "[19]\tvalid_0's rmse: 0.0298297\n",
      "[20]\tvalid_0's rmse: 0.0298296\n",
      "[21]\tvalid_0's rmse: 0.0298296\n",
      "[22]\tvalid_0's rmse: 0.0298295\n",
      "[23]\tvalid_0's rmse: 0.0298295\n",
      "[24]\tvalid_0's rmse: 0.0298295\n",
      "[25]\tvalid_0's rmse: 0.0298294\n",
      "[26]\tvalid_0's rmse: 0.0298294\n",
      "[27]\tvalid_0's rmse: 0.0298294\n",
      "[28]\tvalid_0's rmse: 0.0298293\n",
      "[29]\tvalid_0's rmse: 0.0298293\n",
      "[30]\tvalid_0's rmse: 0.0298293\n",
      "[31]\tvalid_0's rmse: 0.0298292\n",
      "[32]\tvalid_0's rmse: 0.0298292\n",
      "[33]\tvalid_0's rmse: 0.0298292\n",
      "[34]\tvalid_0's rmse: 0.0298291\n",
      "[35]\tvalid_0's rmse: 0.0298289\n",
      "[36]\tvalid_0's rmse: 0.0298288\n",
      "[37]\tvalid_0's rmse: 0.0298286\n",
      "[38]\tvalid_0's rmse: 0.0298286\n",
      "[39]\tvalid_0's rmse: 0.0298285\n",
      "[40]\tvalid_0's rmse: 0.0298285\n",
      "[41]\tvalid_0's rmse: 0.0298284\n",
      "[42]\tvalid_0's rmse: 0.0298282\n",
      "[43]\tvalid_0's rmse: 0.0298282\n",
      "[44]\tvalid_0's rmse: 0.029828\n",
      "[45]\tvalid_0's rmse: 0.0298278\n",
      "[46]\tvalid_0's rmse: 0.0298277\n",
      "[47]\tvalid_0's rmse: 0.0298275\n",
      "[48]\tvalid_0's rmse: 0.0298273\n",
      "[49]\tvalid_0's rmse: 0.0298271\n",
      "[50]\tvalid_0's rmse: 0.029827\n",
      "[51]\tvalid_0's rmse: 0.0298268\n",
      "[52]\tvalid_0's rmse: 0.0298266\n",
      "[53]\tvalid_0's rmse: 0.0298265\n",
      "[54]\tvalid_0's rmse: 0.0298263\n",
      "[55]\tvalid_0's rmse: 0.0298262\n",
      "[56]\tvalid_0's rmse: 0.029826\n",
      "[57]\tvalid_0's rmse: 0.0298259\n",
      "[58]\tvalid_0's rmse: 0.0298257\n",
      "[59]\tvalid_0's rmse: 0.0298256\n",
      "[60]\tvalid_0's rmse: 0.0298255\n",
      "[61]\tvalid_0's rmse: 0.0298254\n",
      "[62]\tvalid_0's rmse: 0.0298253\n",
      "[63]\tvalid_0's rmse: 0.0298252\n",
      "[64]\tvalid_0's rmse: 0.0298252\n",
      "[65]\tvalid_0's rmse: 0.029825\n",
      "[66]\tvalid_0's rmse: 0.029825\n",
      "[67]\tvalid_0's rmse: 0.0298248\n",
      "[68]\tvalid_0's rmse: 0.0298248\n",
      "[69]\tvalid_0's rmse: 0.0298246\n",
      "[70]\tvalid_0's rmse: 0.0298246\n",
      "[71]\tvalid_0's rmse: 0.0298245\n",
      "[72]\tvalid_0's rmse: 0.0298244\n",
      "[73]\tvalid_0's rmse: 0.0298243\n",
      "[74]\tvalid_0's rmse: 0.0298242\n",
      "[75]\tvalid_0's rmse: 0.0298241\n",
      "[76]\tvalid_0's rmse: 0.0298239\n",
      "[77]\tvalid_0's rmse: 0.0298238\n",
      "[78]\tvalid_0's rmse: 0.0298236\n",
      "[79]\tvalid_0's rmse: 0.0298236\n",
      "[80]\tvalid_0's rmse: 0.0298234\n",
      "[81]\tvalid_0's rmse: 0.0298234\n",
      "[82]\tvalid_0's rmse: 0.0298233\n",
      "[83]\tvalid_0's rmse: 0.0298231\n",
      "[84]\tvalid_0's rmse: 0.029823\n",
      "[85]\tvalid_0's rmse: 0.0298228\n",
      "[86]\tvalid_0's rmse: 0.0298228\n",
      "[87]\tvalid_0's rmse: 0.0298226\n",
      "[88]\tvalid_0's rmse: 0.0298225\n",
      "[89]\tvalid_0's rmse: 0.0298223\n",
      "[90]\tvalid_0's rmse: 0.0298222\n",
      "[91]\tvalid_0's rmse: 0.029822\n",
      "[92]\tvalid_0's rmse: 0.029822\n",
      "[93]\tvalid_0's rmse: 0.029822\n",
      "[94]\tvalid_0's rmse: 0.0298219\n",
      "[95]\tvalid_0's rmse: 0.0298219\n",
      "[96]\tvalid_0's rmse: 0.0298217\n",
      "[97]\tvalid_0's rmse: 0.0298217\n",
      "[98]\tvalid_0's rmse: 0.0298215\n",
      "[99]\tvalid_0's rmse: 0.0298214\n",
      "[100]\tvalid_0's rmse: 0.0298214\n",
      "[101]\tvalid_0's rmse: 0.0298213\n",
      "[102]\tvalid_0's rmse: 0.0298211\n",
      "[103]\tvalid_0's rmse: 0.0298211\n",
      "[104]\tvalid_0's rmse: 0.029821\n",
      "[105]\tvalid_0's rmse: 0.0298208\n",
      "[106]\tvalid_0's rmse: 0.0298206\n",
      "[107]\tvalid_0's rmse: 0.0298204\n",
      "[108]\tvalid_0's rmse: 0.0298203\n",
      "[109]\tvalid_0's rmse: 0.0298203\n",
      "[110]\tvalid_0's rmse: 0.0298201\n",
      "[111]\tvalid_0's rmse: 0.02982\n",
      "[112]\tvalid_0's rmse: 0.02982\n",
      "[113]\tvalid_0's rmse: 0.0298198\n",
      "[114]\tvalid_0's rmse: 0.0298197\n",
      "[115]\tvalid_0's rmse: 0.0298195\n",
      "[116]\tvalid_0's rmse: 0.0298195\n",
      "[117]\tvalid_0's rmse: 0.0298194\n",
      "[118]\tvalid_0's rmse: 0.0298192\n",
      "[119]\tvalid_0's rmse: 0.029819\n",
      "[120]\tvalid_0's rmse: 0.029819\n",
      "[121]\tvalid_0's rmse: 0.0298189\n",
      "[122]\tvalid_0's rmse: 0.0298188\n",
      "[123]\tvalid_0's rmse: 0.0298186\n",
      "[124]\tvalid_0's rmse: 0.0298184\n",
      "[125]\tvalid_0's rmse: 0.0298182\n",
      "[126]\tvalid_0's rmse: 0.0298182\n",
      "[127]\tvalid_0's rmse: 0.0298181\n",
      "[128]\tvalid_0's rmse: 0.029818\n",
      "[129]\tvalid_0's rmse: 0.0298178\n",
      "[130]\tvalid_0's rmse: 0.0298177\n",
      "[131]\tvalid_0's rmse: 0.0298176\n",
      "[132]\tvalid_0's rmse: 0.0298174\n",
      "[133]\tvalid_0's rmse: 0.0298173\n",
      "[134]\tvalid_0's rmse: 0.0298173\n",
      "[135]\tvalid_0's rmse: 0.0298172\n",
      "[136]\tvalid_0's rmse: 0.0298172\n",
      "[137]\tvalid_0's rmse: 0.0298172\n",
      "[138]\tvalid_0's rmse: 0.0298171\n",
      "[139]\tvalid_0's rmse: 0.029817\n",
      "[140]\tvalid_0's rmse: 0.0298169\n",
      "[141]\tvalid_0's rmse: 0.0298169\n",
      "[142]\tvalid_0's rmse: 0.0298169\n",
      "[143]\tvalid_0's rmse: 0.0298167\n",
      "[144]\tvalid_0's rmse: 0.0298167\n",
      "[145]\tvalid_0's rmse: 0.0298167\n",
      "[146]\tvalid_0's rmse: 0.0298166\n",
      "[147]\tvalid_0's rmse: 0.0298166\n",
      "[148]\tvalid_0's rmse: 0.0298166\n",
      "[149]\tvalid_0's rmse: 0.0298165\n",
      "[150]\tvalid_0's rmse: 0.0298165\n",
      "[151]\tvalid_0's rmse: 0.0298165\n",
      "[152]\tvalid_0's rmse: 0.0298164\n",
      "[153]\tvalid_0's rmse: 0.0298163\n",
      "[154]\tvalid_0's rmse: 0.0298162\n",
      "[155]\tvalid_0's rmse: 0.0298162\n",
      "[156]\tvalid_0's rmse: 0.0298161\n",
      "[157]\tvalid_0's rmse: 0.0298161\n",
      "[158]\tvalid_0's rmse: 0.029816\n",
      "[159]\tvalid_0's rmse: 0.029816\n",
      "[160]\tvalid_0's rmse: 0.0298159\n",
      "[161]\tvalid_0's rmse: 0.0298159\n",
      "[162]\tvalid_0's rmse: 0.0298158\n",
      "[163]\tvalid_0's rmse: 0.0298158\n",
      "[164]\tvalid_0's rmse: 0.0298159\n",
      "[165]\tvalid_0's rmse: 0.0298159\n",
      "[166]\tvalid_0's rmse: 0.0298158\n",
      "[167]\tvalid_0's rmse: 0.0298158\n",
      "[168]\tvalid_0's rmse: 0.0298157\n",
      "[169]\tvalid_0's rmse: 0.0298157\n",
      "[170]\tvalid_0's rmse: 0.0298157\n",
      "[171]\tvalid_0's rmse: 0.0298156\n",
      "[172]\tvalid_0's rmse: 0.0298156\n",
      "[173]\tvalid_0's rmse: 0.0298157\n",
      "[174]\tvalid_0's rmse: 0.0298157\n",
      "[175]\tvalid_0's rmse: 0.0298156\n",
      "[176]\tvalid_0's rmse: 0.0298156\n",
      "[177]\tvalid_0's rmse: 0.0298155\n",
      "[178]\tvalid_0's rmse: 0.0298153\n",
      "[179]\tvalid_0's rmse: 0.0298153\n",
      "[180]\tvalid_0's rmse: 0.0298153\n",
      "[181]\tvalid_0's rmse: 0.0298152\n",
      "[182]\tvalid_0's rmse: 0.0298151\n",
      "[183]\tvalid_0's rmse: 0.029815\n",
      "[184]\tvalid_0's rmse: 0.0298149\n",
      "[185]\tvalid_0's rmse: 0.0298149\n",
      "[186]\tvalid_0's rmse: 0.029815\n",
      "[187]\tvalid_0's rmse: 0.029815\n",
      "[188]\tvalid_0's rmse: 0.029815\n",
      "[189]\tvalid_0's rmse: 0.0298149\n",
      "[190]\tvalid_0's rmse: 0.0298149\n",
      "[191]\tvalid_0's rmse: 0.0298148\n",
      "[192]\tvalid_0's rmse: 0.0298148\n",
      "[193]\tvalid_0's rmse: 0.0298148\n",
      "[194]\tvalid_0's rmse: 0.0298146\n",
      "[195]\tvalid_0's rmse: 0.0298146\n",
      "[196]\tvalid_0's rmse: 0.0298145\n",
      "[197]\tvalid_0's rmse: 0.0298144\n",
      "[198]\tvalid_0's rmse: 0.0298144\n",
      "[199]\tvalid_0's rmse: 0.0298143\n",
      "[200]\tvalid_0's rmse: 0.0298143\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[199]\tvalid_0's rmse: 0.0298143\n",
      "\n",
      "[Training XGB]\n",
      "[0]\teval-rmse:0.03002\n",
      "[1]\teval-rmse:0.03002\n",
      "[2]\teval-rmse:0.03002\n",
      "[3]\teval-rmse:0.03002\n",
      "[4]\teval-rmse:0.03002\n",
      "[5]\teval-rmse:0.03002\n",
      "[6]\teval-rmse:0.03002\n",
      "[7]\teval-rmse:0.03002\n",
      "[8]\teval-rmse:0.03002\n",
      "[9]\teval-rmse:0.03002\n",
      "[10]\teval-rmse:0.03002\n",
      "[11]\teval-rmse:0.03001\n",
      "[12]\teval-rmse:0.03001\n",
      "[13]\teval-rmse:0.03001\n",
      "[14]\teval-rmse:0.03001\n",
      "[15]\teval-rmse:0.03001\n",
      "[16]\teval-rmse:0.03001\n",
      "[17]\teval-rmse:0.03001\n",
      "[18]\teval-rmse:0.03001\n",
      "[19]\teval-rmse:0.03001\n",
      "[20]\teval-rmse:0.03001\n",
      "[21]\teval-rmse:0.03001\n",
      "[22]\teval-rmse:0.03001\n",
      "[23]\teval-rmse:0.03001\n",
      "[24]\teval-rmse:0.03001\n",
      "[25]\teval-rmse:0.03001\n",
      "[26]\teval-rmse:0.03001\n",
      "[27]\teval-rmse:0.03001\n",
      "[28]\teval-rmse:0.03001\n",
      "[29]\teval-rmse:0.03001\n",
      "[30]\teval-rmse:0.03001\n",
      "[31]\teval-rmse:0.03001\n",
      "[32]\teval-rmse:0.03001\n",
      "[33]\teval-rmse:0.03001\n",
      "[34]\teval-rmse:0.03001\n",
      "[35]\teval-rmse:0.03001\n",
      "[36]\teval-rmse:0.03001\n",
      "[37]\teval-rmse:0.03001\n",
      "[38]\teval-rmse:0.03001\n",
      "[39]\teval-rmse:0.03001\n",
      "[40]\teval-rmse:0.03001\n",
      "[41]\teval-rmse:0.03001\n",
      "[42]\teval-rmse:0.03001\n",
      "[43]\teval-rmse:0.03001\n",
      "[44]\teval-rmse:0.03001\n",
      "[45]\teval-rmse:0.03001\n",
      "[46]\teval-rmse:0.03001\n",
      "[47]\teval-rmse:0.03001\n",
      "[48]\teval-rmse:0.03001\n",
      "[49]\teval-rmse:0.03001\n",
      "[50]\teval-rmse:0.03001\n",
      "[51]\teval-rmse:0.03001\n",
      "[52]\teval-rmse:0.03001\n",
      "[53]\teval-rmse:0.03001\n",
      "[54]\teval-rmse:0.03001\n",
      "[55]\teval-rmse:0.03001\n",
      "[56]\teval-rmse:0.03001\n",
      "[57]\teval-rmse:0.03001\n",
      "[58]\teval-rmse:0.03001\n",
      "[59]\teval-rmse:0.03001\n",
      "[60]\teval-rmse:0.03001\n",
      "[61]\teval-rmse:0.03001\n",
      "[62]\teval-rmse:0.03001\n",
      "[63]\teval-rmse:0.03001\n",
      "[64]\teval-rmse:0.03001\n",
      "[65]\teval-rmse:0.03001\n",
      "[66]\teval-rmse:0.03001\n",
      "[67]\teval-rmse:0.03001\n",
      "[68]\teval-rmse:0.03001\n",
      "[69]\teval-rmse:0.03001\n",
      "[70]\teval-rmse:0.03001\n",
      "[71]\teval-rmse:0.03001\n",
      "[72]\teval-rmse:0.03001\n",
      "[73]\teval-rmse:0.03001\n",
      "[74]\teval-rmse:0.03001\n",
      "[75]\teval-rmse:0.03001\n",
      "[76]\teval-rmse:0.03001\n",
      "[77]\teval-rmse:0.03001\n",
      "[78]\teval-rmse:0.03001\n",
      "[79]\teval-rmse:0.03001\n",
      "[80]\teval-rmse:0.03001\n",
      "[81]\teval-rmse:0.03001\n",
      "[82]\teval-rmse:0.03001\n",
      "[83]\teval-rmse:0.03001\n",
      "[84]\teval-rmse:0.03001\n",
      "[85]\teval-rmse:0.03001\n",
      "[86]\teval-rmse:0.03001\n",
      "[87]\teval-rmse:0.03001\n",
      "[88]\teval-rmse:0.03001\n",
      "[89]\teval-rmse:0.03001\n",
      "[90]\teval-rmse:0.03001\n",
      "[91]\teval-rmse:0.03001\n",
      "[92]\teval-rmse:0.03001\n",
      "[93]\teval-rmse:0.03001\n",
      "[94]\teval-rmse:0.03001\n",
      "[95]\teval-rmse:0.03001\n",
      "[96]\teval-rmse:0.03001\n",
      "[97]\teval-rmse:0.03001\n",
      "[98]\teval-rmse:0.03001\n",
      "[99]\teval-rmse:0.03001\n",
      "[100]\teval-rmse:0.03001\n",
      "[101]\teval-rmse:0.03001\n",
      "[102]\teval-rmse:0.03001\n",
      "[103]\teval-rmse:0.03001\n",
      "[104]\teval-rmse:0.03001\n",
      "[105]\teval-rmse:0.03001\n",
      "[106]\teval-rmse:0.03001\n",
      "[107]\teval-rmse:0.03001\n",
      "[108]\teval-rmse:0.03001\n",
      "[109]\teval-rmse:0.03001\n",
      "[110]\teval-rmse:0.03001\n",
      "[111]\teval-rmse:0.03001\n",
      "[112]\teval-rmse:0.03001\n",
      "[113]\teval-rmse:0.03001\n",
      "[114]\teval-rmse:0.03001\n",
      "[115]\teval-rmse:0.03001\n",
      "[116]\teval-rmse:0.03001\n",
      "[117]\teval-rmse:0.03001\n",
      "[118]\teval-rmse:0.03001\n",
      "[119]\teval-rmse:0.03001\n",
      "[120]\teval-rmse:0.03001\n",
      "[121]\teval-rmse:0.03001\n",
      "[122]\teval-rmse:0.03001\n",
      "[123]\teval-rmse:0.03001\n",
      "[124]\teval-rmse:0.03001\n",
      "[125]\teval-rmse:0.03001\n",
      "[126]\teval-rmse:0.03001\n",
      "[127]\teval-rmse:0.03001\n",
      "[128]\teval-rmse:0.03001\n",
      "[129]\teval-rmse:0.03001\n",
      "[130]\teval-rmse:0.03001\n",
      "[131]\teval-rmse:0.03001\n",
      "[132]\teval-rmse:0.03001\n",
      "[133]\teval-rmse:0.03001\n",
      "[134]\teval-rmse:0.03001\n",
      "[135]\teval-rmse:0.03001\n",
      "[136]\teval-rmse:0.03001\n",
      "[137]\teval-rmse:0.03001\n",
      "[138]\teval-rmse:0.03001\n",
      "[139]\teval-rmse:0.03001\n",
      "[140]\teval-rmse:0.03001\n",
      "[141]\teval-rmse:0.03001\n",
      "[142]\teval-rmse:0.03001\n",
      "[143]\teval-rmse:0.03001\n",
      "[144]\teval-rmse:0.03001\n",
      "[145]\teval-rmse:0.03001\n",
      "[146]\teval-rmse:0.03001\n",
      "[147]\teval-rmse:0.03001\n",
      "[148]\teval-rmse:0.03001\n",
      "[149]\teval-rmse:0.03001\n",
      "[150]\teval-rmse:0.03001\n",
      "[151]\teval-rmse:0.03001\n",
      "[152]\teval-rmse:0.03001\n",
      "[153]\teval-rmse:0.03001\n",
      "[154]\teval-rmse:0.03001\n",
      "[155]\teval-rmse:0.03001\n",
      "[156]\teval-rmse:0.03001\n",
      "[157]\teval-rmse:0.03001\n",
      "[158]\teval-rmse:0.03001\n",
      "[159]\teval-rmse:0.03001\n",
      "[160]\teval-rmse:0.03001\n",
      "[161]\teval-rmse:0.03001\n",
      "[162]\teval-rmse:0.03001\n",
      "[163]\teval-rmse:0.03001\n",
      "[164]\teval-rmse:0.03001\n",
      "[165]\teval-rmse:0.03001\n",
      "[166]\teval-rmse:0.03001\n",
      "[167]\teval-rmse:0.03001\n",
      "[168]\teval-rmse:0.03001\n",
      "[169]\teval-rmse:0.03001\n",
      "[170]\teval-rmse:0.03001\n",
      "[171]\teval-rmse:0.03001\n",
      "[172]\teval-rmse:0.03001\n",
      "[173]\teval-rmse:0.03001\n",
      "[174]\teval-rmse:0.03001\n",
      "[175]\teval-rmse:0.03001\n",
      "[176]\teval-rmse:0.03001\n",
      "[177]\teval-rmse:0.03001\n",
      "[178]\teval-rmse:0.03001\n",
      "[179]\teval-rmse:0.03001\n",
      "[180]\teval-rmse:0.03001\n",
      "[181]\teval-rmse:0.03001\n",
      "[182]\teval-rmse:0.03001\n",
      "[183]\teval-rmse:0.03001\n",
      "[184]\teval-rmse:0.03001\n",
      "[185]\teval-rmse:0.03001\n",
      "[186]\teval-rmse:0.03001\n",
      "[187]\teval-rmse:0.03001\n",
      "[188]\teval-rmse:0.03001\n",
      "[189]\teval-rmse:0.03001\n",
      "[190]\teval-rmse:0.03001\n",
      "[191]\teval-rmse:0.03001\n",
      "[192]\teval-rmse:0.03001\n",
      "[193]\teval-rmse:0.03001\n",
      "[194]\teval-rmse:0.03001\n",
      "[195]\teval-rmse:0.03001\n",
      "[196]\teval-rmse:0.03001\n",
      "[197]\teval-rmse:0.03001\n",
      "[198]\teval-rmse:0.03001\n",
      "[199]\teval-rmse:0.03001\n",
      "[0]\teval-rmse:0.02986\n",
      "[1]\teval-rmse:0.02986\n",
      "[2]\teval-rmse:0.02986\n",
      "[3]\teval-rmse:0.02986\n",
      "[4]\teval-rmse:0.02986\n",
      "[5]\teval-rmse:0.02986\n",
      "[6]\teval-rmse:0.02986\n",
      "[7]\teval-rmse:0.02986\n",
      "[8]\teval-rmse:0.02986\n",
      "[9]\teval-rmse:0.02986\n",
      "[10]\teval-rmse:0.02986\n",
      "[11]\teval-rmse:0.02986\n",
      "[12]\teval-rmse:0.02986\n",
      "[13]\teval-rmse:0.02986\n",
      "[14]\teval-rmse:0.02986\n",
      "[15]\teval-rmse:0.02986\n",
      "[16]\teval-rmse:0.02986\n",
      "[17]\teval-rmse:0.02986\n",
      "[18]\teval-rmse:0.02986\n",
      "[19]\teval-rmse:0.02986\n",
      "[20]\teval-rmse:0.02986\n",
      "[21]\teval-rmse:0.02986\n",
      "[22]\teval-rmse:0.02986\n",
      "[23]\teval-rmse:0.02986\n",
      "[24]\teval-rmse:0.02986\n",
      "[25]\teval-rmse:0.02986\n",
      "[26]\teval-rmse:0.02986\n",
      "[27]\teval-rmse:0.02986\n",
      "[28]\teval-rmse:0.02986\n",
      "[29]\teval-rmse:0.02986\n",
      "[30]\teval-rmse:0.02986\n",
      "[31]\teval-rmse:0.02986\n",
      "[32]\teval-rmse:0.02986\n",
      "[33]\teval-rmse:0.02986\n",
      "[34]\teval-rmse:0.02986\n",
      "[35]\teval-rmse:0.02986\n",
      "[36]\teval-rmse:0.02986\n",
      "[37]\teval-rmse:0.02986\n",
      "[38]\teval-rmse:0.02986\n",
      "[39]\teval-rmse:0.02986\n",
      "[40]\teval-rmse:0.02986\n",
      "[41]\teval-rmse:0.02986\n",
      "[42]\teval-rmse:0.02986\n",
      "[43]\teval-rmse:0.02986\n",
      "[44]\teval-rmse:0.02986\n",
      "[45]\teval-rmse:0.02986\n",
      "[46]\teval-rmse:0.02986\n",
      "[47]\teval-rmse:0.02986\n",
      "[48]\teval-rmse:0.02986\n",
      "[49]\teval-rmse:0.02986\n",
      "[50]\teval-rmse:0.02986\n",
      "[51]\teval-rmse:0.02986\n",
      "[52]\teval-rmse:0.02986\n",
      "[53]\teval-rmse:0.02986\n",
      "[54]\teval-rmse:0.02986\n",
      "[55]\teval-rmse:0.02986\n",
      "[56]\teval-rmse:0.02986\n",
      "[57]\teval-rmse:0.02986\n",
      "[58]\teval-rmse:0.02986\n",
      "[59]\teval-rmse:0.02986\n",
      "[60]\teval-rmse:0.02986\n",
      "[61]\teval-rmse:0.02986\n",
      "[62]\teval-rmse:0.02986\n",
      "[63]\teval-rmse:0.02986\n",
      "[64]\teval-rmse:0.02986\n",
      "[65]\teval-rmse:0.02986\n",
      "[66]\teval-rmse:0.02985\n",
      "[67]\teval-rmse:0.02985\n",
      "[68]\teval-rmse:0.02985\n",
      "[69]\teval-rmse:0.02985\n",
      "[70]\teval-rmse:0.02985\n",
      "[71]\teval-rmse:0.02985\n",
      "[72]\teval-rmse:0.02985\n",
      "[73]\teval-rmse:0.02985\n",
      "[74]\teval-rmse:0.02985\n",
      "[75]\teval-rmse:0.02985\n",
      "[76]\teval-rmse:0.02985\n",
      "[77]\teval-rmse:0.02985\n",
      "[78]\teval-rmse:0.02985\n",
      "[79]\teval-rmse:0.02985\n",
      "[80]\teval-rmse:0.02985\n",
      "[81]\teval-rmse:0.02985\n",
      "[82]\teval-rmse:0.02985\n",
      "[83]\teval-rmse:0.02985\n",
      "[84]\teval-rmse:0.02985\n",
      "[85]\teval-rmse:0.02985\n",
      "[86]\teval-rmse:0.02985\n",
      "[87]\teval-rmse:0.02985\n",
      "[88]\teval-rmse:0.02985\n",
      "[89]\teval-rmse:0.02985\n",
      "[90]\teval-rmse:0.02985\n",
      "[91]\teval-rmse:0.02985\n",
      "[92]\teval-rmse:0.02985\n",
      "[93]\teval-rmse:0.02985\n",
      "[94]\teval-rmse:0.02985\n",
      "[95]\teval-rmse:0.02985\n",
      "[96]\teval-rmse:0.02985\n",
      "[97]\teval-rmse:0.02985\n",
      "[98]\teval-rmse:0.02985\n",
      "[99]\teval-rmse:0.02985\n",
      "[100]\teval-rmse:0.02985\n",
      "[101]\teval-rmse:0.02985\n",
      "[102]\teval-rmse:0.02985\n",
      "[103]\teval-rmse:0.02985\n",
      "[104]\teval-rmse:0.02985\n",
      "[105]\teval-rmse:0.02985\n",
      "[106]\teval-rmse:0.02985\n",
      "[107]\teval-rmse:0.02985\n",
      "[108]\teval-rmse:0.02985\n",
      "[109]\teval-rmse:0.02985\n",
      "[110]\teval-rmse:0.02985\n",
      "[111]\teval-rmse:0.02985\n",
      "[112]\teval-rmse:0.02985\n",
      "[113]\teval-rmse:0.02985\n",
      "[114]\teval-rmse:0.02985\n",
      "[115]\teval-rmse:0.02985\n",
      "[116]\teval-rmse:0.02985\n",
      "[117]\teval-rmse:0.02985\n",
      "[118]\teval-rmse:0.02985\n",
      "[119]\teval-rmse:0.02985\n",
      "[120]\teval-rmse:0.02985\n",
      "[121]\teval-rmse:0.02985\n",
      "[122]\teval-rmse:0.02985\n",
      "[123]\teval-rmse:0.02985\n",
      "[124]\teval-rmse:0.02985\n",
      "[125]\teval-rmse:0.02985\n",
      "[126]\teval-rmse:0.02985\n",
      "[127]\teval-rmse:0.02985\n",
      "[128]\teval-rmse:0.02985\n",
      "[129]\teval-rmse:0.02985\n",
      "[130]\teval-rmse:0.02985\n",
      "[131]\teval-rmse:0.02985\n",
      "[132]\teval-rmse:0.02985\n",
      "[133]\teval-rmse:0.02985\n",
      "[134]\teval-rmse:0.02985\n",
      "[135]\teval-rmse:0.02985\n",
      "[136]\teval-rmse:0.02985\n",
      "[137]\teval-rmse:0.02985\n",
      "[138]\teval-rmse:0.02985\n",
      "[139]\teval-rmse:0.02985\n",
      "[140]\teval-rmse:0.02985\n",
      "[141]\teval-rmse:0.02985\n",
      "[142]\teval-rmse:0.02985\n",
      "[143]\teval-rmse:0.02985\n",
      "[144]\teval-rmse:0.02985\n",
      "[145]\teval-rmse:0.02985\n",
      "[146]\teval-rmse:0.02985\n",
      "[147]\teval-rmse:0.02985\n",
      "[148]\teval-rmse:0.02985\n",
      "[149]\teval-rmse:0.02985\n",
      "[150]\teval-rmse:0.02985\n",
      "[151]\teval-rmse:0.02985\n",
      "[152]\teval-rmse:0.02985\n",
      "[153]\teval-rmse:0.02985\n",
      "[154]\teval-rmse:0.02985\n",
      "[155]\teval-rmse:0.02985\n",
      "[156]\teval-rmse:0.02985\n",
      "[157]\teval-rmse:0.02985\n",
      "[158]\teval-rmse:0.02985\n",
      "[159]\teval-rmse:0.02985\n",
      "[160]\teval-rmse:0.02985\n",
      "[161]\teval-rmse:0.02985\n",
      "[162]\teval-rmse:0.02985\n",
      "[163]\teval-rmse:0.02985\n",
      "[164]\teval-rmse:0.02985\n",
      "[165]\teval-rmse:0.02985\n",
      "[166]\teval-rmse:0.02985\n",
      "[167]\teval-rmse:0.02985\n",
      "[168]\teval-rmse:0.02985\n",
      "[169]\teval-rmse:0.02985\n",
      "[170]\teval-rmse:0.02985\n",
      "[171]\teval-rmse:0.02985\n",
      "[172]\teval-rmse:0.02985\n",
      "[173]\teval-rmse:0.02985\n",
      "[174]\teval-rmse:0.02985\n",
      "[175]\teval-rmse:0.02985\n",
      "[176]\teval-rmse:0.02985\n",
      "[177]\teval-rmse:0.02985\n",
      "[178]\teval-rmse:0.02985\n",
      "[179]\teval-rmse:0.02985\n",
      "[180]\teval-rmse:0.02985\n",
      "[181]\teval-rmse:0.02985\n",
      "[182]\teval-rmse:0.02985\n",
      "[183]\teval-rmse:0.02985\n",
      "[184]\teval-rmse:0.02985\n",
      "[185]\teval-rmse:0.02985\n",
      "[186]\teval-rmse:0.02985\n",
      "[187]\teval-rmse:0.02985\n",
      "[188]\teval-rmse:0.02985\n",
      "[189]\teval-rmse:0.02985\n",
      "[190]\teval-rmse:0.02985\n",
      "[191]\teval-rmse:0.02985\n",
      "[192]\teval-rmse:0.02985\n",
      "[193]\teval-rmse:0.02985\n",
      "[194]\teval-rmse:0.02985\n",
      "[195]\teval-rmse:0.02985\n",
      "[196]\teval-rmse:0.02985\n",
      "[197]\teval-rmse:0.02985\n",
      "[198]\teval-rmse:0.02985\n",
      "[199]\teval-rmse:0.02985\n",
      "[0]\teval-rmse:0.03016\n",
      "[1]\teval-rmse:0.03016\n",
      "[2]\teval-rmse:0.03016\n",
      "[3]\teval-rmse:0.03016\n",
      "[4]\teval-rmse:0.03016\n",
      "[5]\teval-rmse:0.03016\n",
      "[6]\teval-rmse:0.03016\n",
      "[7]\teval-rmse:0.03016\n",
      "[8]\teval-rmse:0.03016\n",
      "[9]\teval-rmse:0.03016\n",
      "[10]\teval-rmse:0.03016\n",
      "[11]\teval-rmse:0.03016\n",
      "[12]\teval-rmse:0.03016\n",
      "[13]\teval-rmse:0.03016\n",
      "[14]\teval-rmse:0.03016\n",
      "[15]\teval-rmse:0.03016\n",
      "[16]\teval-rmse:0.03016\n",
      "[17]\teval-rmse:0.03016\n",
      "[18]\teval-rmse:0.03016\n",
      "[19]\teval-rmse:0.03016\n",
      "[20]\teval-rmse:0.03016\n",
      "[21]\teval-rmse:0.03016\n",
      "[22]\teval-rmse:0.03016\n",
      "[23]\teval-rmse:0.03016\n",
      "[24]\teval-rmse:0.03016\n",
      "[25]\teval-rmse:0.03016\n",
      "[26]\teval-rmse:0.03016\n",
      "[27]\teval-rmse:0.03016\n",
      "[28]\teval-rmse:0.03016\n",
      "[29]\teval-rmse:0.03016\n",
      "[30]\teval-rmse:0.03016\n",
      "[31]\teval-rmse:0.03016\n",
      "[32]\teval-rmse:0.03016\n",
      "[33]\teval-rmse:0.03016\n",
      "[34]\teval-rmse:0.03016\n",
      "[35]\teval-rmse:0.03016\n",
      "[36]\teval-rmse:0.03016\n",
      "[37]\teval-rmse:0.03016\n",
      "[38]\teval-rmse:0.03016\n",
      "[39]\teval-rmse:0.03016\n",
      "[40]\teval-rmse:0.03016\n",
      "[41]\teval-rmse:0.03016\n",
      "[42]\teval-rmse:0.03016\n",
      "[43]\teval-rmse:0.03016\n",
      "[44]\teval-rmse:0.03016\n",
      "[45]\teval-rmse:0.03016\n",
      "[46]\teval-rmse:0.03016\n",
      "[47]\teval-rmse:0.03016\n",
      "[48]\teval-rmse:0.03016\n",
      "[49]\teval-rmse:0.03016\n",
      "[50]\teval-rmse:0.03016\n",
      "[51]\teval-rmse:0.03016\n",
      "[0]\teval-rmse:0.02988\n",
      "[1]\teval-rmse:0.02988\n",
      "[2]\teval-rmse:0.02988\n",
      "[3]\teval-rmse:0.02988\n",
      "[4]\teval-rmse:0.02988\n",
      "[5]\teval-rmse:0.02988\n",
      "[6]\teval-rmse:0.02988\n",
      "[7]\teval-rmse:0.02988\n",
      "[8]\teval-rmse:0.02988\n",
      "[9]\teval-rmse:0.02988\n",
      "[10]\teval-rmse:0.02988\n",
      "[11]\teval-rmse:0.02988\n",
      "[12]\teval-rmse:0.02988\n",
      "[13]\teval-rmse:0.02988\n",
      "[14]\teval-rmse:0.02988\n",
      "[15]\teval-rmse:0.02988\n",
      "[16]\teval-rmse:0.02988\n",
      "[17]\teval-rmse:0.02988\n",
      "[18]\teval-rmse:0.02988\n",
      "[19]\teval-rmse:0.02988\n",
      "[0]\teval-rmse:0.02983\n",
      "[1]\teval-rmse:0.02983\n",
      "[2]\teval-rmse:0.02983\n",
      "[3]\teval-rmse:0.02983\n",
      "[4]\teval-rmse:0.02983\n",
      "[5]\teval-rmse:0.02983\n",
      "[6]\teval-rmse:0.02983\n",
      "[7]\teval-rmse:0.02983\n",
      "[8]\teval-rmse:0.02983\n",
      "[9]\teval-rmse:0.02983\n",
      "[10]\teval-rmse:0.02983\n",
      "[11]\teval-rmse:0.02983\n",
      "[12]\teval-rmse:0.02983\n",
      "[13]\teval-rmse:0.02983\n",
      "[14]\teval-rmse:0.02983\n",
      "[15]\teval-rmse:0.02983\n",
      "[16]\teval-rmse:0.02983\n",
      "[17]\teval-rmse:0.02983\n",
      "[18]\teval-rmse:0.02983\n",
      "[19]\teval-rmse:0.02983\n",
      "[20]\teval-rmse:0.02983\n",
      "[21]\teval-rmse:0.02983\n",
      "[22]\teval-rmse:0.02983\n",
      "[23]\teval-rmse:0.02983\n",
      "[24]\teval-rmse:0.02983\n",
      "[25]\teval-rmse:0.02983\n",
      "[26]\teval-rmse:0.02983\n",
      "[27]\teval-rmse:0.02983\n",
      "[28]\teval-rmse:0.02983\n",
      "[29]\teval-rmse:0.02983\n",
      "[30]\teval-rmse:0.02983\n",
      "[31]\teval-rmse:0.02983\n",
      "[32]\teval-rmse:0.02983\n",
      "[33]\teval-rmse:0.02983\n",
      "[34]\teval-rmse:0.02983\n",
      "[35]\teval-rmse:0.02983\n",
      "[36]\teval-rmse:0.02983\n",
      "[37]\teval-rmse:0.02983\n",
      "[38]\teval-rmse:0.02983\n",
      "[39]\teval-rmse:0.02983\n",
      "[40]\teval-rmse:0.02983\n",
      "[41]\teval-rmse:0.02983\n",
      "[42]\teval-rmse:0.02983\n",
      "[43]\teval-rmse:0.02983\n",
      "[44]\teval-rmse:0.02983\n",
      "[45]\teval-rmse:0.02983\n",
      "[46]\teval-rmse:0.02983\n",
      "[47]\teval-rmse:0.02983\n",
      "[48]\teval-rmse:0.02983\n",
      "[49]\teval-rmse:0.02983\n",
      "[50]\teval-rmse:0.02983\n",
      "[51]\teval-rmse:0.02983\n",
      "[52]\teval-rmse:0.02983\n",
      "[53]\teval-rmse:0.02983\n",
      "[54]\teval-rmse:0.02983\n",
      "[55]\teval-rmse:0.02983\n",
      "[56]\teval-rmse:0.02983\n",
      "[57]\teval-rmse:0.02982\n",
      "[58]\teval-rmse:0.02982\n",
      "[59]\teval-rmse:0.02982\n",
      "[60]\teval-rmse:0.02982\n",
      "[61]\teval-rmse:0.02982\n",
      "[62]\teval-rmse:0.02982\n",
      "[63]\teval-rmse:0.02982\n",
      "[64]\teval-rmse:0.02982\n",
      "[65]\teval-rmse:0.02982\n",
      "[66]\teval-rmse:0.02982\n",
      "[67]\teval-rmse:0.02982\n",
      "[68]\teval-rmse:0.02982\n",
      "[69]\teval-rmse:0.02982\n",
      "[70]\teval-rmse:0.02982\n",
      "[71]\teval-rmse:0.02982\n",
      "[72]\teval-rmse:0.02982\n",
      "[73]\teval-rmse:0.02982\n",
      "[74]\teval-rmse:0.02982\n",
      "[75]\teval-rmse:0.02982\n",
      "[76]\teval-rmse:0.02982\n",
      "[77]\teval-rmse:0.02982\n",
      "[78]\teval-rmse:0.02982\n",
      "[79]\teval-rmse:0.02982\n",
      "[80]\teval-rmse:0.02982\n",
      "[81]\teval-rmse:0.02982\n",
      "[82]\teval-rmse:0.02982\n",
      "[83]\teval-rmse:0.02982\n",
      "[84]\teval-rmse:0.02982\n",
      "[85]\teval-rmse:0.02982\n",
      "[86]\teval-rmse:0.02982\n",
      "[87]\teval-rmse:0.02982\n",
      "[88]\teval-rmse:0.02982\n",
      "[89]\teval-rmse:0.02982\n",
      "[90]\teval-rmse:0.02982\n",
      "[91]\teval-rmse:0.02982\n",
      "[92]\teval-rmse:0.02982\n",
      "[93]\teval-rmse:0.02982\n",
      "[94]\teval-rmse:0.02982\n",
      "[95]\teval-rmse:0.02982\n",
      "[96]\teval-rmse:0.02982\n",
      "[97]\teval-rmse:0.02982\n",
      "[98]\teval-rmse:0.02982\n",
      "[99]\teval-rmse:0.02982\n",
      "[100]\teval-rmse:0.02982\n",
      "[101]\teval-rmse:0.02982\n",
      "[102]\teval-rmse:0.02982\n",
      "[103]\teval-rmse:0.02982\n",
      "[104]\teval-rmse:0.02982\n",
      "[105]\teval-rmse:0.02982\n",
      "[106]\teval-rmse:0.02982\n",
      "[107]\teval-rmse:0.02982\n",
      "[108]\teval-rmse:0.02982\n",
      "[109]\teval-rmse:0.02982\n",
      "[110]\teval-rmse:0.02982\n",
      "[111]\teval-rmse:0.02982\n",
      "[112]\teval-rmse:0.02982\n",
      "[113]\teval-rmse:0.02982\n",
      "[114]\teval-rmse:0.02982\n",
      "[115]\teval-rmse:0.02982\n",
      "[116]\teval-rmse:0.02982\n",
      "[117]\teval-rmse:0.02982\n",
      "[118]\teval-rmse:0.02982\n",
      "[119]\teval-rmse:0.02982\n",
      "[120]\teval-rmse:0.02982\n",
      "[121]\teval-rmse:0.02982\n",
      "[122]\teval-rmse:0.02982\n",
      "[123]\teval-rmse:0.02982\n",
      "[124]\teval-rmse:0.02982\n",
      "[125]\teval-rmse:0.02982\n",
      "[126]\teval-rmse:0.02982\n",
      "[127]\teval-rmse:0.02982\n",
      "[128]\teval-rmse:0.02982\n",
      "[129]\teval-rmse:0.02982\n",
      "[130]\teval-rmse:0.02982\n",
      "[131]\teval-rmse:0.02982\n",
      "[132]\teval-rmse:0.02982\n",
      "[133]\teval-rmse:0.02982\n",
      "[134]\teval-rmse:0.02982\n",
      "[135]\teval-rmse:0.02982\n",
      "[136]\teval-rmse:0.02982\n",
      "[137]\teval-rmse:0.02982\n",
      "[138]\teval-rmse:0.02982\n",
      "[139]\teval-rmse:0.02982\n",
      "[140]\teval-rmse:0.02982\n",
      "[141]\teval-rmse:0.02982\n",
      "[142]\teval-rmse:0.02982\n",
      "[143]\teval-rmse:0.02982\n",
      "[144]\teval-rmse:0.02982\n",
      "[145]\teval-rmse:0.02982\n",
      "[146]\teval-rmse:0.02982\n",
      "[147]\teval-rmse:0.02982\n",
      "[148]\teval-rmse:0.02982\n",
      "[149]\teval-rmse:0.02982\n",
      "[150]\teval-rmse:0.02982\n",
      "[151]\teval-rmse:0.02982\n",
      "[152]\teval-rmse:0.02982\n",
      "[153]\teval-rmse:0.02982\n",
      "[154]\teval-rmse:0.02982\n",
      "[155]\teval-rmse:0.02982\n",
      "[156]\teval-rmse:0.02982\n",
      "[157]\teval-rmse:0.02982\n",
      "[158]\teval-rmse:0.02982\n",
      "[159]\teval-rmse:0.02982\n",
      "[160]\teval-rmse:0.02982\n",
      "[161]\teval-rmse:0.02982\n",
      "[162]\teval-rmse:0.02982\n",
      "[163]\teval-rmse:0.02982\n",
      "[164]\teval-rmse:0.02982\n",
      "[165]\teval-rmse:0.02982\n",
      "[166]\teval-rmse:0.02982\n",
      "[167]\teval-rmse:0.02982\n",
      "[168]\teval-rmse:0.02982\n",
      "[169]\teval-rmse:0.02982\n",
      "[170]\teval-rmse:0.02982\n",
      "[171]\teval-rmse:0.02982\n",
      "[172]\teval-rmse:0.02982\n",
      "[173]\teval-rmse:0.02982\n",
      "[174]\teval-rmse:0.02982\n",
      "[175]\teval-rmse:0.02982\n",
      "[176]\teval-rmse:0.02982\n",
      "[177]\teval-rmse:0.02982\n",
      "[178]\teval-rmse:0.02982\n",
      "[179]\teval-rmse:0.02982\n",
      "[180]\teval-rmse:0.02982\n",
      "[181]\teval-rmse:0.02982\n",
      "[182]\teval-rmse:0.02982\n",
      "[183]\teval-rmse:0.02982\n",
      "[184]\teval-rmse:0.02982\n",
      "[185]\teval-rmse:0.02982\n",
      "[186]\teval-rmse:0.02982\n",
      "[187]\teval-rmse:0.02982\n",
      "[188]\teval-rmse:0.02981\n",
      "[189]\teval-rmse:0.02982\n",
      "[190]\teval-rmse:0.02981\n",
      "[191]\teval-rmse:0.02981\n",
      "[192]\teval-rmse:0.02981\n",
      "[193]\teval-rmse:0.02981\n",
      "[194]\teval-rmse:0.02981\n",
      "[195]\teval-rmse:0.02981\n",
      "[196]\teval-rmse:0.02981\n",
      "[197]\teval-rmse:0.02981\n",
      "[198]\teval-rmse:0.02981\n",
      "[199]\teval-rmse:0.02981\n",
      "\n",
      "[Training CAT]\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Full Mitsui Commodity Prediction Pipeline \n",
    "# Training + Inference + simple Feature Engineering\n",
    "# + Inference  All in one notebook.\n",
    "# ====================================================\n",
    "\n",
    "import os, gc, warnings, random, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "\n",
    "import kaggle_evaluation.mitsui_inference_server\n",
    "\n",
    "# ====================================================\n",
    "# Config\n",
    "# ====================================================\n",
    "class Config:\n",
    "    AUTHOR = 'mitsui_ai'\n",
    "    VERSION = 2\n",
    "    SEED = 42\n",
    "    N_FOLDS = 5\n",
    "    BOOSTERS = ['lgbm', 'xgb', 'cat']\n",
    "    MAX_ROUNDS = 200\n",
    "    EARLY_STOP = 20\n",
    "    VERBOSE = 1\n",
    "    DATA_DIR = Path('/kaggle/input/mitsui-commodity-prediction-challenge')\n",
    "    MODEL_DIR = Path('./models'); os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    OOF_DIR = Path('./oof'); os.makedirs(OOF_DIR, exist_ok=True)\n",
    "    TARGET_COUNT = 424\n",
    "    FEATURES_TO_ADD = ['target_id']\n",
    "\n",
    "    LGBM_PARAMS = {\n",
    "        'objective': 'regression', 'metric': 'rmse',\n",
    "        'learning_rate': 0.005, 'num_leaves': 8, 'seed': SEED,\n",
    "            # 'device': 'gpu',                 # <--- Enable GPU\n",
    "            # 'gpu_platform_id': 0,           # Optional: specify platform/device\n",
    "            # 'gpu_device_id': 0,\n",
    " \n",
    "\n",
    "    }\n",
    "\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse',\n",
    "        'learning_rate': 0.005, 'max_depth': 4, 'random_state': SEED,\n",
    "        #     'tree_method': 'gpu_hist',      # <--- GPU-accelerated algorithm\n",
    "        # 'predictor': 'gpu_predictor',   # Use GPU for prediction\n",
    "\n",
    "    }\n",
    "\n",
    "    CAT_PARAMS = {\n",
    "        'loss_function': 'RMSE', 'learning_rate': 0.005,\n",
    "        'iterations': MAX_ROUNDS, 'depth': 4,\n",
    "        'random_seed': SEED, 'verbose': False,\n",
    "        # 'task_type': 'GPU',             # <--- Enable GPU\n",
    "        # 'devices': '0:1',                 # Device ID\n",
    "    \n",
    "    }\n",
    "\n",
    "# ====================================================\n",
    "# Seed and Utility\n",
    "# ====================================================\n",
    "def set_seed(seed=Config.SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed()\n",
    "\n",
    "# ====================================================\n",
    "# Feature Engineering\n",
    "# ====================================================\n",
    "def add_features(df):\n",
    "    df['dayofweek'] = df['date_id'] % 7\n",
    "    df['month'] = (df['date_id'] // 30) % 12\n",
    "    df['quarter'] = df['month'] // 3\n",
    "    df['day_of_month'] = df['date_id'] % 30\n",
    "\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "    df['is_month_start'] = (df['day_of_month'] == 0).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] == 29).astype(int)\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Train Booster\n",
    "# ====================================================\n",
    "def train_model(booster, x_tr, y_tr, x_val, y_val):\n",
    "    if booster == 'lgbm':\n",
    "        train_set = lgb.Dataset(x_tr, y_tr)\n",
    "        val_set = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(\n",
    "            Config.LGBM_PARAMS, train_set,\n",
    "            num_boost_round=Config.MAX_ROUNDS,\n",
    "            valid_sets=[val_set],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(Config.EARLY_STOP),\n",
    "                lgb.log_evaluation(Config.VERBOSE)\n",
    "            ]\n",
    "        )\n",
    "        return model, model.predict(x_val)\n",
    "\n",
    "    elif booster == 'xgb':\n",
    "        train_d = xgb.DMatrix(x_tr, label=y_tr)\n",
    "        valid_d = xgb.DMatrix(x_val, label=y_val)\n",
    "        model = xgb.train(\n",
    "            Config.XGB_PARAMS, train_d,\n",
    "            num_boost_round=Config.MAX_ROUNDS,\n",
    "            evals=[(valid_d, 'eval')],\n",
    "            early_stopping_rounds=Config.EARLY_STOP,\n",
    "            verbose_eval=Config.VERBOSE\n",
    "        )\n",
    "        return model, model.predict(xgb.DMatrix(x_val))\n",
    "\n",
    "    elif booster == 'cat':\n",
    "        train_pool = Pool(x_tr, label=y_tr)\n",
    "        val_pool = Pool(x_val, label=y_val)\n",
    "        model = CatBoostRegressor(**Config.CAT_PARAMS)\n",
    "        model.fit(train_pool, eval_set=val_pool,\n",
    "                  early_stopping_rounds=Config.EARLY_STOP)\n",
    "        return model, model.predict(x_val)\n",
    "\n",
    "# ====================================================\n",
    "# Training CV Wrapper\n",
    "# ====================================================\n",
    "def run_cv(booster, df, features):\n",
    "    oof_preds = np.zeros(len(df))\n",
    "    kf = KFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        x_tr = df.loc[train_idx, features]\n",
    "        y_tr = df.loc[train_idx, 'target']\n",
    "        x_val = df.loc[val_idx, features]\n",
    "        y_val = df.loc[val_idx, 'target']\n",
    "\n",
    "        model, val_preds = train_model(booster, x_tr, y_tr, x_val, y_val)\n",
    "        oof_preds[val_idx] = val_preds\n",
    "\n",
    "        with open(Config.MODEL_DIR / f'{booster}_fold{fold}.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    np.save(Config.OOF_DIR / f'oof_{booster}.npy', oof_preds)\n",
    "\n",
    "# ====================================================\n",
    "# Load Data\n",
    "# ====================================================\n",
    "train_df = pl.read_csv(Config.DATA_DIR / 'train.csv').to_pandas()\n",
    "label_df = pl.read_csv(Config.DATA_DIR / 'train_labels.csv').to_pandas()\n",
    "\n",
    "features = list(train_df.columns[1:]) + Config.FEATURES_TO_ADD\n",
    "\n",
    "df_all = []\n",
    "for j, col in enumerate(label_df.columns[1:]):\n",
    "    temp = train_df.copy()\n",
    "    temp['target'] = label_df[col]\n",
    "    temp['target_id'] = j\n",
    "    temp = add_features(temp)\n",
    "    temp = temp.dropna(subset=['target'])\n",
    "    df_all.append(temp)\n",
    "\n",
    "train_full = pd.concat(df_all, axis=0).reset_index(drop=True)\n",
    "\n",
    "# ====================================================\n",
    "# Train All Boosters\n",
    "# ====================================================\n",
    "for booster in Config.BOOSTERS:\n",
    "    print(f\"\\n[Training {booster.upper()}]\")\n",
    "    run_cv(booster, train_full, features + [\n",
    "          'dayofweek', 'month', 'quarter', 'day_of_month',\n",
    "        'is_weekend', 'is_month_start', 'is_month_end'\n",
    "    \n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e745e2d",
   "metadata": {
    "papermill": {
     "duration": 0.063351,
     "end_time": "2025-10-05T15:18:23.267623",
     "exception": false,
     "start_time": "2025-10-05T15:18:23.204272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43888f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:18:23.398357Z",
     "iopub.status.busy": "2025-10-05T15:18:23.397960Z",
     "iopub.status.idle": "2025-10-05T15:20:11.887646Z",
     "shell.execute_reply": "2025-10-05T15:20:11.886534Z"
    },
    "papermill": {
     "duration": 108.558409,
     "end_time": "2025-10-05T15:20:11.889761",
     "exception": false,
     "start_time": "2025-10-05T15:18:23.331352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/templates.py:96: RuntimeWarning: 1665 seconds elapsed before server startup.\n",
      "                This exceeds the startup time limit of 900 seconds that the gateway will enforce\n",
      "                during the rerun on the hidden test set. Start the server before performing any time consuming steps.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Inference Wrapper\n",
    "# ====================================================\n",
    "model_registry = {}\n",
    "for booster in Config.BOOSTERS:\n",
    "    model_registry[booster] = []\n",
    "    for fold in range(Config.N_FOLDS):\n",
    "        with open(f'/kaggle/working/models/{booster}_fold{fold}.pkl', 'rb') as f:\n",
    "            model_registry[booster].append(pickle.load(f))\n",
    "\n",
    "\n",
    "def predict(test, *_):\n",
    "    test_df = test.to_pandas()\n",
    "    for col in test_df.columns:\n",
    "        if test_df[col].dtype == 'object':\n",
    "            test_df[col] = np.nan\n",
    "    test_df = add_features(test_df)\n",
    "\n",
    "    test_blocks = []\n",
    "    for tid in range(Config.TARGET_COUNT):\n",
    "        temp = test_df.copy()\n",
    "        temp['target_id'] = tid\n",
    "        test_blocks.append(temp)\n",
    "\n",
    "    x_test = pd.concat(test_blocks, axis=0)\n",
    "    features_final = list(train_df.columns[1:]) + Config.FEATURES_TO_ADD + [\n",
    "              'dayofweek', 'month', 'quarter', 'day_of_month',\n",
    "            'is_weekend', 'is_month_start', 'is_month_end'\n",
    "    ]\n",
    "    \n",
    "    x_test = x_test[features_final]\n",
    "\n",
    "    preds = []\n",
    "    for booster in Config.BOOSTERS:\n",
    "        for model in model_registry[booster]:\n",
    "            if booster == 'lgbm':\n",
    "                preds.append(model.predict(x_test, predict_disable_shape_check=True))\n",
    "            elif booster == 'xgb':\n",
    "                preds.append(model.predict(xgb.DMatrix(x_test)))\n",
    "            elif booster == 'cat':\n",
    "                preds.append(model.predict(x_test))\n",
    "    preds = np.mean(np.array(preds), axis=0)\n",
    "\n",
    "    return pl.DataFrame({f'target_{i}': preds[i] for i in range(Config.TARGET_COUNT)})\n",
    "\n",
    "# ====================================================\n",
    "# Serve Model (Locally or on Hidden Test)\n",
    "# ====================================================\n",
    "inference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((Config.DATA_DIR,))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13613251,
     "sourceId": 94771,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1803.351847,
   "end_time": "2025-10-05T15:20:15.464155",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-05T14:50:12.112308",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
