{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98010fb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:48:26.587305Z",
     "iopub.status.busy": "2026-02-10T16:48:26.587008Z",
     "iopub.status.idle": "2026-02-10T16:48:40.085802Z",
     "shell.execute_reply": "2026-02-10T16:48:40.085233Z"
    },
    "papermill": {
     "duration": 13.50352,
     "end_time": "2026-02-10T16:48:40.087481",
     "exception": false,
     "start_time": "2026-02-10T16:48:26.583961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import more_itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b26247fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:48:40.092165Z",
     "iopub.status.busy": "2026-02-10T16:48:40.091540Z",
     "iopub.status.idle": "2026-02-10T16:49:12.728164Z",
     "shell.execute_reply": "2026-02-10T16:49:12.727400Z"
    },
    "papermill": {
     "duration": 32.642253,
     "end_time": "2026-02-10T16:49:12.731455",
     "exception": false,
     "start_time": "2026-02-10T16:48:40.089202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 16:48:45.192544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770742125.379407      23 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770742125.430493      23 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770742125.876472      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770742125.876501      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770742125.876504      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770742125.876507      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForCausalLM(\n",
      "  (model): Gemma3TextModel(\n",
      "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma3DecoderLayer(\n",
      "        (self_attn): Gemma3Attention(\n",
      "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
      "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Gemma3MLP(\n",
      "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
      "          (act_fn): GELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "    (rotary_emb): Gemma3RotaryEmbedding()\n",
      "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "GEMMA_PATH = kagglehub.model_download(\"google/gemma-3/transformers/gemma-3-1b-it\")\n",
    "processor = AutoTokenizer.from_pretrained(GEMMA_PATH)\n",
    "\n",
    "# Determine if CUDA (GPU) is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(GEMMA_PATH).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61b625f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:49:12.736256Z",
     "iopub.status.busy": "2026-02-10T16:49:12.735662Z",
     "iopub.status.idle": "2026-02-10T16:49:12.750744Z",
     "shell.execute_reply": "2026-02-10T16:49:12.750226Z"
    },
    "papermill": {
     "duration": 0.019186,
     "end_time": "2026-02-10T16:49:12.752310",
     "exception": false,
     "start_time": "2026-02-10T16:49:12.733124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules-enforcement/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d73ecdc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:49:12.756977Z",
     "iopub.status.busy": "2026-02-10T16:49:12.756488Z",
     "iopub.status.idle": "2026-02-10T16:49:12.761761Z",
     "shell.execute_reply": "2026-02-10T16:49:12.761171Z"
    },
    "papermill": {
     "duration": 0.009188,
     "end_time": "2026-02-10T16:49:12.763224",
     "exception": false,
     "start_time": "2026-02-10T16:49:12.754036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt(input: pd.Series):\n",
    "    return \"\"\"<start_of_turn>user\n",
    "You are a really experienced moderator for the subreddit /r/%s. Your job\n",
    "is to determine if the following reported comments violates the rule:\n",
    "%s\n",
    "\n",
    "%s\n",
    "Decision:\n",
    "True\n",
    "\n",
    "%s\n",
    "Decision:\n",
    "False\n",
    "\n",
    "%s\n",
    "Decision:\n",
    "False\n",
    "\n",
    "%s\n",
    "Decision:\n",
    "True\n",
    "\n",
    "%s\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\\n\"\"\" % (\n",
    "    input['subreddit'],\n",
    "    input['rule'],\n",
    "    \"\\n\".join([\"| \" + x for x in input['positive_example_1'].split('\\n')]),\n",
    "    \"\\n\".join([\"| \" + x for x in input['negative_example_1'].split('\\n')]),\n",
    "    \"\\n\".join([\"| \" + x for x in input['negative_example_2'].split('\\n')]),\n",
    "    \"\\n\".join([\"| \" + x for x in input['positive_example_2'].split('\\n')]),\n",
    "    \"\\n\".join([\"| \" + x for x in input['body'].split('\\n')])    \n",
    ")\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5321365b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:49:12.767834Z",
     "iopub.status.busy": "2026-02-10T16:49:12.767339Z",
     "iopub.status.idle": "2026-02-10T16:49:13.520324Z",
     "shell.execute_reply": "2026-02-10T16:49:13.519682Z"
    },
    "papermill": {
     "duration": 0.757074,
     "end_time": "2026-02-10T16:49:13.521977",
     "exception": false,
     "start_time": "2026-02-10T16:49:12.764903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_ids = [processor.get_vocab()[word] for word in ['True', 'False']]\n",
    "if any(token_id == processor.get_vocab()['<unk>'] for token_id in token_ids):\n",
    "      raise ValueError('One of the target classes is not in the vocabulary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83379bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:49:13.526549Z",
     "iopub.status.busy": "2026-02-10T16:49:13.526288Z",
     "iopub.status.idle": "2026-02-10T16:49:16.111499Z",
     "shell.execute_reply": "2026-02-10T16:49:16.110893Z"
    },
    "papermill": {
     "duration": 2.589367,
     "end_time": "2026-02-10T16:49:16.113172",
     "exception": false,
     "start_time": "2026-02-10T16:49:13.523805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "for batch in more_itertools.batched(test_data.iterrows(), 4):\n",
    "    prompts = [prompt(x) for _, x in batch]\n",
    "    pre = processor(text=prompts, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                    max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**pre)\n",
    "    logits = outputs.logits[:, -1, token_ids]  \n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    responses.extend(probabilities[:, 0].tolist())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd9c0aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:49:16.118134Z",
     "iopub.status.busy": "2026-02-10T16:49:16.117637Z",
     "iopub.status.idle": "2026-02-10T16:49:16.121595Z",
     "shell.execute_reply": "2026-02-10T16:49:16.121044Z"
    },
    "papermill": {
     "duration": 0.007837,
     "end_time": "2026-02-10T16:49:16.122880",
     "exception": false,
     "start_time": "2026-02-10T16:49:16.115043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({\n",
    "    'row_id': test_data['row_id'],\n",
    "    'rule_violation': responses\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af30fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:49:16.127144Z",
     "iopub.status.busy": "2026-02-10T16:49:16.126713Z",
     "iopub.status.idle": "2026-02-10T16:49:16.134572Z",
     "shell.execute_reply": "2026-02-10T16:49:16.134072Z"
    },
    "papermill": {
     "duration": 0.011407,
     "end_time": "2026-02-10T16:49:16.135855",
     "exception": false,
     "start_time": "2026-02-10T16:49:16.124448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "modelId": 222398,
     "modelInstanceId": 239467,
     "sourceId": 282742,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 54.964954,
   "end_time": "2026-02-10T16:49:19.224048",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-10T16:48:24.259094",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
