{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1db956",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-03T17:02:09.577743Z",
     "iopub.status.busy": "2025-11-03T17:02:09.577312Z",
     "iopub.status.idle": "2025-11-03T17:23:45.914759Z",
     "shell.execute_reply": "2025-11-03T17:23:45.913561Z"
    },
    "papermill": {
     "duration": 1296.342172,
     "end_time": "2025-11-03T17:23:45.916618",
     "exception": false,
     "start_time": "2025-11-03T17:02:09.574446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing in chunks of 10000000\n",
      "Total unique predictions: 34775536\n",
      "Processed chunk 1\n",
      "Processed chunk 2\n",
      "Processed chunk 3\n",
      "Processed chunk 4\n",
      "Saved median ensemble to submission.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def median_ensemble(file_paths, output_path='submission.tsv', chunksize=100000):\n",
    "    print(f\"Processing in chunks of {chunksize}\")\n",
    "\n",
    "    # Step 1: Collect all unique keys (protein_go_term pairs)\n",
    "    all_keys = set()\n",
    "    for path in file_paths:\n",
    "        for chunk in pd.read_csv(path, sep='\\t', header=None,\n",
    "                                 names=['protein', 'go_term', 'score'],\n",
    "                                 dtype={'protein': str, 'go_term': str, 'score': float},\n",
    "                                 chunksize=chunksize):\n",
    "            chunk = chunk.dropna(subset=['protein', 'go_term'])\n",
    "            chunk['key'] = chunk['protein'] + '_' + chunk['go_term']\n",
    "            all_keys.update(chunk['key'].values)\n",
    "\n",
    "    all_keys = sorted(all_keys)\n",
    "    print(f\"Total unique predictions: {len(all_keys)}\")\n",
    "\n",
    "    temp_files = []\n",
    "\n",
    "    # Step 2: Process by chunks of keys\n",
    "    for start_idx in range(0, len(all_keys), chunksize):\n",
    "        end_idx = min(start_idx + chunksize, len(all_keys))\n",
    "        key_chunk = all_keys[start_idx:end_idx]\n",
    "        result = pd.DataFrame({'key': key_chunk})\n",
    "\n",
    "        # Step 3: Load model scores for current key chunk\n",
    "        for i, path in enumerate(file_paths):\n",
    "            model_data = []\n",
    "            for chunk in pd.read_csv(path, sep='\\t', header=None,\n",
    "                                     names=['protein', 'go_term', 'score'],\n",
    "                                     dtype={'protein': str, 'go_term': str, 'score': float},\n",
    "                                     chunksize=chunksize):\n",
    "                chunk['key'] = chunk['protein'] + '_' + chunk['go_term']\n",
    "                chunk_filtered = chunk[chunk['key'].isin(key_chunk)][['key', 'score']]\n",
    "                model_data.append(chunk_filtered)\n",
    "\n",
    "            if model_data:\n",
    "                model_df = pd.concat(model_data, ignore_index=True)\n",
    "                model_df = model_df.rename(columns={'score': f'score_{i}'})\n",
    "                result = result.merge(model_df, on='key', how='left')\n",
    "\n",
    "        # Step 4: Replace NaNs with 0\n",
    "        for i in range(len(file_paths)):\n",
    "            result[f'score_{i}'] = result[f'score_{i}'].fillna(0)\n",
    "\n",
    "        # Step 5: Compute median across models\n",
    "        model_cols = [f'score_{i}' for i in range(len(file_paths))]\n",
    "        result['final_score'] = result[model_cols].median(axis=1)\n",
    "\n",
    "        # Step 6: Split key back into protein and go_term\n",
    "        result['protein'] = result['key'].str.rsplit('_', n=1).str[0]\n",
    "        result['go_term'] = result['key'].str.rsplit('_', n=1).str[-1]\n",
    "\n",
    "        temp_file = f'temp_chunk_{start_idx}.csv'\n",
    "        result[['protein', 'go_term', 'final_score']].to_csv(temp_file, index=False, sep='\\t', header=False)\n",
    "        temp_files.append(temp_file)\n",
    "        print(f\"Processed chunk {len(temp_files)}\")\n",
    "\n",
    "    # Step 7: Combine all chunk files\n",
    "    all_data = [pd.read_csv(f, sep='\\t', header=None, names=['protein', 'go_term', 'final_score'])\n",
    "                for f in temp_files]\n",
    "    final_result = pd.concat(all_data, ignore_index=True)\n",
    "    final_result.to_csv(output_path, sep='\\t', index=False, header=False)\n",
    "\n",
    "    # Step 8: Clean up temp files\n",
    "    for temp_file in temp_files:\n",
    "        os.remove(temp_file)\n",
    "\n",
    "    print(f\"Saved median ensemble to {output_path}\")\n",
    "    return final_result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_paths = [\n",
    "        '/kaggle/input/cafa-6-t5-embeddings-with-ensemble/submission.tsv',\n",
    "        '/kaggle/input/cafa-6-predictions/submission.tsv'\n",
    "    ]\n",
    "\n",
    "    result = median_ensemble(file_paths, chunksize=10_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5f512",
   "metadata": {
    "papermill": {
     "duration": 0.001188,
     "end_time": "2025-11-03T17:23:45.923919",
     "exception": false,
     "start_time": "2025-11-03T17:23:45.922731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "sourceId": 268508255,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 269339911,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 270571028,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 271888783,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1301.97325,
   "end_time": "2025-11-03T17:23:47.450105",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-03T17:02:05.476855",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
