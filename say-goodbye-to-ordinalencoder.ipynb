{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"},{"sourceId":9335009,"sourceType":"datasetVersion","datasetId":5656615}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is based on [the notebook by @INVISIBLE](https://www.kaggle.com/code/vyacheslavbolotin/ps-s5e4-hyperspace-as-feats).**\n\n**Replace OrdinalEncoder/LabelEncoder with more powerful encoding.**\n\n**OrdinalEncoder uses lexicographical order by default, so we can replace it with more powerful encoding.[Discussion here](https://www.kaggle.com/competitions/playground-series-s5e4/discussion/573253#3178652).**\n\n**You can use \"compare version\" to see the changes made.**","metadata":{}},{"cell_type":"code","source":"pip install -qq scikit-learn==1.6.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:30:55.552142Z","iopub.execute_input":"2025-04-15T00:30:55.5523Z","iopub.status.idle":"2025-04-15T00:31:03.099754Z","shell.execute_reply.started":"2025-04-15T00:30:55.552284Z","shell.execute_reply":"2025-04-15T00:31:03.098837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, gc, psutil\nimport polars as pl, pandas as pd;  pd.set_option('display.max_columns', 100) \n\nimport numpy as np\n\nimport matplotlib.pyplot as plt;    plt.style.use(\"seaborn-v0_8\")\nimport seaborn as sns;              sns.set_palette(\"husl\")\n\nimport warnings;                    warnings.simplefilter('ignore')\n\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing   import TargetEncoder\nfrom sklearn.model_selection import KFold\n\nfrom tqdm import tqdm\n\n\ndef memory_usage():\n    \"\"\"\n    Returns the current memory usage of the process in MB.\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    mem = process.memory_info().rss / (1024 * 1024)  # Convert bytes to megabytes\n    return f\"Memory Usage: {mem:.2f} MB\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:35:07.869665Z","iopub.execute_input":"2025-04-15T00:35:07.869973Z","iopub.status.idle":"2025-04-15T00:35:08.493347Z","shell.execute_reply.started":"2025-04-15T00:35:07.869952Z","shell.execute_reply":"2025-04-15T00:35:08.49262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_df       = pd.read_csv(\"/kaggle/input/podcast-listening-time-prediction-dataset/podcast_dataset.csv\")\ntrain_df          = pd.read_csv('/kaggle/input/playground-series-s5e4/train.csv', index_col='id')\ntest_df           = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv',  index_col='id')\nsample_submission = pd.read_csv('/kaggle/input/playground-series-s5e4/sample_submission.csv')\n\n\n# Concatenate original data with synthetics ones\ntrain_df = pd.concat([train_df, original_df], axis=0, ignore_index=True); train_df.drop_duplicates()\n\n\nprint(\"\\nData Info:\");                  display(train_df.info())\nprint(\"\\nNumerical Features Summary:\"); display(train_df.describe())\nprint(\"\\nFirst 3 rows of Dataset:\");    display(train_df.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:35:52.375409Z","iopub.execute_input":"2025-04-15T00:35:52.375654Z","iopub.status.idle":"2025-04-15T00:35:54.131343Z","shell.execute_reply.started":"2025-04-15T00:35:52.375637Z","shell.execute_reply":"2025-04-15T00:35:54.130252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visual analitic\n\ndef numerical_distrib_analysis(data, numerical_features):\n    \"\"\"\n    Analyzes the distribution of numerical features using histograms and boxplots.\n\n    :param data: Pandas DataFrame containing the dataset\n    :param numerical_features: List of numerical column names\n    \"\"\"\n    for feature in numerical_features:\n        plt.figure(figsize=(5, 2))\n\n        # Histogram with KDE curve\n        plt.subplot(1, 2, 1)\n        sns.histplot(data[feature], kde=True, bins=30)\n        plt.title(f\"Histogram of {feature}\")\n        plt.xlabel(feature)\n        plt.ylabel(\"Frequency\")\n\n        # Boxplot to detect outliers\n        plt.subplot(1, 2, 2)\n        sns.boxplot(x=data[feature])\n        plt.title(f\"Boxplot of {feature}\")\n\n        plt.tight_layout()\n        plt.show()\n\n        # Additional statistics\n        print(f\"\\nStatistics for {feature}:\")\n        print(f\"Skewness: {data[feature].skew():.2f}\")\n        print(f\"Missing Values: {data[feature].isnull().sum()}\")\n\n\ndef categorical_distrib_analysis(data, categorical_features, top_n=10):\n    \"\"\"\n    Analyzes and visualizes the distribution of categorical features.\n\n    :param data: Pandas DataFrame containing the dataset\n    :param categorical_features: List of categorical column names\n    :param top_n: Max number of top categories to display for high-cardinality features\n    \"\"\"\n    for feature in categorical_features:\n        plt.figure(figsize=(4, 3))\n\n        unique_count = data[feature].nunique()\n\n        if unique_count > top_n:\n            # Show only the top_n most frequent categories\n            top_categories = data[feature].value_counts().nlargest(top_n)\n            sns.barplot(x=top_categories.index, y=top_categories.values, palette=\"pastel\")\n            plt.title(f\"Top {top_n} Categories of {feature}\")\n        else:\n            # Show all categories\n            sns.countplot(x=data[feature], order=data[feature].value_counts().index, palette=\"pastel\")\n            plt.title(f\"Distribution of {feature}\")\n\n        plt.xlabel(feature)\n        plt.ylabel(\"Count\")\n        plt.xticks(rotation=45)\n        plt.show()\n\n        # Print stats\n        print(f\"Feature: {feature}\")\n        print(f\"Number of Unique Values: {unique_count}\")\n        print(f\"Missing Values: {data[feature].isnull().sum()}\\n\")\n\n\ndef numerical_correlation_analysis(data, numerical_features, target):\n    \"\"\"\n    Analyzes and visualizes relationships between numerical features and the target.\n\n    :param data: Pandas DataFrame containing the dataset\n    :param numerical_features: List of numerical column names\n    :param target: Name of the target variable\n    \"\"\"\n    for feature in numerical_features:\n        if feature != target:\n            # Scatter plot: feature vs target\n            plt.figure(figsize=(4, 3))\n            sns.scatterplot(x=data[feature], y=data[target], alpha=0.5)\n            plt.title(f\"{feature} vs {target}\")\n            plt.xlabel(feature)\n            plt.ylabel(target)\n            plt.show()\n\n    # Correlation matrix\n    correlation_matrix = data[numerical_features].corr()\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n    plt.title(\"Correlation Matrix of Numerical Features\")\n    plt.show()\n\n\ndef categorical_correlation_analysis(data, categorical_features, target, high_cardinality_threshold=10):\n    \"\"\"\n    Visualizes categorical features against the target using boxplots.\n\n    :param data: Pandas DataFrame containing the dataset\n    :param categorical_features: List of categorical column names\n    :param target: Name of the target variable\n    :param high_cardinality_threshold: Max number of unique values allowed for plotting\n    \"\"\"\n    for feature in categorical_features:\n        if data[feature].nunique() <= high_cardinality_threshold:\n            # Boxplot: target distribution per category\n            plt.figure(figsize=(5, 3))\n            sns.boxplot(x=data[feature], y=data[target], palette='husl')\n            plt.title(f\"{feature} vs {target}\")\n            plt.xlabel(feature)\n            plt.ylabel(target)\n            plt.xticks(rotation=45)\n            plt.show()\n        else:\n            print(f\"Skipping {feature}: too many unique values ({data[feature].nunique()})\\n\")\n\n\n# Visual analitic in action -----------------------------------------------------\n\n\nnumerical_features = [\n    'Episode_Length_minutes', \n    'Host_Popularity_percentage',\n    'Guest_Popularity_percentage', \n    'Number_of_Ads',\n    'Listening_Time_minutes',\n]\ncategorical_features = [\n    'Podcast_Name', \n    'Genre', \n    'Publication_Day',\n    'Publication_Time', \n    'Episode_Sentiment'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:36:27.35056Z","iopub.execute_input":"2025-04-15T00:36:27.350882Z","iopub.status.idle":"2025-04-15T00:36:51.475964Z","shell.execute_reply.started":"2025-04-15T00:36:27.350817Z","shell.execute_reply":"2025-04-15T00:36:51.4751Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pfe(df):\n    \n    _dict_podc = {\n        'Mystery Matters': 1.01, 'News Roundup':    1.08, 'Global News':        1.15,\n        'Joke Junction'  : 1.02, 'Daily Digest':    1.09, 'Tech Talks':         1.16,\n        'Study Sessions' : 1.03, 'Music Matters':   1.10, 'Sport Spot':         1.17,\n        'Digital Digest' : 1.04, 'Sports Central':  1.11, 'Funny Folks':        1.18,\n        'Mind & Body'    : 1.05, 'Melody Mix':      1.12, 'Sports Weekly':      1.19,\n        'Fitness First'  : 1.06, 'Game Day':        1.13, 'Business Briefs':    1.20,\n        'Criminal Minds' : 1.07, 'Gadget Geek':     1.14, 'Tech Trends':        1.21, \n        \n        'Innovators'     : 2.01, 'Health Hour':     2.08, 'Detective Diaries':  2.15,\n        'Sound Waves'    : 2.02, 'Brain Boost':     2.09, \"Athlete's Arena\":    2.16, \n        'Wellness Wave'  : 2.03, 'Style Guide':     2.10, 'World Watch':        2.17, \n        'Humor Hub'      : 2.04, 'Money Matters':   2.11, 'Healthy Living':     2.18, \n        'Home & Living'  : 2.05, 'Market Masters':  2.12, 'Finance Focus':      2.19,\n        'Learning Lab'   : 2.06, 'Lifestyle Lounge':2.13, 'Crime Chronicles':   2.20,\n        'Comedy Corner'  : 2.07, 'Life Lessons':    2.14, 'Educational Nuggets':2.21,   \n        \n        'Current Affairs': 3.01, 'Laugh Line':      3.02, 'True Crime Stories': 3.03, \n        'Fashion Forward': 3.04, 'Tune Time':       3.05, 'Business Insights':  3.06, \"TheEnd\":3.07\n    }\n    _dict_genr = {\n        'True Crime':      4.01, 'Comedy':          4.02, 'Education':          4.03, \n        'Technology':      4.04, 'Health':          4.05, 'News':               4.06,\n        'Music':           4.07, 'Sports':          4.08, 'Business':           4.09, 'Lifestyle': 4.10\n    }\n    _dict_week = {\n        'Monday':          5.01, 'Tuesday':         5.02, 'Wednesday':          5.03, \n        'Thursday':        5.04, 'Friday':          5.05, 'Saturday':           5.06, 'Sunday': 5.07\n    }    \n    _dict_time = {\n        'Morning':         7.01, 'Afternoon':       7.02, 'Evening':            7.03, 'Night': 3\n    }\n    _dict_sent = {\n        'Negative':        8.01, 'Neutral':         8.02, 'Positive':           8.03\n    }\n\n    def _2_NaN(x):\n        if x['x1']<0 and x['x2']<0: return -21\n        if x['x1']<0              : return -3\n        if               x['x2']<0: return -7\n        return +7\n\n        \n    df['x1'] = df.Episode_Length_minutes;      df['x1'].fillna(\"NaN\", inplace=True)     \n    df['x2'] = df.Guest_Popularity_percentage; df['x2'].fillna(\"NaN\", inplace=True)\n    df['x3'] = df.Episode_Title       .apply(lambda x: int(x[8:]) / 100 + 10)\n    df['x1'] = df['x1']               .apply(lambda x: -3 if x==\"NaN\" else x)\n    df['x2'] = df['x2']               .apply(lambda x: -7 if x==\"NaN\" else x)\n    df[\"x4\"] = df                     .apply(lambda x: _2_NaN(x), axis=1)\n    df['x5'] = df['Genre']            .replace(_dict_genr)\n    df['x6'] = df['Podcast_Name']     .replace(_dict_podc)\n    df['x7'] = df['Publication_Day']  .replace(_dict_week)\n    df['x8'] = df['Publication_Time'] .replace(_dict_time)\n    df['x9'] = df['Episode_Sentiment'].replace(_dict_sent)\n   \n    p1 = df['x1'].mean()\n    p2 = df['x2'].mean()\n    p3 = df['x3'].mean()\n    p4 = df['x4'].mean()\n    p5 = df['x5'].mean()\n    p6 = df['x6'].mean()\n    p7 = df['x7'].mean()\n    p8 = df['x8'].mean()\n    p9 = df['x9'].mean()\n\n    df['x10'] = np.sqrt(\n        (p1-df.x1)**2+\n        (p2-df.x2)**2+\n        (p3-df.x3)**2+\n        (p4-df.x4)**2+\n        (p5-df.x5)**2+\n        (p6-df.x6)**2+\n        (p7-df.x7)**2+\n        (p8-df.x8)**2+\n        (p9-df.x9)**2)\n\n    polar_df = pl.from_pandas(df)\n    \n    polar_df = polar_df.with_columns(     \n        _2_1 = ((pl.col('x1')-pl.col('x3'))**2+(pl.col('x2')-pl.col('x4'))**2).sqrt(),\n        _2_2 = ((pl.col('x1')-pl.col('x5'))**2+(pl.col('x2')-pl.col('x6'))**2).sqrt(),\n        _2_3 = ((pl.col('x1')-pl.col('x7'))**2+(pl.col('x2')-pl.col('x8'))**2).sqrt(),\n        _2_4 = ((pl.col('x3')-pl.col('x5'))**2+(pl.col('x4')-pl.col('x6'))**2).sqrt(),\n        _2_5 = ((pl.col('x3')-pl.col('x7'))**2+(pl.col('x4')-pl.col('x8'))**2).sqrt(),\n        _2_6 = ((pl.col('x5')-pl.col('x7'))**2+(pl.col('x6')-pl.col('x8'))**2).sqrt(),\n        _3_1 = ((pl.col('x1')-pl.col('x4'))**2+(pl.col('x2')-pl.col('x5'))**2+(pl.col('x3')-pl.col('x6'))**2).sqrt(),\n        _3_2 = ((pl.col('x1')-pl.col('x7'))**2+(pl.col('x2')-pl.col('x8'))**2+(pl.col('x3')-pl.col('x9'))**2).sqrt(),\n        _3_3 = ((pl.col('x4')-pl.col('x7'))**2+(pl.col('x5')-pl.col('x8'))**2+(pl.col('x6')-pl.col('x9'))**2).sqrt(),\n        _4_1 = ((pl.col('x1')-pl.col('x5'))**2+(pl.col('x2')-pl.col('x6'))**2+(pl.col('x3')-pl.col('x7'))**2+(pl.col('x4')-pl.col('x8'))**2).sqrt(),)\n                \n    df = polar_df.to_pandas()\n\n    p1 = df['_2_1'].mean()\n    p2 = df['_2_2'].mean()\n    p3 = df['_2_3'].mean()\n    p4 = df['_2_4'].mean()\n    p5 = df['_2_5'].mean()\n    p6 = df['_2_6'].mean()\n    p7 = df['_3_1'].mean()\n    p8 = df['_3_2'].mean()\n    p9 = df['_3_3'].mean()\n    p0 = df['_4_1'].mean()\n\n    df['_x_11'] = np.sqrt(\n        (p1-df._2_1)**2+\n        (p2-df._2_2)**2+\n        (p3-df._2_3)**2+\n        (p4-df._2_4)**2+\n        (p5-df._2_5)**2+\n        (p6-df._2_6)**2+\n        (p7-df._3_1)**2+\n        (p8-df._3_2)**2+\n        (p9-df._3_3)**2+\n        (p0-df._4_1)**2)\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:38:07.718737Z","iopub.execute_input":"2025-04-15T00:38:07.719047Z","iopub.status.idle":"2025-04-15T00:38:07.735802Z","shell.execute_reply.started":"2025-04-15T00:38:07.719028Z","shell.execute_reply":"2025-04-15T00:38:07.735076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Preparing managing null values..\")\n\ntrain_df = pfe(train_df)\ntest_df  = pfe(test_df)\n\nprint(\"Missing Values per Column:\"); print(train_df.isnull().sum())\nprint(\"Missing Values per Column:\"); print(test_df .isnull().sum())\n\n# Replacing null values by median\ntrain_df['Episode_Length_minutes'].fillna(train_df['Episode_Length_minutes'].median(), inplace=True)\ntest_df ['Episode_Length_minutes'].fillna(train_df['Episode_Length_minutes'].median(), inplace=True)\n\n# Null values could mean no guest \ntrain_df['Guest_Popularity_percentage'].fillna(train_df['Guest_Popularity_percentage'].median(), inplace=True)\ntest_df ['Guest_Popularity_percentage'].fillna(test_df ['Guest_Popularity_percentage'].median(), inplace=True)\n\n\ntrain_df = train_df[train_df['Number_of_Ads']<10] # Deleting outliers \n\n\ntrain_df.dropna(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:38:29.906453Z","iopub.execute_input":"2025-04-15T00:38:29.906747Z","iopub.status.idle":"2025-04-15T00:38:41.15398Z","shell.execute_reply.started":"2025-04-15T00:38:29.906728Z","shell.execute_reply":"2025-04-15T00:38:41.153308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nclass CategoryMeanTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, cat_cols=None):\n        self.cat_cols = cat_cols\n        self.mappings_ = {}\n    def fit(self, X, y):\n        X = X.copy()\n        if self.cat_cols is None:\n            self.cat_cols = X.select_dtypes(include=['category']).columns.tolist()\n        self.mappings_ = {}\n        for col in self.cat_cols:\n            df_temp = pd.DataFrame({col: X[col], 'y': y})\n            group_means = df_temp.groupby(col, dropna=False)['y'].mean()\n            sorted_categories = group_means.sort_values().index\n            self.mappings_[col] = {cat: i for i, cat in enumerate(sorted_categories)}\n        return self\n\n    def transform(self, X, y=None):\n        X = X.copy()\n        for col, mapping in self.mappings_.items():\n            if col in X.columns:\n                X[col] = X[col].map(mapping)\n        return X\n\ncmtencoder = CategoryMeanTransformer(cat_cols=categorical_features)\nx_col = ['Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre',\n       'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time',\n       'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment',]\ny_col = ['Listening_Time_minutes']\ntrain_df[x_col] = cmtencoder.fit_transform(train_df[x_col],np.array(train_df[y_col]).reshape(-1,))\ntest_df[x_col] = cmtencoder.transform(test_df[x_col])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\n\n# creating most relevant feature \ntrain_df['Episode_Num'] = train_df['Episode_Title'].str[8:].astype('category')\ntest_df ['Episode_Num'] = test_df ['Episode_Title'].str[8:].astype('category')\n\ntrain_df = train_df.drop(columns=['Episode_Title'])\ntest_df  = test_df .drop(columns=['Episode_Title'])\n\ndisplay(train_df['Podcast_Name'].nunique(), train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:39:20.349553Z","iopub.execute_input":"2025-04-15T00:39:20.349882Z","iopub.status.idle":"2025-04-15T00:39:21.426131Z","shell.execute_reply.started":"2025-04-15T00:39:20.349857Z","shell.execute_reply":"2025-04-15T00:39:21.425615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom itertools import combinations\n\ncolumns_to_encode = [\n    'Episode_Length_minutes', \n    'Episode_Num', \n    'Host_Popularity_percentage', \n    'Number_of_Ads', \n    'Episode_Sentiment', \n    'Publication_Day', \n    'Publication_Time',\n    'Genre',\n    'Guest_Popularity_percentage'\n]\n\npair_size = [2, 3, 4]\n\n\nfor r in pair_size: \n    combinations_list = list(combinations(columns_to_encode,r))\n    batch_size = 20\n\n    print('\\n pair_size:', r, '\\n')\n    \n    for i in range(0, len(combinations_list), batch_size):\n        \n        batch = combinations_list[i : i +batch_size]\n        \n        for cols in tqdm(batch):\n            new_col_name = '_'.join(cols)\n\n            train_df[new_col_name] = train_df[list(cols)].astype(str).agg('_'.join, axis=1) \n            test_df [new_col_name] = test_df [list(cols)].astype(str).agg('_'.join, axis=1) \n            test_df [new_col_name] = test_df [new_col_name].astype('category')\n            train_df[new_col_name] = train_df[new_col_name].astype('category')\n            \n        gc.collect()\n        \n        print(f\"Memory usage: {train_df.memory_usage(deep=True).sum() / (1024*1024):.2f} MB\")\n        print(f\"Total number of columns: {len(train_df.columns)}\")\n\n    print(\"~\"*19)","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-04-15T01:06:56.223028Z","shell.execute_reply.started":"2025-04-15T00:40:38.637799Z","shell.execute_reply":"2025-04-15T01:06:56.221731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Models ","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(columns=['Listening_Time_minutes'])\ny = train_df['Listening_Time_minutes']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:08:04.653597Z","iopub.execute_input":"2025-04-15T01:08:04.653868Z","iopub.status.idle":"2025-04-15T01:08:05.256465Z","shell.execute_reply.started":"2025-04-15T01:08:04.653851Z","shell.execute_reply":"2025-04-15T01:08:05.255777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\n\nN_SPLITS =    7  \ncv = KFold(N_SPLITS, random_state=42, shuffle=True)\ny_pred = np.zeros(len(sample_submission))\n\nfor idx_train, idx_valid in cv.split(X, y):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    X_test = test_df[X.columns].copy()\n\n    encoded_columns = train_df.columns[30:] # original: train_df.columns[11:] + 19 new feats\n    encoder = TargetEncoder(random_state=42)\n\n    X_train[encoded_columns] = encoder.fit_transform(X_train[encoded_columns], y_train)  \n    X_valid[encoded_columns] = encoder.transform(X_valid[encoded_columns])\n    X_test [encoded_columns] = encoder.transform(X_test[encoded_columns])\n\n    model = lgb.LGBMRegressor(\n        n_iter           = 1000,\n        max_depth        = -1,\n        num_leaves       = 2048,\n        colsample_bytree = 0.7,\n        learning_rate    = 0.015,\n        objective        = 'l2',\n        metric           = 'rmse', \n        verbosity        = -1,\n        #max_bin          = 1024,\n        #device           = \"gpu\",\n    )\n\n\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_valid, y_valid)],\n        callbacks=[\n            lgb.log_evaluation(100),\n            lgb.early_stopping(stopping_rounds=100)\n            ],\n    )\n\n    y_pred += model.predict(X_test)\n\npred_lgbm = y_pred / N_SPLITS\n\n# Training until validation scores don't improve for 100 rounds\n# [100]\tvalid_0's rmse: 13.1727\n# [200]\tvalid_0's rmse: 11.9743\n# [300]\tvalid_0's rmse: 11.8755\n# [400]\tvalid_0's rmse: 11.8561\n# [500]\tvalid_0's rmse: 11.8528\n# [600]\tvalid_0's rmse: 11.8519\n# Early stopping, best iteration is:\n# [535]\tvalid_0's rmse: 11.8512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:08:39.187261Z","iopub.execute_input":"2025-04-15T01:08:39.187529Z","execution_failed":"2025-04-15T01:49:59.199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_lgbm = pd.DataFrame({'id': sample_submission.id, 'Listening_Time_minutes' : pred_lgbm})\nsubmission_lgbm.to_csv('submission.csv', index=False)\nsubmission_lgbm.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T12:29:45.074576Z","iopub.execute_input":"2025-04-13T12:29:45.075464Z","iopub.status.idle":"2025-04-13T12:29:45.625444Z","shell.execute_reply.started":"2025-04-13T12:29:45.075442Z","shell.execute_reply":"2025-04-13T12:29:45.624909Z"}},"outputs":[],"execution_count":null}]}