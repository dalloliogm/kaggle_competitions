{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91714,"databundleVersionId":11251744,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dalloliogm/predicting-using-a-logistic-regression-with-regula?scriptVersionId=228916956\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Rain prediction using Logistic Regression\n\nThis notebook predicts rainfall using a simple Logistic Regression model.","metadata":{}},{"cell_type":"markdown","source":"## Parameters and options","metadata":{}},{"cell_type":"code","source":"config = {\n    \"features\": ['day', \n                 'pressure', \n                 'maxtemp', \n                 'temparature', \n                 'mintemp', \n                 'dewpoint', \n                 'humidity', \n                 'cloud', \n                 'sunshine', \n                 'winddirection', \n                 'windspeed'\n                 ],\n    # Note: in this notebook https://www.kaggle.com/code/hopesb/rain-fall-prediction/notebook they removed:\n    # [\"mintemp\", \"temparature\", \"maxtemp\", \"winddirection\"\n    \"clustering_variables\": ['day', 'temparature', 'sunshine', 'cloud', 'windspeed'],\n    \"n_clusters\": 3,\n    \"n_lags\": 5,\n    \"lag_columns\": ['humidity', 'temparature', 'pressure', 'cloud', 'windspeed', 'dewpoint', 'sunshine'],\n    \"device\": \"cpu\"\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing libraries and reading files","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n\n\ndef plot_cv_scores(cv_scores, title):\n    \"\"\"Plot cross-validation ROC AUC scores.\"\"\"\n    plt.figure(figsize=(10, 6))\n    num_folds = len(cv_scores)\n    plt.plot(range(1, num_folds + 1), cv_scores, marker='o', linestyle='--', color='b')\n    plt.axhline(y=cv_scores.mean(), color='r', linestyle='-', label=f'Mean: {cv_scores.mean():.4f}')\n    plt.xlabel('Fold Number', fontsize=12)\n    plt.ylabel('ROC AUC', fontsize=12)\n    plt.title(title, fontsize=14)\n    plt.xticks(range(1, num_folds + 1, 2))\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.show()\n\ndef evaluate_pipeline(pipeline, X, y, pipeline_name, n_splits=5, n_repeats=10, random_state=666):\n    \"\"\"\n    Evaluate a pipeline using repeated stratified k-fold cross-validation and plot the results.\n    \n    Parameters:\n        pipeline: The sklearn Pipeline to evaluate.\n        X: Feature DataFrame.\n        y: Target array or Series.\n        pipeline_name: String, name of the pipeline (used for printing/plot titles).\n        n_splits: Number of folds (default 5).\n        n_repeats: Number of repeats (default 10).\n        random_state: Random seed for reproducibility.\n    \n    Returns:\n        cv_scores: Array of cross-validation scores.\n    \"\"\"\n    auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n    rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n    cv_scores = cross_val_score(pipeline, X, y, cv=rskf, scoring=auc_scorer)\n    mean_score = np.mean(cv_scores)\n    print(f\"{pipeline_name} Repeated CV AUC scores:\", cv_scores)\n    print(f\"Mean AUC {pipeline_name}: {mean_score:.4f}\")\n    plot_cv_scores(cv_scores, title=f\"{pipeline_name}: Cross-validation fold estimates\")\n    return cv_scores\n\n\ndef plot_feature_importance(pipeline, X):\n    \"\"\"\n    Plot feature importance for a pipeline.\n    \n    This function manually applies the feature engineering steps\n    (skipping imputation and scaling) to obtain the final feature names,\n    then extracts importance values from the classifier step.\n    \n    Parameters:\n        pipeline: The sklearn Pipeline containing at least these steps:\n                  'feature_eng', 'additional_fe', 'lag_features', 'clf'.\n        X: pandas DataFrame with the original features.\n    \n    Raises:\n        ValueError: If the classifier does not have a recognized attribute for feature importance.\n    \"\"\"\n    # Check that required steps exist in the pipeline\n    required_steps = ['feature_eng', 'additional_fe', 'lag_features', 'clf']\n    missing_steps = [step for step in required_steps if step not in pipeline.named_steps]\n    if missing_steps:\n        raise ValueError(f\"Pipeline is missing required steps: {missing_steps}\")\n    \n    # Apply the feature engineering steps manually (skipping imputer and scaler)\n    X_transformed = pipeline.named_steps['feature_eng'].transform(X)\n    X_transformed = pipeline.named_steps['additional_fe'].transform(X_transformed)\n    X_transformed = pipeline.named_steps['lag_features'].transform(X_transformed)\n    feature_names = X_transformed.columns\n\n    # Get the classifier from the pipeline\n    clf = pipeline.named_steps['clf']\n\n    # Extract feature importance\n    if hasattr(clf, \"coef_\"):\n        # For linear models like LogisticRegression\n        coefficients = clf.coef_[0]\n        feat_importance = pd.DataFrame({\n            'feature': feature_names,\n            'importance': coefficients\n        })\n        feat_importance['abs_importance'] = feat_importance['importance'].abs()\n        feat_importance = feat_importance.sort_values('abs_importance', ascending=True)\n        title = \"Feature Importance from Logistic Regression (Coefficients)\"\n    elif hasattr(clf, \"feature_importances_\"):\n        # For tree-based models like XGBoost or LightGBM\n        importances = clf.feature_importances_\n        feat_importance = pd.DataFrame({\n            'feature': feature_names,\n            'importance': importances\n        })\n        feat_importance = feat_importance.sort_values('importance', ascending=True)\n        title = \"Feature Importance from Model\"\n    else:\n        raise ValueError(\"Classifier does not have a known feature importance attribute.\")\n    \n    # Plot feature importance as a horizontal bar chart\n    plt.figure(figsize=(12, 12))\n    plt.barh(feat_importance['feature'], feat_importance['importance'])\n    plt.xlabel(\"Importance\")\n    plt.title(title)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!head /kaggle/input/playground-series-s5e3/sample_submission.csv\n!head /kaggle/input/playground-series-s5e3/train.csv\n!head /kaggle/input/playground-series-s5e3/test.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s5e3/train.csv\")#.set_index(\"id\")\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.describe().style.background_gradient(cmap='summer')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.day.max()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.day.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/playground-series-s5e3/test.csv\")#.set_index(\"id\")\ntest.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Quick EDA","metadata":{}},{"cell_type":"code","source":"train.corr().style.background_gradient(cmap='winter')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n#sns.pairplot(train, kind=\"kde\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling Missing values\n\nIt seems there is just one missing value, in the test dataset","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\ntest[test.isnull().any(axis=1)]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Adding season (spring, summer, fall, winter) and cyclical features\n\nThe day variables goes from 1 to 365, so we only have data from one year.\n\nTo tell our model that day 1 is close to day 365 (as they are both in winter) we add a cyclical feature","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass SeasonMonthTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        # Nothing to fit for this transformer\n        return self\n    \n    def transform(self, X):\n        # Make a copy to avoid modifying the original\n        X_trans = X.copy()\n        \n        # Convert day -> month\n        def day_to_month(day):\n            if day%365 <= 31: return 1\n            elif day%365 <= 59: return 2\n            elif day%365 <= 90: return 3\n            elif day%365 <= 120: return 4\n            elif day%365 <= 151: return 5\n            elif day%365 <= 181: return 6\n            elif day%365 <= 212: return 7\n            elif day%365 <= 243: return 8\n            elif day%365 <= 273: return 9\n            elif day%365 <= 304: return 10\n            elif day%365 <= 334: return 11\n            else: return 12\n        \n        X_trans['month'] = X_trans['day'].apply(day_to_month)\n        \n        # Convert day -> season\n        def day_to_season(day):\n            if 80 <= day%365 < 172:\n                return 'spring'\n            elif 172 <= day%365 < 264:\n                return 'summer'\n            elif 264 <= day%365 < 356:\n                return 'autumn'\n            else:\n                return 'winter'\n        \n        X_trans['season'] = X_trans['day'].apply(day_to_season)\n\n        \n        X_trans['day_sin'] = np.sin(2 * np.pi * X_trans['day'] / 365)\n        X_trans['day_cos'] = np.cos(2 * np.pi * X_trans['day'] / 365)\n        \n        # One-hot encode season\n        X_trans = pd.get_dummies(X_trans, columns=['season'], drop_first=True)\n        \n        return X_trans\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Additional features\n\nInspired by https://www.kaggle.com/code/josephnehrenz/87-9-logistic-s5e3-rainfall-probability-in-r","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nimport pandas as pd\n\nclass AdditionalFeatureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_trans = X.copy()\n        \n        # ------------------- Sunshine Metrics -------------------\n        if all(col in X.columns for col in ['sunshine', 'cloud', 'humidity']):\n            X_trans['relative_sunshine'] = X_trans['sunshine'] / (100 - X_trans['cloud'] + 1)\n            X_trans['sunshine_ratio'] = X_trans['sunshine'] / (X_trans['cloud'] + X_trans['humidity'] + 1e-5)\n        if all(col in X.columns for col in ['sunshine', 'cloud']):\n            X_trans['cloud_sun_ratio'] = X_trans['cloud'] / (X_trans['sunshine'] + 1)\n        if 'sunshine' in X.columns:\n            X_trans['sunshine_pct'] = X_trans['sunshine'] / 24.0  # Assuming max sunshine is 24 hours\n        \n        # ------------------- Cloud Metrics -------------------\n        if 'cloud' in X.columns:\n            X_trans['cloud_gradient'] = X_trans['cloud'] - X_trans['cloud'].shift(1, \n                                        fill_value=X_trans['cloud'].iloc[0])\n            X_trans['cloud_category'] = pd.cut(X_trans['cloud'], bins=[0,20,50,80,100],\n                                               labels=[0,1,2,3], include_lowest=True).astype(float)\n            X_trans['sky_opacity'] = X_trans['cloud'] / 100.0\n        \n        # ------------------- Temperature Metrics -------------------\n        if all(col in X.columns for col in ['maxtemp', 'mintemp']):\n            X_trans['temp_range'] = X_trans['maxtemp'] - X_trans['mintemp']\n        if 'temparature' in X.columns:\n            X_trans['temp_change'] = X_trans['temparature'] - X_trans['temparature'].shift(1, \n                                           fill_value=X_trans['temparature'].iloc[0])\n            X_trans['temp_ewm'] = X_trans['temparature'].ewm(span=10, adjust=False).mean()\n            if 'humidity' in X.columns:\n                X_trans['temp_humidity_interaction'] = X_trans['temparature'] + 0.2 * X_trans['humidity']\n        \n        # ------------------- Pressure Metrics -------------------\n        if 'pressure' in X.columns:\n            X_trans['pressure_rolling_mean'] = X_trans['pressure'].rolling(window=7, min_periods=1).mean()\n            X_trans['pressure_rolling_std'] = X_trans['pressure'].rolling(window=7, min_periods=1).std()\n            X_trans['pressure_diff'] = X_trans['pressure'] - X_trans['pressure'].shift(1, \n                                               fill_value=X_trans['pressure'].iloc[0])\n        \n        # ------------------- Humidity Metrics -------------------\n        if all(col in X.columns for col in ['temparature', 'dewpoint']):\n            X_trans['dewpoint_depression'] = X_trans['temparature'] - X_trans['dewpoint']\n            X_trans['rh_approx'] = 100 - (5 * X_trans['dewpoint_depression'])\n        if all(col in X.columns for col in ['humidity', 'cloud']):\n            X_trans['humidity_cloud_interaction'] = (X_trans['humidity'] * X_trans['cloud']) / 10000.0\n            X_trans['inv_humidity_cloud'] = 100 - X_trans['humidity'] - X_trans['cloud']\n        \n        # ------------------- Dewpoint Metrics -------------------\n        if 'temparature' in X.columns:\n            X_trans['svp'] = 6.1078 * np.exp((17.27 * X_trans['temparature']) / (X_trans['temparature'] + 237.3))\n        if all(col in X.columns for col in ['temparature', 'humidity']):\n            X_trans['abs_humidity'] = (6.112 * np.exp((17.67 * X_trans['temparature']) / (X_trans['temparature'] + 243.5)) * \n                                       X_trans['humidity'] * 2.1674) / (273.15 + X_trans['temparature'])\n        \n        return X_trans\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lagging features","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\n\nclass LagFeatureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None, n_lags=5):\n        # Default columns if none provided\n        self.columns = columns if columns is not None else ['humidity', 'temparature', 'pressure', 'sunshine']\n        self.n_lags = n_lags\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_trans = X.copy()\n        for col in self.columns:\n            if col in X_trans.columns:\n                for lag in range(1, self.n_lags + 1):\n                    X_trans[f\"{col}_lag_{lag}\"] = X_trans[col].shift(lag)\n            #else:\n                # Log a warning if the column is missing\n                #print(f\"Warning: Column '{col}' not found in data. Skipping lag features for this column.\")\n        return X_trans\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predicting Clusters\n\nThanks to this notebook: https://www.kaggle.com/code/felixleung/looks-like-there-are-3-clusters\n","metadata":{}},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport pandas as pd\n\n# Suppose you use the 'day' column (or any set of features) for clustering\nkmeans = KMeans(n_clusters=config[\"n_clusters\"], random_state=42)\n\n# Fit on training data\ntrain['cluster'] = kmeans.fit_predict(train[config[\"clustering_variables\"]])\n# Predict clusters for test data using the same model\ntest['cluster'] = kmeans.predict(test[config[\"clustering_variables\"]])\n\n# One-hot encode the cluster labels (optional but recommended)\ntrain = pd.get_dummies(train, columns=['cluster'], prefix='cluster')\ntest = pd.get_dummies(test, columns=['cluster'], prefix='cluster')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Prediction using Logistic Regression, XGBoost or LightGBM","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.metrics import roc_auc_score, make_scorer\nimport numpy as np\nfrom lightgbm import LGBMClassifier\n\n\n# Define a helper function to create the pipeline\ndef create_pipeline(lag_columns = config[\"lag_columns\"], \n                    n_lags=config[\"n_lags\"], \n                    clf=False):\n    steps = [\n        ('feature_eng', SeasonMonthTransformer()),\n        ('additional_fe', AdditionalFeatureTransformer()),\n        ('lag_features', LagFeatureTransformer(columns=lag_columns, n_lags=n_lags)),\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler()),\n        ('clf', clf)\n    ]\n    return Pipeline(steps)\n\n# Define your feature columns and data\nfeatures = [*config[\"features\"], *[c for c in train.columns if c.startswith('cluster')]]\n\nX = train[features]\ny = train['rainfall']\n\n# Create the logistic regression pipeline\npipeline_lg = create_pipeline(\n    n_lags = config[\"n_lags\"],\n    lag_columns = config[\"lag_columns\"],\n    clf=LogisticRegression(\n        penalty='l1',        # L1 (Lasso) regularization\n        C=1.0,               # adjust for stronger/weaker regularization\n        max_iter=1000,\n        random_state=42,\n        solver='liblinear'   # supports L1 penalty\n    )\n)\n\n# Create the XGBoost pipeline \npipeline_xgb = create_pipeline(\n    n_lags = config[\"n_lags\"],\n    lag_columns = config[\"lag_columns\"],\n    clf=XGBClassifier(\n        device=config[\"device\"],\n        n_estimators=10000,\n        learning_rate=0.1,\n        max_depth=6,\n#        early_stopping_rounds=100,\n        alpha=0.1,\n        random_state=42,\n        colsample_bytree=0.9, \n        subsample=0.9,\n        use_label_encoder=False,    # Disable label encoder to avoid warnings\n        eval_metric='auc'           # Set evaluation metric to AUC\n    )\n)\n\n# Create the XGBoost pipeline (with a different lag configuration)\npipeline_lgbm = create_pipeline(\n    n_lags = config[\"n_lags\"],\n    lag_columns = config[\"lag_columns\"],\n    clf=LGBMClassifier(\n        device=config[\"device\"],\n        n_estimators=10000,\n        learning_rate=0.1,\n        max_depth=6,\n        random_state=42,\n        verbose=-1  # This will suppress the warnings\n    )\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv_scores_lgbm = evaluate_pipeline(pipeline_lgbm, X, y, \"LightGBM\")\nmean_cv_scores_lgbm = np.mean(cv_scores_lgbm)\npipeline_lgbm.fit(X, y)\nplot_feature_importance(pipeline_lgbm, X)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv_scores_lg   = evaluate_pipeline(pipeline_lg, X, y, \"Logistic Regression\")\nmean_cv_scores_lg = np.mean(cv_scores_lg)\npipeline_lg.fit(X, y)\nplot_feature_importance(pipeline_lg, X)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv_scores_xgb  = evaluate_pipeline(pipeline_xgb, X, y, \"XGBoost\")\nmean_cv_scores_xgb = np.mean(cv_scores_xgb)\npipeline_xgb.fit(X, y)\nplot_feature_importance(pipeline_xgb, X)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Choose a model\n\nModify this variable to choose which model to submit","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Suppose these are your mean crossâ€‘validation AUROC scores:\nresults = {\"LG\": mean_cv_scores_lg, \"XGB\": mean_cv_scores_xgb, \"LGBM\": mean_cv_scores_lgbm}\nprint(\"CV results:\", results)\n\n# Normalize the weights so they sum to 1:\ntotal_score = mean_cv_scores_lg + mean_cv_scores_xgb + mean_cv_scores_lgbm\nweights = [mean_cv_scores_lg / total_score, mean_cv_scores_xgb / total_score, mean_cv_scores_lgbm / total_score]\nprint(\"Normalized weights:\", weights)\n\n# Create a voting ensemble using soft voting (using predicted probabilities)\npipeline = VotingClassifier(estimators=[\n    ('LG', pipeline_lg),\n    ('XGB', pipeline_xgb),\n    ('LGBM', pipeline_lgbm)\n], voting='soft', weights=weights)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Dictionary mapping model names to their mean cross-validation scores\n#results = {\"LG\": mean_cv_scores_lg, \"XGB\": mean_cv_scores_xgb, \"LGBM\": mean_cv_scores_lgbm}#\n#\n## Get the best model key and score\n#best_model, best_score = max(results.items(), key=lambda x: x[1])\n#print(\"Best model:\", best_model, \"with score:\", best_score)\n\n## Dictionary mapping model names to their corresponding pipelines\n#pipelines = {\"LG\": pipeline_lg, \"XGB\": pipeline_xgb, \"LGBM\": pipeline_lgbm}\n\n## Assign the pipeline corresponding to the best model\n#pipeline = pipelines[best_model]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pipeline = pipeline_lg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fitting model","metadata":{}},{"cell_type":"code","source":"# Refit on the full training set to ensure coefficients are available\npipeline.fit(X, y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Submission file","metadata":{}},{"cell_type":"code","source":"X_test = test[features]\nX_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline.fit(X,y)\n# Predict probabilities on the test set\ntest_preds = pipeline.predict_proba(X_test)[:, 1]\n\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'rainfall': test_preds\n})\n\n# Save the submission file\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds[0:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}