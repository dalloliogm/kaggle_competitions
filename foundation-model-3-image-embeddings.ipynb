{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9bd076",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-30T09:14:49.382770Z",
     "iopub.status.busy": "2025-03-30T09:14:49.382444Z",
     "iopub.status.idle": "2025-03-30T09:15:15.922998Z",
     "shell.execute_reply": "2025-03-30T09:15:15.921558Z"
    },
    "papermill": {
     "duration": 26.546694,
     "end_time": "2025-03-30T09:15:15.925278",
     "exception": false,
     "start_time": "2025-03-30T09:14:49.378584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (8349, 38)\n",
      "Training Foundation Autoencoder on cell abundance ranks...\n",
      "Foundation Autoencoder Epoch 1/30, Loss: 66.8052\n",
      "Foundation Autoencoder Epoch 2/30, Loss: 19.3671\n",
      "Foundation Autoencoder Epoch 3/30, Loss: 15.2501\n",
      "Foundation Autoencoder Epoch 4/30, Loss: 12.6367\n",
      "Foundation Autoencoder Epoch 5/30, Loss: 10.6469\n",
      "Foundation Autoencoder Epoch 6/30, Loss: 9.7660\n",
      "Foundation Autoencoder Epoch 7/30, Loss: 9.0377\n",
      "Foundation Autoencoder Epoch 8/30, Loss: 8.1585\n",
      "Foundation Autoencoder Epoch 9/30, Loss: 7.5562\n",
      "Foundation Autoencoder Epoch 10/30, Loss: 7.1817\n",
      "Foundation Autoencoder Epoch 11/30, Loss: 6.9370\n",
      "Foundation Autoencoder Epoch 12/30, Loss: 6.7522\n",
      "Foundation Autoencoder Epoch 13/30, Loss: 6.6207\n",
      "Foundation Autoencoder Epoch 14/30, Loss: 6.4620\n",
      "Foundation Autoencoder Epoch 15/30, Loss: 6.3581\n",
      "Foundation Autoencoder Epoch 16/30, Loss: 6.2273\n",
      "Foundation Autoencoder Epoch 17/30, Loss: 6.1449\n",
      "Foundation Autoencoder Epoch 18/30, Loss: 6.0698\n",
      "Foundation Autoencoder Epoch 19/30, Loss: 6.0020\n",
      "Foundation Autoencoder Epoch 20/30, Loss: 5.9389\n",
      "Foundation Autoencoder Epoch 21/30, Loss: 5.8895\n",
      "Foundation Autoencoder Epoch 22/30, Loss: 5.8434\n",
      "Foundation Autoencoder Epoch 23/30, Loss: 5.7949\n",
      "Foundation Autoencoder Epoch 24/30, Loss: 5.7497\n",
      "Foundation Autoencoder Epoch 25/30, Loss: 5.7131\n",
      "Foundation Autoencoder Epoch 26/30, Loss: 5.6804\n",
      "Foundation Autoencoder Epoch 27/30, Loss: 5.6658\n",
      "Foundation Autoencoder Epoch 28/30, Loss: 5.6157\n",
      "Foundation Autoencoder Epoch 29/30, Loss: 5.5909\n",
      "Foundation Autoencoder Epoch 30/30, Loss: 5.5446\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Load Training Data (spots/Train) with Slice Information\n",
    "# --------------------------------------------------------------------\n",
    "h5_file_path = \"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\"\n",
    "\n",
    "with h5py.File(h5_file_path, \"r\") as f:\n",
    "    train_spots = f[\"spots/Train\"]\n",
    "    # Load each slide and tag with its slice name.\n",
    "    train_spot_tables = {\n",
    "        slide: pd.DataFrame(np.array(train_spots[slide])).assign(slice_name=slide)\n",
    "        for slide in train_spots.keys()\n",
    "    }\n",
    "# Concatenate all slides into one DataFrame.\n",
    "train_df = pd.concat(train_spot_tables.values(), ignore_index=True)\n",
    "\n",
    "# Assume first two columns are coordinates, next 35 are cell abundances.\n",
    "cell_types = [f\"C{i+1}\" for i in range(35)]\n",
    "train_df.columns = [\"x\", \"y\"] + cell_types + [\"slice_name\"]\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Compute Ranks of Cell Abundances for Each Spot\n",
    "# --------------------------------------------------------------------\n",
    "# Here we compute descending ranks: highest abundance gets rank 1.\n",
    "# The DataFrame.rank method is applied row-wise (axis=1).\n",
    "ranks = train_df[cell_types].rank(axis=1, method=\"dense\", ascending=False).values\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Define the Foundation Autoencoder (learns embeddings from cell abundance ranks)\n",
    "# --------------------------------------------------------------------\n",
    "class FoundationAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=35, embed_dim=16, hidden_dim=64):\n",
    "        super(FoundationAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)\n",
    "        recon = self.decoder(emb)\n",
    "        return emb, recon\n",
    "\n",
    "# Dataset for foundation autoencoder training on ranks.\n",
    "class FoundationDataset(Dataset):\n",
    "    def __init__(self, rank_values):\n",
    "        self.data = torch.tensor(rank_values, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def train_foundation_autoencoder(model, dataloader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for data in dataloader:\n",
    "            data = data.to(device)\n",
    "            emb, recon = model(data)\n",
    "            loss = criterion(recon, data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * data.size(0)\n",
    "        print(f\"Foundation Autoencoder Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader.dataset):.4f}\")\n",
    "    return model\n",
    "\n",
    "# Create dataset and dataloader using the computed ranks.\n",
    "foundation_dataset = FoundationDataset(ranks)\n",
    "foundation_loader = DataLoader(foundation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "foundation_model = FoundationAutoencoder(input_dim=35, embed_dim=16, hidden_dim=64)\n",
    "print(\"Training Foundation Autoencoder on cell abundance ranks...\")\n",
    "foundation_model = train_foundation_autoencoder(foundation_model, foundation_loader, num_epochs=30, lr=0.001, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ae9f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:15:15.937431Z",
     "iopub.status.busy": "2025-03-30T09:15:15.936829Z",
     "iopub.status.idle": "2025-03-30T10:07:11.773468Z",
     "shell.execute_reply": "2025-03-30T10:07:11.772351Z"
    },
    "papermill": {
     "duration": 3115.844398,
     "end_time": "2025-03-30T10:07:11.775186",
     "exception": false,
     "start_time": "2025-03-30T09:15:15.930788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Main Mapping With Image Model...\n",
      "Main Mapping With Image Epoch 1/200, Loss: 1.4247\n",
      "Main Mapping With Image Epoch 2/200, Loss: 0.9764\n",
      "Main Mapping With Image Epoch 3/200, Loss: 0.9248\n",
      "Main Mapping With Image Epoch 4/200, Loss: 0.8425\n",
      "Main Mapping With Image Epoch 5/200, Loss: 0.7963\n",
      "Main Mapping With Image Epoch 6/200, Loss: 0.7723\n",
      "Main Mapping With Image Epoch 7/200, Loss: 0.7541\n",
      "Main Mapping With Image Epoch 8/200, Loss: 0.7478\n",
      "Main Mapping With Image Epoch 9/200, Loss: 0.6984\n",
      "Main Mapping With Image Epoch 10/200, Loss: 0.6942\n",
      "Main Mapping With Image Epoch 11/200, Loss: 0.6883\n",
      "Main Mapping With Image Epoch 12/200, Loss: 0.6485\n",
      "Main Mapping With Image Epoch 13/200, Loss: 0.6345\n",
      "Main Mapping With Image Epoch 14/200, Loss: 0.6096\n",
      "Main Mapping With Image Epoch 15/200, Loss: 0.6254\n",
      "Main Mapping With Image Epoch 16/200, Loss: 0.5947\n",
      "Main Mapping With Image Epoch 17/200, Loss: 0.5599\n",
      "Main Mapping With Image Epoch 18/200, Loss: 0.5363\n",
      "Main Mapping With Image Epoch 19/200, Loss: 0.5264\n",
      "Main Mapping With Image Epoch 20/200, Loss: 0.5176\n",
      "Main Mapping With Image Epoch 21/200, Loss: 0.5002\n",
      "Main Mapping With Image Epoch 22/200, Loss: 0.4834\n",
      "Main Mapping With Image Epoch 23/200, Loss: 0.4755\n",
      "Main Mapping With Image Epoch 24/200, Loss: 0.4747\n",
      "Main Mapping With Image Epoch 25/200, Loss: 0.4470\n",
      "Main Mapping With Image Epoch 26/200, Loss: 0.4506\n",
      "Main Mapping With Image Epoch 27/200, Loss: 0.4330\n",
      "Main Mapping With Image Epoch 28/200, Loss: 0.4155\n",
      "Main Mapping With Image Epoch 29/200, Loss: 0.4250\n",
      "Main Mapping With Image Epoch 30/200, Loss: 0.4279\n",
      "Main Mapping With Image Epoch 31/200, Loss: 0.3981\n",
      "Main Mapping With Image Epoch 32/200, Loss: 0.3953\n",
      "Main Mapping With Image Epoch 33/200, Loss: 0.3907\n",
      "Main Mapping With Image Epoch 34/200, Loss: 0.3672\n",
      "Main Mapping With Image Epoch 35/200, Loss: 0.3536\n",
      "Main Mapping With Image Epoch 36/200, Loss: 0.3541\n",
      "Main Mapping With Image Epoch 37/200, Loss: 0.3386\n",
      "Main Mapping With Image Epoch 38/200, Loss: 0.3368\n",
      "Main Mapping With Image Epoch 39/200, Loss: 0.3309\n",
      "Main Mapping With Image Epoch 40/200, Loss: 0.3279\n",
      "Main Mapping With Image Epoch 41/200, Loss: 0.3247\n",
      "Main Mapping With Image Epoch 42/200, Loss: 0.3093\n",
      "Main Mapping With Image Epoch 43/200, Loss: 0.2998\n",
      "Main Mapping With Image Epoch 44/200, Loss: 0.2894\n",
      "Main Mapping With Image Epoch 45/200, Loss: 0.2807\n",
      "Main Mapping With Image Epoch 46/200, Loss: 0.2870\n",
      "Main Mapping With Image Epoch 47/200, Loss: 0.2846\n",
      "Main Mapping With Image Epoch 48/200, Loss: 0.2675\n",
      "Main Mapping With Image Epoch 49/200, Loss: 0.2648\n",
      "Main Mapping With Image Epoch 50/200, Loss: 0.2462\n",
      "Main Mapping With Image Epoch 51/200, Loss: 0.2524\n",
      "Main Mapping With Image Epoch 52/200, Loss: 0.2495\n",
      "Main Mapping With Image Epoch 53/200, Loss: 0.2393\n",
      "Main Mapping With Image Epoch 54/200, Loss: 0.2288\n",
      "Main Mapping With Image Epoch 55/200, Loss: 0.2316\n",
      "Main Mapping With Image Epoch 56/200, Loss: 0.2241\n",
      "Main Mapping With Image Epoch 57/200, Loss: 0.2184\n",
      "Main Mapping With Image Epoch 58/200, Loss: 0.2144\n",
      "Main Mapping With Image Epoch 59/200, Loss: 0.2135\n",
      "Main Mapping With Image Epoch 60/200, Loss: 0.1992\n",
      "Main Mapping With Image Epoch 61/200, Loss: 0.2053\n",
      "Main Mapping With Image Epoch 62/200, Loss: 0.2021\n",
      "Main Mapping With Image Epoch 63/200, Loss: 0.1930\n",
      "Main Mapping With Image Epoch 64/200, Loss: 0.1964\n",
      "Main Mapping With Image Epoch 65/200, Loss: 0.1834\n",
      "Main Mapping With Image Epoch 66/200, Loss: 0.1940\n",
      "Main Mapping With Image Epoch 67/200, Loss: 0.1866\n",
      "Main Mapping With Image Epoch 68/200, Loss: 0.1770\n",
      "Main Mapping With Image Epoch 69/200, Loss: 0.1753\n",
      "Main Mapping With Image Epoch 70/200, Loss: 0.1741\n",
      "Main Mapping With Image Epoch 71/200, Loss: 0.1773\n",
      "Main Mapping With Image Epoch 72/200, Loss: 0.1641\n",
      "Main Mapping With Image Epoch 73/200, Loss: 0.1730\n",
      "Main Mapping With Image Epoch 74/200, Loss: 0.1675\n",
      "Main Mapping With Image Epoch 75/200, Loss: 0.1560\n",
      "Main Mapping With Image Epoch 76/200, Loss: 0.1516\n",
      "Main Mapping With Image Epoch 77/200, Loss: 0.1699\n",
      "Main Mapping With Image Epoch 78/200, Loss: 0.1539\n",
      "Main Mapping With Image Epoch 79/200, Loss: 0.1470\n",
      "Main Mapping With Image Epoch 80/200, Loss: 0.1459\n",
      "Main Mapping With Image Epoch 81/200, Loss: 0.1595\n",
      "Main Mapping With Image Epoch 82/200, Loss: 0.1448\n",
      "Main Mapping With Image Epoch 83/200, Loss: 0.1427\n",
      "Main Mapping With Image Epoch 84/200, Loss: 0.1405\n",
      "Main Mapping With Image Epoch 85/200, Loss: 0.1422\n",
      "Main Mapping With Image Epoch 86/200, Loss: 0.1407\n",
      "Main Mapping With Image Epoch 87/200, Loss: 0.1423\n",
      "Main Mapping With Image Epoch 88/200, Loss: 0.1395\n",
      "Main Mapping With Image Epoch 89/200, Loss: 0.1444\n",
      "Main Mapping With Image Epoch 90/200, Loss: 0.1322\n",
      "Main Mapping With Image Epoch 91/200, Loss: 0.1296\n",
      "Main Mapping With Image Epoch 92/200, Loss: 0.1372\n",
      "Main Mapping With Image Epoch 93/200, Loss: 0.1342\n",
      "Main Mapping With Image Epoch 94/200, Loss: 0.1332\n",
      "Main Mapping With Image Epoch 95/200, Loss: 0.1238\n",
      "Main Mapping With Image Epoch 96/200, Loss: 0.1260\n",
      "Main Mapping With Image Epoch 97/200, Loss: 0.1246\n",
      "Main Mapping With Image Epoch 98/200, Loss: 0.1283\n",
      "Main Mapping With Image Epoch 99/200, Loss: 0.1195\n",
      "Main Mapping With Image Epoch 100/200, Loss: 0.1272\n",
      "Main Mapping With Image Epoch 101/200, Loss: 0.1327\n",
      "Main Mapping With Image Epoch 102/200, Loss: 0.1222\n",
      "Main Mapping With Image Epoch 103/200, Loss: 0.1284\n",
      "Main Mapping With Image Epoch 104/200, Loss: 0.1204\n",
      "Main Mapping With Image Epoch 105/200, Loss: 0.1154\n",
      "Main Mapping With Image Epoch 106/200, Loss: 0.1186\n",
      "Main Mapping With Image Epoch 107/200, Loss: 0.1228\n",
      "Main Mapping With Image Epoch 108/200, Loss: 0.1180\n",
      "Main Mapping With Image Epoch 109/200, Loss: 0.1152\n",
      "Main Mapping With Image Epoch 110/200, Loss: 0.1370\n",
      "Main Mapping With Image Epoch 111/200, Loss: 0.1135\n",
      "Main Mapping With Image Epoch 112/200, Loss: 0.1152\n",
      "Main Mapping With Image Epoch 113/200, Loss: 0.1199\n",
      "Main Mapping With Image Epoch 114/200, Loss: 0.1217\n",
      "Main Mapping With Image Epoch 115/200, Loss: 0.1163\n",
      "Main Mapping With Image Epoch 116/200, Loss: 0.1191\n",
      "Main Mapping With Image Epoch 117/200, Loss: 0.1172\n",
      "Main Mapping With Image Epoch 118/200, Loss: 0.1096\n",
      "Main Mapping With Image Epoch 119/200, Loss: 0.1100\n",
      "Main Mapping With Image Epoch 120/200, Loss: 0.1189\n",
      "Main Mapping With Image Epoch 121/200, Loss: 0.1126\n",
      "Main Mapping With Image Epoch 122/200, Loss: 0.1100\n",
      "Main Mapping With Image Epoch 123/200, Loss: 0.1147\n",
      "Main Mapping With Image Epoch 124/200, Loss: 0.1208\n",
      "Main Mapping With Image Epoch 125/200, Loss: 0.1066\n",
      "Main Mapping With Image Epoch 126/200, Loss: 0.1103\n",
      "Main Mapping With Image Epoch 127/200, Loss: 0.1175\n",
      "Main Mapping With Image Epoch 128/200, Loss: 0.1154\n",
      "Main Mapping With Image Epoch 129/200, Loss: 0.1105\n",
      "Main Mapping With Image Epoch 130/200, Loss: 0.1122\n",
      "Main Mapping With Image Epoch 131/200, Loss: 0.1105\n",
      "Main Mapping With Image Epoch 132/200, Loss: 0.1093\n",
      "Main Mapping With Image Epoch 133/200, Loss: 0.1251\n",
      "Main Mapping With Image Epoch 134/200, Loss: 0.1101\n",
      "Main Mapping With Image Epoch 135/200, Loss: 0.1027\n",
      "Main Mapping With Image Epoch 136/200, Loss: 0.1048\n",
      "Main Mapping With Image Epoch 137/200, Loss: 0.1008\n",
      "Main Mapping With Image Epoch 138/200, Loss: 0.1125\n",
      "Main Mapping With Image Epoch 139/200, Loss: 0.1095\n",
      "Main Mapping With Image Epoch 140/200, Loss: 0.1081\n",
      "Main Mapping With Image Epoch 141/200, Loss: 0.1062\n",
      "Main Mapping With Image Epoch 142/200, Loss: 0.1057\n",
      "Main Mapping With Image Epoch 143/200, Loss: 0.1038\n",
      "Main Mapping With Image Epoch 144/200, Loss: 0.1205\n",
      "Main Mapping With Image Epoch 145/200, Loss: 0.1182\n",
      "Main Mapping With Image Epoch 146/200, Loss: 0.1036\n",
      "Main Mapping With Image Epoch 147/200, Loss: 0.1015\n",
      "Main Mapping With Image Epoch 148/200, Loss: 0.1041\n",
      "Main Mapping With Image Epoch 149/200, Loss: 0.1089\n",
      "Main Mapping With Image Epoch 150/200, Loss: 0.1033\n",
      "Main Mapping With Image Epoch 151/200, Loss: 0.1011\n",
      "Main Mapping With Image Epoch 152/200, Loss: 0.1090\n",
      "Main Mapping With Image Epoch 153/200, Loss: 0.1020\n",
      "Main Mapping With Image Epoch 154/200, Loss: 0.1072\n",
      "Main Mapping With Image Epoch 155/200, Loss: 0.1074\n",
      "Main Mapping With Image Epoch 156/200, Loss: 0.1039\n",
      "Main Mapping With Image Epoch 157/200, Loss: 0.1085\n",
      "Main Mapping With Image Epoch 158/200, Loss: 0.1041\n",
      "Main Mapping With Image Epoch 159/200, Loss: 0.1053\n",
      "Main Mapping With Image Epoch 160/200, Loss: 0.1022\n",
      "Main Mapping With Image Epoch 161/200, Loss: 0.1012\n",
      "Main Mapping With Image Epoch 162/200, Loss: 0.1033\n",
      "Main Mapping With Image Epoch 163/200, Loss: 0.1067\n",
      "Main Mapping With Image Epoch 164/200, Loss: 0.1050\n",
      "Main Mapping With Image Epoch 165/200, Loss: 0.0998\n",
      "Main Mapping With Image Epoch 166/200, Loss: 0.0986\n",
      "Main Mapping With Image Epoch 167/200, Loss: 0.1046\n",
      "Main Mapping With Image Epoch 168/200, Loss: 0.1047\n",
      "Main Mapping With Image Epoch 169/200, Loss: 0.1050\n",
      "Main Mapping With Image Epoch 170/200, Loss: 0.1008\n",
      "Main Mapping With Image Epoch 171/200, Loss: 0.0979\n",
      "Main Mapping With Image Epoch 172/200, Loss: 0.1001\n",
      "Main Mapping With Image Epoch 173/200, Loss: 0.1029\n",
      "Main Mapping With Image Epoch 174/200, Loss: 0.0998\n",
      "Main Mapping With Image Epoch 175/200, Loss: 0.1020\n",
      "Main Mapping With Image Epoch 176/200, Loss: 0.0980\n",
      "Main Mapping With Image Epoch 177/200, Loss: 0.0983\n",
      "Main Mapping With Image Epoch 178/200, Loss: 0.1016\n",
      "Main Mapping With Image Epoch 179/200, Loss: 0.0991\n",
      "Main Mapping With Image Epoch 180/200, Loss: 0.1007\n",
      "Main Mapping With Image Epoch 181/200, Loss: 0.1011\n",
      "Main Mapping With Image Epoch 182/200, Loss: 0.1010\n",
      "Main Mapping With Image Epoch 183/200, Loss: 0.0987\n",
      "Main Mapping With Image Epoch 184/200, Loss: 0.1023\n",
      "Main Mapping With Image Epoch 185/200, Loss: 0.0970\n",
      "Main Mapping With Image Epoch 186/200, Loss: 0.0981\n",
      "Main Mapping With Image Epoch 187/200, Loss: 0.1017\n",
      "Main Mapping With Image Epoch 188/200, Loss: 0.1044\n",
      "Main Mapping With Image Epoch 189/200, Loss: 0.0977\n",
      "Main Mapping With Image Epoch 190/200, Loss: 0.0939\n",
      "Main Mapping With Image Epoch 191/200, Loss: 0.0957\n",
      "Main Mapping With Image Epoch 192/200, Loss: 0.0981\n",
      "Main Mapping With Image Epoch 193/200, Loss: 0.0986\n",
      "Main Mapping With Image Epoch 194/200, Loss: 0.0971\n",
      "Main Mapping With Image Epoch 195/200, Loss: 0.0993\n",
      "Main Mapping With Image Epoch 196/200, Loss: 0.0956\n",
      "Main Mapping With Image Epoch 197/200, Loss: 0.0936\n",
      "Main Mapping With Image Epoch 198/200, Loss: 0.0952\n",
      "Main Mapping With Image Epoch 199/200, Loss: 0.0983\n",
      "Main Mapping With Image Epoch 200/200, Loss: 0.1005\n",
      "Test data shape: (2088, 3)\n",
      "Submission file 'submission.csv' created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Precompute Foundation Embeddings for All Training Spots\n",
    "# --------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    foundation_model.eval()\n",
    "    train_abundances = torch.tensor(train_df[cell_types].values, dtype=torch.float32).to(device)\n",
    "    train_embeddings, _ = foundation_model(train_abundances)\n",
    "    train_embeddings = train_embeddings.cpu().numpy()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Define Dataset Classes that Load Images from the H5 File\n",
    "# --------------------------------------------------------------------\n",
    "class MainMappingWithImageH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    For training: Maps (x, y) coordinates and the corresponding image patch \n",
    "    (loaded from the H5 file) to the precomputed foundation embedding.\n",
    "    Expects a DataFrame with columns: \"x\", \"y\", and \"slice_name\".\n",
    "    \"\"\"\n",
    "    def __init__(self, df, embeddings, h5_file_path, patch_size=64, transform=None, train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.embeddings = embeddings  # Precomputed embeddings in the same order as df.\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.train = train\n",
    "        self.images = {}\n",
    "        group = \"Train\" if train else \"Test\"\n",
    "        # Load images for each unique slice from the H5 file.\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            for slice_name in self.df['slice_name'].unique():\n",
    "                img_array = np.array(f[f\"images/{group}\"][slice_name])\n",
    "                # Normalize and convert to uint8 if necessary.\n",
    "                if img_array.dtype != np.uint8:\n",
    "                    img_array = img_array - img_array.min()\n",
    "                    if img_array.max() > 0:\n",
    "                        img_array = img_array / img_array.max()\n",
    "                    img_array = (img_array * 255).astype(np.uint8)\n",
    "                if img_array.ndim > 3:\n",
    "                    img_array = np.squeeze(img_array)\n",
    "                # If the image is grayscale (2D), convert to RGB.\n",
    "                if img_array.ndim == 2:\n",
    "                    img_array = np.stack([img_array]*3, axis=-1)\n",
    "                if img_array.shape[-1] != 3:\n",
    "                    raise ValueError(f\"Unexpected number of channels in image for slice {slice_name}: {img_array.shape}\")\n",
    "                self.images[slice_name] = Image.fromarray(img_array, mode=\"RGB\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        coord = np.array([row['x'], row['y']], dtype=np.float32)\n",
    "        slice_name = row['slice_name']\n",
    "        image = self.images[slice_name]\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "        half_patch = self.patch_size // 2\n",
    "        left = max(x - half_patch, 0)\n",
    "        upper = max(y - half_patch, 0)\n",
    "        right = left + self.patch_size\n",
    "        lower = upper + self.patch_size\n",
    "        patch = image.crop((left, upper, right, lower))\n",
    "        patch = self.transform(patch)\n",
    "        target = self.embeddings[idx]\n",
    "        return torch.tensor(coord, dtype=torch.float32), patch, torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "class TestMappingWithImageH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    For testing: Maps (x, y) coordinates and the corresponding image patch \n",
    "    (loaded from the H5 file) to be used for prediction.\n",
    "    Expects a DataFrame with columns: \"x\", \"y\", and \"slice_name\".\n",
    "    \"\"\"\n",
    "    def __init__(self, df, h5_file_path, patch_size=64, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.images = {}\n",
    "        group = \"Test\"\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            for slice_name in self.df['slice_name'].unique():\n",
    "                img_array = np.array(f[f\"images/{group}\"][slice_name])\n",
    "                if img_array.dtype != np.uint8:\n",
    "                    img_array = img_array - img_array.min()\n",
    "                    if img_array.max() > 0:\n",
    "                        img_array = img_array / img_array.max()\n",
    "                    img_array = (img_array * 255).astype(np.uint8)\n",
    "                if img_array.ndim > 3:\n",
    "                    img_array = np.squeeze(img_array)\n",
    "                if img_array.ndim == 2:\n",
    "                    img_array = np.stack([img_array]*3, axis=-1)\n",
    "                if img_array.shape[-1] != 3:\n",
    "                    raise ValueError(f\"Unexpected number of channels in image for slice {slice_name}: {img_array.shape}\")\n",
    "                self.images[slice_name] = Image.fromarray(img_array, mode=\"RGB\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        coord = np.array([row['x'], row['y']], dtype=np.float32)\n",
    "        slice_name = row['slice_name']\n",
    "        image = self.images[slice_name]\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "        half_patch = self.patch_size // 2\n",
    "        left = max(x - half_patch, 0)\n",
    "        upper = max(y - half_patch, 0)\n",
    "        right = left + self.patch_size\n",
    "        lower = upper + self.patch_size\n",
    "        patch = image.crop((left, upper, right, lower))\n",
    "        patch = self.transform(patch)\n",
    "        return torch.tensor(coord, dtype=torch.float32), patch\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Define the Main Model that Uses Image Patches and Coordinates\n",
    "# --------------------------------------------------------------------\n",
    "class MainModelMappingWithImage(nn.Module):\n",
    "    def __init__(self, coord_input_dim=2, patch_channels=3, patch_size=64, embed_dim=16, hidden_dim=64):\n",
    "        super(MainModelMappingWithImage, self).__init__()\n",
    "        # CNN to encode image patches.\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(patch_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Calculate the image feature dimension.\n",
    "        img_feat_dim = 32 * (patch_size // 4) * (patch_size // 4)\n",
    "        # Fully connected layers to map concatenated [coords, image_features] to embedding.\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(coord_input_dim + img_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, coords, image_patches):\n",
    "        img_features = self.image_encoder(image_patches)\n",
    "        combined = torch.cat([coords, img_features], dim=1)\n",
    "        embedding = self.fc(combined)\n",
    "        return embedding\n",
    "\n",
    "def train_main_mapping_with_image(model, dataloader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for coords, patches, target_emb in dataloader:\n",
    "            coords = coords.to(device)\n",
    "            patches = patches.to(device)\n",
    "            target_emb = target_emb.to(device)\n",
    "            pred_emb = model(coords, patches)\n",
    "            loss = criterion(pred_emb, target_emb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * coords.size(0)\n",
    "        print(f\"Main Mapping With Image Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader.dataset):.4f}\")\n",
    "    return model\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6. Create Dataset and Train the Main Mapping with Image Model (Training)\n",
    "# --------------------------------------------------------------------\n",
    "patch_size = 64\n",
    "\n",
    "# Create the training dataset using images from the H5 file.\n",
    "main_image_dataset = MainMappingWithImageH5Dataset(\n",
    "    train_df, train_embeddings, h5_file_path, patch_size=patch_size, transform=transforms.ToTensor(), train=True\n",
    ")\n",
    "main_image_loader = DataLoader(main_image_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "main_model_img = MainModelMappingWithImage(coord_input_dim=2, patch_channels=3, patch_size=patch_size, embed_dim=16, hidden_dim=64)\n",
    "print(\"Training Main Mapping With Image Model...\")\n",
    "main_model_img = train_main_mapping_with_image(main_model_img, main_image_loader, num_epochs=200, lr=0.001, device=device)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 7. Inference on Test Data and Submission Creation\n",
    "# --------------------------------------------------------------------\n",
    "# Load test spots for slide \"S_7\" from the H5 file.\n",
    "with h5py.File(h5_file_path, \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_array = np.array(test_spots[\"S_7\"])\n",
    "    test_df = pd.DataFrame(test_array)\n",
    "# Test file has three columns: x, y, Test_set. Drop the third column.\n",
    "if test_df.shape[1] == 3:\n",
    "    test_df.columns = [\"x\", \"y\", \"Test_set\"]\n",
    "    test_df = test_df[[\"x\", \"y\"]]\n",
    "# Add slice_name column so we know which image to load.\n",
    "test_df[\"slice_name\"] = \"S_7\"\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "# Create the test dataset (loading images from H5).\n",
    "test_dataset = TestMappingWithImageH5Dataset(test_df, h5_file_path, patch_size=patch_size, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Predict foundation embeddings from test spots using the main mapping model with image.\n",
    "predicted_embeddings_list = []\n",
    "main_model_img.eval()\n",
    "with torch.no_grad():\n",
    "    for coords, patches in test_loader:\n",
    "        coords = coords.to(device)\n",
    "        patches = patches.to(device)\n",
    "        pred_emb = main_model_img(coords, patches)\n",
    "        predicted_embeddings_list.append(pred_emb)\n",
    "    predicted_embeddings = torch.cat(predicted_embeddings_list, dim=0)\n",
    "\n",
    "# Use the foundation decoder to convert predicted embeddings into cell abundance predictions.\n",
    "foundation_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_abundances = foundation_model.decoder(predicted_embeddings)\n",
    "    predicted_abundances = predicted_abundances.cpu().numpy()\n",
    "\n",
    "# Create submission DataFrame and save CSV.\n",
    "submission_df = pd.DataFrame(predicted_abundances, columns=cell_types)\n",
    "submission_df.insert(0, 'ID', test_df.index)\n",
    "submission_file = \"submission.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "print(f\"Submission file '{submission_file}' created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf35224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T10:07:11.805430Z",
     "iopub.status.busy": "2025-03-30T10:07:11.805061Z",
     "iopub.status.idle": "2025-03-30T10:07:11.841712Z",
     "shell.execute_reply": "2025-03-30T10:07:11.840568Z"
    },
    "papermill": {
     "duration": 0.053334,
     "end_time": "2025-03-30T10:07:11.843337",
     "exception": false,
     "start_time": "2025-03-30T10:07:11.790003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>...</th>\n",
       "      <th>C26</th>\n",
       "      <th>C27</th>\n",
       "      <th>C28</th>\n",
       "      <th>C29</th>\n",
       "      <th>C30</th>\n",
       "      <th>C31</th>\n",
       "      <th>C32</th>\n",
       "      <th>C33</th>\n",
       "      <th>C34</th>\n",
       "      <th>C35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.763583</td>\n",
       "      <td>0.559712</td>\n",
       "      <td>0.504092</td>\n",
       "      <td>0.377614</td>\n",
       "      <td>0.955572</td>\n",
       "      <td>0.445789</td>\n",
       "      <td>0.504675</td>\n",
       "      <td>0.643154</td>\n",
       "      <td>0.572241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544684</td>\n",
       "      <td>0.521559</td>\n",
       "      <td>0.708626</td>\n",
       "      <td>0.577624</td>\n",
       "      <td>0.682163</td>\n",
       "      <td>0.370543</td>\n",
       "      <td>0.430406</td>\n",
       "      <td>0.508911</td>\n",
       "      <td>0.352763</td>\n",
       "      <td>0.456446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.943129</td>\n",
       "      <td>0.550176</td>\n",
       "      <td>0.574221</td>\n",
       "      <td>0.320776</td>\n",
       "      <td>0.416162</td>\n",
       "      <td>0.512455</td>\n",
       "      <td>0.390099</td>\n",
       "      <td>0.339472</td>\n",
       "      <td>0.505287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316860</td>\n",
       "      <td>0.373518</td>\n",
       "      <td>0.778617</td>\n",
       "      <td>0.356571</td>\n",
       "      <td>0.353154</td>\n",
       "      <td>0.301482</td>\n",
       "      <td>0.388514</td>\n",
       "      <td>0.433179</td>\n",
       "      <td>0.139234</td>\n",
       "      <td>0.384685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.809420</td>\n",
       "      <td>0.486681</td>\n",
       "      <td>0.526829</td>\n",
       "      <td>0.328963</td>\n",
       "      <td>0.373891</td>\n",
       "      <td>0.747974</td>\n",
       "      <td>0.418411</td>\n",
       "      <td>0.407674</td>\n",
       "      <td>0.841889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403996</td>\n",
       "      <td>0.739756</td>\n",
       "      <td>0.937707</td>\n",
       "      <td>0.403693</td>\n",
       "      <td>0.413494</td>\n",
       "      <td>0.278678</td>\n",
       "      <td>0.439418</td>\n",
       "      <td>0.603190</td>\n",
       "      <td>0.174138</td>\n",
       "      <td>0.424326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.818887</td>\n",
       "      <td>0.548490</td>\n",
       "      <td>0.500727</td>\n",
       "      <td>0.198702</td>\n",
       "      <td>0.324906</td>\n",
       "      <td>0.473520</td>\n",
       "      <td>0.370911</td>\n",
       "      <td>0.265438</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293712</td>\n",
       "      <td>0.338673</td>\n",
       "      <td>0.752931</td>\n",
       "      <td>0.306866</td>\n",
       "      <td>0.359484</td>\n",
       "      <td>0.291650</td>\n",
       "      <td>0.383856</td>\n",
       "      <td>0.418729</td>\n",
       "      <td>0.080695</td>\n",
       "      <td>0.372940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.818887</td>\n",
       "      <td>0.548490</td>\n",
       "      <td>0.500727</td>\n",
       "      <td>0.198702</td>\n",
       "      <td>0.324906</td>\n",
       "      <td>0.473520</td>\n",
       "      <td>0.370911</td>\n",
       "      <td>0.265438</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293712</td>\n",
       "      <td>0.338673</td>\n",
       "      <td>0.752931</td>\n",
       "      <td>0.306866</td>\n",
       "      <td>0.359484</td>\n",
       "      <td>0.291650</td>\n",
       "      <td>0.383856</td>\n",
       "      <td>0.418729</td>\n",
       "      <td>0.080695</td>\n",
       "      <td>0.372940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>2083</td>\n",
       "      <td>0.818887</td>\n",
       "      <td>0.548490</td>\n",
       "      <td>0.500727</td>\n",
       "      <td>0.198702</td>\n",
       "      <td>0.324906</td>\n",
       "      <td>0.473520</td>\n",
       "      <td>0.370911</td>\n",
       "      <td>0.265438</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293712</td>\n",
       "      <td>0.338673</td>\n",
       "      <td>0.752931</td>\n",
       "      <td>0.306866</td>\n",
       "      <td>0.359484</td>\n",
       "      <td>0.291650</td>\n",
       "      <td>0.383856</td>\n",
       "      <td>0.418729</td>\n",
       "      <td>0.080695</td>\n",
       "      <td>0.372940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>2084</td>\n",
       "      <td>1.006178</td>\n",
       "      <td>0.560337</td>\n",
       "      <td>0.842047</td>\n",
       "      <td>0.748770</td>\n",
       "      <td>0.764607</td>\n",
       "      <td>0.903455</td>\n",
       "      <td>0.599092</td>\n",
       "      <td>0.606300</td>\n",
       "      <td>1.498239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713476</td>\n",
       "      <td>1.280247</td>\n",
       "      <td>1.351504</td>\n",
       "      <td>0.780125</td>\n",
       "      <td>0.661228</td>\n",
       "      <td>0.384516</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>1.016193</td>\n",
       "      <td>0.420525</td>\n",
       "      <td>0.505329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>2085</td>\n",
       "      <td>0.818887</td>\n",
       "      <td>0.548490</td>\n",
       "      <td>0.500727</td>\n",
       "      <td>0.198702</td>\n",
       "      <td>0.324906</td>\n",
       "      <td>0.473520</td>\n",
       "      <td>0.370911</td>\n",
       "      <td>0.265438</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293712</td>\n",
       "      <td>0.338673</td>\n",
       "      <td>0.752931</td>\n",
       "      <td>0.306866</td>\n",
       "      <td>0.359484</td>\n",
       "      <td>0.291650</td>\n",
       "      <td>0.383856</td>\n",
       "      <td>0.418729</td>\n",
       "      <td>0.080695</td>\n",
       "      <td>0.372940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>2086</td>\n",
       "      <td>0.818887</td>\n",
       "      <td>0.548490</td>\n",
       "      <td>0.500727</td>\n",
       "      <td>0.198702</td>\n",
       "      <td>0.324906</td>\n",
       "      <td>0.473520</td>\n",
       "      <td>0.370911</td>\n",
       "      <td>0.265438</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293712</td>\n",
       "      <td>0.338673</td>\n",
       "      <td>0.752931</td>\n",
       "      <td>0.306866</td>\n",
       "      <td>0.359484</td>\n",
       "      <td>0.291650</td>\n",
       "      <td>0.383856</td>\n",
       "      <td>0.418729</td>\n",
       "      <td>0.080695</td>\n",
       "      <td>0.372940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>2087</td>\n",
       "      <td>0.811604</td>\n",
       "      <td>0.487918</td>\n",
       "      <td>0.533224</td>\n",
       "      <td>0.339430</td>\n",
       "      <td>0.377656</td>\n",
       "      <td>0.756727</td>\n",
       "      <td>0.423616</td>\n",
       "      <td>0.418293</td>\n",
       "      <td>0.862499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411267</td>\n",
       "      <td>0.756894</td>\n",
       "      <td>0.947939</td>\n",
       "      <td>0.410799</td>\n",
       "      <td>0.420229</td>\n",
       "      <td>0.282764</td>\n",
       "      <td>0.445153</td>\n",
       "      <td>0.615069</td>\n",
       "      <td>0.184415</td>\n",
       "      <td>0.427886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID        C1        C2        C3        C4        C5        C6  \\\n",
       "0        0  0.763583  0.559712  0.504092  0.377614  0.955572  0.445789   \n",
       "1        1  0.943129  0.550176  0.574221  0.320776  0.416162  0.512455   \n",
       "2        2  0.809420  0.486681  0.526829  0.328963  0.373891  0.747974   \n",
       "3        3  0.818887  0.548490  0.500727  0.198702  0.324906  0.473520   \n",
       "4        4  0.818887  0.548490  0.500727  0.198702  0.324906  0.473520   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "2083  2083  0.818887  0.548490  0.500727  0.198702  0.324906  0.473520   \n",
       "2084  2084  1.006178  0.560337  0.842047  0.748770  0.764607  0.903455   \n",
       "2085  2085  0.818887  0.548490  0.500727  0.198702  0.324906  0.473520   \n",
       "2086  2086  0.818887  0.548490  0.500727  0.198702  0.324906  0.473520   \n",
       "2087  2087  0.811604  0.487918  0.533224  0.339430  0.377656  0.756727   \n",
       "\n",
       "            C7        C8        C9  ...       C26       C27       C28  \\\n",
       "0     0.504675  0.643154  0.572241  ...  0.544684  0.521559  0.708626   \n",
       "1     0.390099  0.339472  0.505287  ...  0.316860  0.373518  0.778617   \n",
       "2     0.418411  0.407674  0.841889  ...  0.403996  0.739756  0.937707   \n",
       "3     0.370911  0.265438  0.498343  ...  0.293712  0.338673  0.752931   \n",
       "4     0.370911  0.265438  0.498343  ...  0.293712  0.338673  0.752931   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2083  0.370911  0.265438  0.498343  ...  0.293712  0.338673  0.752931   \n",
       "2084  0.599092  0.606300  1.498239  ...  0.713476  1.280247  1.351504   \n",
       "2085  0.370911  0.265438  0.498343  ...  0.293712  0.338673  0.752931   \n",
       "2086  0.370911  0.265438  0.498343  ...  0.293712  0.338673  0.752931   \n",
       "2087  0.423616  0.418293  0.862499  ...  0.411267  0.756894  0.947939   \n",
       "\n",
       "           C29       C30       C31       C32       C33       C34       C35  \n",
       "0     0.577624  0.682163  0.370543  0.430406  0.508911  0.352763  0.456446  \n",
       "1     0.356571  0.353154  0.301482  0.388514  0.433179  0.139234  0.384685  \n",
       "2     0.403693  0.413494  0.278678  0.439418  0.603190  0.174138  0.424326  \n",
       "3     0.306866  0.359484  0.291650  0.383856  0.418729  0.080695  0.372940  \n",
       "4     0.306866  0.359484  0.291650  0.383856  0.418729  0.080695  0.372940  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2083  0.306866  0.359484  0.291650  0.383856  0.418729  0.080695  0.372940  \n",
       "2084  0.780125  0.661228  0.384516  0.611429  1.016193  0.420525  0.505329  \n",
       "2085  0.306866  0.359484  0.291650  0.383856  0.418729  0.080695  0.372940  \n",
       "2086  0.306866  0.359484  0.291650  0.383856  0.418729  0.080695  0.372940  \n",
       "2087  0.410799  0.420229  0.282764  0.445153  0.615069  0.184415  0.427886  \n",
       "\n",
       "[2088 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11390004,
     "sourceId": 94147,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3147.132265,
   "end_time": "2025-03-30T10:07:13.687085",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-30T09:14:46.554820",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
