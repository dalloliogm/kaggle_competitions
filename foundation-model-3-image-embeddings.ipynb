{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b643952d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-28T18:21:08.683504Z",
     "iopub.status.busy": "2025-03-28T18:21:08.683104Z",
     "iopub.status.idle": "2025-03-28T18:27:28.528130Z",
     "shell.execute_reply": "2025-03-28T18:27:28.526850Z"
    },
    "papermill": {
     "duration": 379.850851,
     "end_time": "2025-03-28T18:27:28.530062",
     "exception": false,
     "start_time": "2025-03-28T18:21:08.679211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (8349, 38)\n",
      "Training Foundation Autoencoder...\n",
      "Foundation Autoencoder Epoch 1/20, Loss: 0.3169\n",
      "Foundation Autoencoder Epoch 2/20, Loss: 0.0383\n",
      "Foundation Autoencoder Epoch 3/20, Loss: 0.0247\n",
      "Foundation Autoencoder Epoch 4/20, Loss: 0.0189\n",
      "Foundation Autoencoder Epoch 5/20, Loss: 0.0148\n",
      "Foundation Autoencoder Epoch 6/20, Loss: 0.0124\n",
      "Foundation Autoencoder Epoch 7/20, Loss: 0.0096\n",
      "Foundation Autoencoder Epoch 8/20, Loss: 0.0079\n",
      "Foundation Autoencoder Epoch 9/20, Loss: 0.0068\n",
      "Foundation Autoencoder Epoch 10/20, Loss: 0.0061\n",
      "Foundation Autoencoder Epoch 11/20, Loss: 0.0059\n",
      "Foundation Autoencoder Epoch 12/20, Loss: 0.0057\n",
      "Foundation Autoencoder Epoch 13/20, Loss: 0.0050\n",
      "Foundation Autoencoder Epoch 14/20, Loss: 0.0046\n",
      "Foundation Autoencoder Epoch 15/20, Loss: 0.0043\n",
      "Foundation Autoencoder Epoch 16/20, Loss: 0.0041\n",
      "Foundation Autoencoder Epoch 17/20, Loss: 0.0038\n",
      "Foundation Autoencoder Epoch 18/20, Loss: 0.0036\n",
      "Foundation Autoencoder Epoch 19/20, Loss: 0.0033\n",
      "Foundation Autoencoder Epoch 20/20, Loss: 0.0040\n",
      "Training Main Mapping With Image Model...\n",
      "Main Mapping With Image Epoch 1/20, Loss: 2.4807\n",
      "Main Mapping With Image Epoch 2/20, Loss: 2.2946\n",
      "Main Mapping With Image Epoch 3/20, Loss: 2.1633\n",
      "Main Mapping With Image Epoch 4/20, Loss: 2.0693\n",
      "Main Mapping With Image Epoch 5/20, Loss: 1.9738\n",
      "Main Mapping With Image Epoch 6/20, Loss: 1.9331\n",
      "Main Mapping With Image Epoch 7/20, Loss: 1.8687\n",
      "Main Mapping With Image Epoch 8/20, Loss: 1.8033\n",
      "Main Mapping With Image Epoch 9/20, Loss: 1.7368\n",
      "Main Mapping With Image Epoch 10/20, Loss: 1.6976\n",
      "Main Mapping With Image Epoch 11/20, Loss: 1.6203\n",
      "Main Mapping With Image Epoch 12/20, Loss: 1.5224\n",
      "Main Mapping With Image Epoch 13/20, Loss: 1.4924\n",
      "Main Mapping With Image Epoch 14/20, Loss: 1.3892\n",
      "Main Mapping With Image Epoch 15/20, Loss: 1.3162\n",
      "Main Mapping With Image Epoch 16/20, Loss: 1.2086\n",
      "Main Mapping With Image Epoch 17/20, Loss: 1.1883\n",
      "Main Mapping With Image Epoch 18/20, Loss: 1.0969\n",
      "Main Mapping With Image Epoch 19/20, Loss: 1.1032\n",
      "Main Mapping With Image Epoch 20/20, Loss: 1.0449\n",
      "Test data shape: (2088, 3)\n",
      "Submission file 'submission.csv' created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Load Training Data (spots/Train) with Slice Information\n",
    "# --------------------------------------------------------------------\n",
    "h5_file_path = \"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\"\n",
    "\n",
    "with h5py.File(h5_file_path, \"r\") as f:\n",
    "    train_spots = f[\"spots/Train\"]\n",
    "    # Load each slide and tag with its slice name.\n",
    "    train_spot_tables = {\n",
    "        slide: pd.DataFrame(np.array(train_spots[slide])).assign(slice_name=slide)\n",
    "        for slide in train_spots.keys()\n",
    "    }\n",
    "# Concatenate all slides\n",
    "train_df = pd.concat(train_spot_tables.values(), ignore_index=True)\n",
    "\n",
    "# Assume first two columns are coordinates, next 35 are cell abundances.\n",
    "cell_types = [f\"C{i+1}\" for i in range(35)]\n",
    "train_df.columns = [\"x\", \"y\"] + cell_types + [\"slice_name\"]\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Define the Foundation Autoencoder (learns embeddings from cell abundances)\n",
    "# --------------------------------------------------------------------\n",
    "class FoundationAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=35, embed_dim=16, hidden_dim=64):\n",
    "        super(FoundationAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)\n",
    "        recon = self.decoder(emb)\n",
    "        return emb, recon\n",
    "\n",
    "# Dataset for foundation autoencoder training\n",
    "class FoundationDataset(Dataset):\n",
    "    def __init__(self, abundances):\n",
    "        self.data = torch.tensor(abundances, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def train_foundation_autoencoder(model, dataloader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for data in dataloader:\n",
    "            data = data.to(device)\n",
    "            emb, recon = model(data)\n",
    "            loss = criterion(recon, data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * data.size(0)\n",
    "        print(f\"Foundation Autoencoder Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader.dataset):.4f}\")\n",
    "    return model\n",
    "\n",
    "# Create dataset and dataloader for the foundation autoencoder\n",
    "foundation_dataset = FoundationDataset(train_df[cell_types].values)\n",
    "foundation_loader = DataLoader(foundation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "foundation_model = FoundationAutoencoder(input_dim=35, embed_dim=16, hidden_dim=64)\n",
    "print(\"Training Foundation Autoencoder...\")\n",
    "foundation_model = train_foundation_autoencoder(foundation_model, foundation_loader, num_epochs=20, lr=0.001, device=device)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Precompute Foundation Embeddings for All Training Spots\n",
    "# --------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    foundation_model.eval()\n",
    "    train_abundances = torch.tensor(train_df[cell_types].values, dtype=torch.float32).to(device)\n",
    "    train_embeddings, _ = foundation_model(train_abundances)\n",
    "    train_embeddings = train_embeddings.cpu().numpy()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Define Dataset Classes that Load Images from the H5 File\n",
    "# --------------------------------------------------------------------\n",
    "class MainMappingWithImageH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    For training: Maps (x, y) coordinates and the corresponding image patch \n",
    "    (loaded from the H5 file) to the precomputed foundation embedding.\n",
    "    Expects a DataFrame with columns: \"x\", \"y\", and \"slice_name\".\n",
    "    \"\"\"\n",
    "    def __init__(self, df, embeddings, h5_file_path, patch_size=64, transform=None, train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.embeddings = embeddings  # Precomputed embeddings in the same order as df.\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.train = train\n",
    "        self.images = {}\n",
    "        group = \"Train\" if train else \"Test\"\n",
    "        # Load images for each unique slice from the H5 file.\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            for slice_name in self.df['slice_name'].unique():\n",
    "                img_array = np.array(f[f\"images/{group}\"][slice_name])\n",
    "                # Normalize and convert to uint8 if necessary.\n",
    "                if img_array.dtype != np.uint8:\n",
    "                    img_array = img_array - img_array.min()\n",
    "                    if img_array.max() > 0:\n",
    "                        img_array = img_array / img_array.max()\n",
    "                    img_array = (img_array * 255).astype(np.uint8)\n",
    "                if img_array.ndim > 3:\n",
    "                    img_array = np.squeeze(img_array)\n",
    "                # If the image is grayscale (2D), convert to RGB.\n",
    "                if img_array.ndim == 2:\n",
    "                    img_array = np.stack([img_array]*3, axis=-1)\n",
    "                if img_array.shape[-1] != 3:\n",
    "                    raise ValueError(f\"Unexpected number of channels in image for slice {slice_name}: {img_array.shape}\")\n",
    "                self.images[slice_name] = Image.fromarray(img_array, mode=\"RGB\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        coord = np.array([row['x'], row['y']], dtype=np.float32)\n",
    "        slice_name = row['slice_name']\n",
    "        image = self.images[slice_name]\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "        half_patch = self.patch_size // 2\n",
    "        left = max(x - half_patch, 0)\n",
    "        upper = max(y - half_patch, 0)\n",
    "        right = left + self.patch_size\n",
    "        lower = upper + self.patch_size\n",
    "        patch = image.crop((left, upper, right, lower))\n",
    "        patch = self.transform(patch)\n",
    "        target = self.embeddings[idx]\n",
    "        return torch.tensor(coord, dtype=torch.float32), patch, torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "class TestMappingWithImageH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    For testing: Maps (x, y) coordinates and the corresponding image patch \n",
    "    (loaded from the H5 file) to be used for prediction.\n",
    "    Expects a DataFrame with columns: \"x\", \"y\", and \"slice_name\".\n",
    "    \"\"\"\n",
    "    def __init__(self, df, h5_file_path, patch_size=64, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.images = {}\n",
    "        group = \"Test\"\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            for slice_name in self.df['slice_name'].unique():\n",
    "                img_array = np.array(f[f\"images/{group}\"][slice_name])\n",
    "                if img_array.dtype != np.uint8:\n",
    "                    img_array = img_array - img_array.min()\n",
    "                    if img_array.max() > 0:\n",
    "                        img_array = img_array / img_array.max()\n",
    "                    img_array = (img_array * 255).astype(np.uint8)\n",
    "                if img_array.ndim > 3:\n",
    "                    img_array = np.squeeze(img_array)\n",
    "                if img_array.ndim == 2:\n",
    "                    img_array = np.stack([img_array]*3, axis=-1)\n",
    "                if img_array.shape[-1] != 3:\n",
    "                    raise ValueError(f\"Unexpected number of channels in image for slice {slice_name}: {img_array.shape}\")\n",
    "                self.images[slice_name] = Image.fromarray(img_array, mode=\"RGB\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        coord = np.array([row['x'], row['y']], dtype=np.float32)\n",
    "        slice_name = row['slice_name']\n",
    "        image = self.images[slice_name]\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "        half_patch = self.patch_size // 2\n",
    "        left = max(x - half_patch, 0)\n",
    "        upper = max(y - half_patch, 0)\n",
    "        right = left + self.patch_size\n",
    "        lower = upper + self.patch_size\n",
    "        patch = image.crop((left, upper, right, lower))\n",
    "        patch = self.transform(patch)\n",
    "        return torch.tensor(coord, dtype=torch.float32), patch\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Define the Main Model that Uses Image Patches and Coordinates\n",
    "# --------------------------------------------------------------------\n",
    "class MainModelMappingWithImage(nn.Module):\n",
    "    def __init__(self, coord_input_dim=2, patch_channels=3, patch_size=64, embed_dim=16, hidden_dim=64):\n",
    "        super(MainModelMappingWithImage, self).__init__()\n",
    "        # CNN to encode image patches.\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(patch_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Calculate the image feature dimension.\n",
    "        img_feat_dim = 32 * (patch_size // 4) * (patch_size // 4)\n",
    "        # Fully connected layers to map concatenated [coords, image_features] to embedding.\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(coord_input_dim + img_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, coords, image_patches):\n",
    "        img_features = self.image_encoder(image_patches)\n",
    "        combined = torch.cat([coords, img_features], dim=1)\n",
    "        embedding = self.fc(combined)\n",
    "        return embedding\n",
    "\n",
    "def train_main_mapping_with_image(model, dataloader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for coords, patches, target_emb in dataloader:\n",
    "            coords = coords.to(device)\n",
    "            patches = patches.to(device)\n",
    "            target_emb = target_emb.to(device)\n",
    "            pred_emb = model(coords, patches)\n",
    "            loss = criterion(pred_emb, target_emb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * coords.size(0)\n",
    "        print(f\"Main Mapping With Image Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader.dataset):.4f}\")\n",
    "    return model\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6. Create Dataset and Train the Main Mapping with Image Model (Training)\n",
    "# --------------------------------------------------------------------\n",
    "patch_size = 64\n",
    "\n",
    "# Create the training dataset using images from the H5 file.\n",
    "main_image_dataset = MainMappingWithImageH5Dataset(\n",
    "    train_df, train_embeddings, h5_file_path, patch_size=patch_size, transform=transforms.ToTensor(), train=True\n",
    ")\n",
    "main_image_loader = DataLoader(main_image_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "main_model_img = MainModelMappingWithImage(coord_input_dim=2, patch_channels=3, patch_size=patch_size, embed_dim=16, hidden_dim=64)\n",
    "print(\"Training Main Mapping With Image Model...\")\n",
    "main_model_img = train_main_mapping_with_image(main_model_img, main_image_loader, num_epochs=20, lr=0.001, device=device)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 7. Inference on Test Data and Submission Creation\n",
    "# --------------------------------------------------------------------\n",
    "# Load test spots for slide \"S_7\" from the H5 file.\n",
    "with h5py.File(h5_file_path, \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_array = np.array(test_spots[\"S_7\"])\n",
    "    test_df = pd.DataFrame(test_array)\n",
    "# Test file has three columns: x, y, Test_set. Drop the third column.\n",
    "if test_df.shape[1] == 3:\n",
    "    test_df.columns = [\"x\", \"y\", \"Test_set\"]\n",
    "    test_df = test_df[[\"x\", \"y\"]]\n",
    "# Add slice_name column so we know which image to load.\n",
    "test_df[\"slice_name\"] = \"S_7\"\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "# Create the test dataset (loading images from H5).\n",
    "test_dataset = TestMappingWithImageH5Dataset(test_df, h5_file_path, patch_size=patch_size, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Predict foundation embeddings from test spots using the main mapping model with image.\n",
    "predicted_embeddings_list = []\n",
    "main_model_img.eval()\n",
    "with torch.no_grad():\n",
    "    for coords, patches in test_loader:\n",
    "        coords = coords.to(device)\n",
    "        patches = patches.to(device)\n",
    "        pred_emb = main_model_img(coords, patches)\n",
    "        predicted_embeddings_list.append(pred_emb)\n",
    "    predicted_embeddings = torch.cat(predicted_embeddings_list, dim=0)\n",
    "\n",
    "# Use the foundation decoder to convert predicted embeddings into cell abundance predictions.\n",
    "foundation_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_abundances = foundation_model.decoder(predicted_embeddings)\n",
    "    predicted_abundances = predicted_abundances.cpu().numpy()\n",
    "\n",
    "# Create submission DataFrame and save CSV.\n",
    "submission_df = pd.DataFrame(predicted_abundances, columns=cell_types)\n",
    "submission_df.insert(0, 'ID', test_df.index)\n",
    "submission_file = \"submission.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "print(f\"Submission file '{submission_file}' created!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11390004,
     "sourceId": 94147,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 385.883258,
   "end_time": "2025-03-28T18:27:31.424325",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-28T18:21:05.541067",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
