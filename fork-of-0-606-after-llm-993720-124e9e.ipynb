{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707fa9fd",
   "metadata": {
    "papermill": {
     "duration": 0.004553,
     "end_time": "2025-09-07T09:38:32.673908",
     "exception": false,
     "start_time": "2025-09-07T09:38:32.669355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## add\n",
    "- post_validate.py : remove fp\n",
    "- predict.py : use llb to predict primary or secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d657d9ca",
   "metadata": {
    "_cell_guid": "eae4b221-a822-451f-8f4b-134c3f9bfe2c",
    "_uuid": "b1883565-f717-4130-a662-5bb541f45ea1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:32.682101Z",
     "iopub.status.busy": "2025-09-07T09:38:32.681764Z",
     "iopub.status.idle": "2025-09-07T09:38:57.511784Z",
     "shell.execute_reply": "2025-09-07T09:38:57.510468Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 24.836823,
     "end_time": "2025-09-07T09:38:57.514371",
     "exception": false,
     "start_time": "2025-09-07T09:38:32.677548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 5.16s\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.18.0\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m157 packages\u001b[0m \u001b[2min 457ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m52 packages\u001b[0m \u001b[2min 17.66s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m14 packages\u001b[0m \u001b[2min 249ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m52 packages\u001b[0m \u001b[2min 297ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250622\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.9.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.7\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogits-processor-zoo\u001b[0m\u001b[2m==0.1.12\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.47b0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions-ai\u001b[0m\u001b[2m==0.4.9\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==24.0.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.7\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.8.5.post1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.18\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! uv pip uninstall --system 'tensorflow'\n",
    "! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'\n",
    "! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a15cfdd",
   "metadata": {
    "_cell_guid": "34135540-31fa-4d24-8934-acb1e0711a4f",
    "_uuid": "92f33014-02d3-41cb-b906-ffeb89a3f353",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.540749Z",
     "iopub.status.busy": "2025-09-07T09:38:57.540454Z",
     "iopub.status.idle": "2025-09-07T09:38:57.548261Z",
     "shell.execute_reply": "2025-09-07T09:38:57.547449Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021946,
     "end_time": "2025-09-07T09:38:57.549545",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.527599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/helpers.py\n",
    "import logging, os, kagglehub, inspect\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "IS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\n",
    "IS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "COMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\n",
    "PDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\n",
    "WORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n",
    "\n",
    "DOI_LINK = 'https://doi.org/'\n",
    "\n",
    "DEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\n",
    "LOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\n",
    "LOG_DIR = Path(LOG_FILE_PATH).parent\n",
    "\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\n",
    "LOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def get_logger(name=None):\n",
    "    if name is None:\n",
    "        frame = inspect.currentframe()\n",
    "        if frame is None or frame.f_back is None:\n",
    "            name = \"__main__\"\n",
    "        else:\n",
    "            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        ch.setFormatter(formatter)\n",
    "        fh = logging.FileHandler(LOG_FILE_PATH)\n",
    "        fh.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "        logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def is_doi_link(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.starts_with(DOI_LINK).and_(\n",
    "        ~pl.col(name).str.contains(r\"/dl\\.\")\n",
    "    )\n",
    "\n",
    "def string_normalization(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n",
    "\n",
    "def get_df(parse_dir: str):\n",
    "    records = []\n",
    "    txt_files = list(Path(parse_dir).glob('*.txt'))\n",
    "    for txt_file in txt_files:\n",
    "        id_ = txt_file.stem\n",
    "        with open(txt_file, 'r') as f:\n",
    "            text = f.read()\n",
    "        records.append({'article_id': id_, 'text': text})\n",
    "    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n",
    "\n",
    "def assume_type(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n",
    "    )\n",
    "\n",
    "def score(df, gt, on, tag='all'):\n",
    "    hits = gt.join(df, on=on)\n",
    "    tp = hits.height\n",
    "    fp = df.height - tp\n",
    "    fn = gt.height - tp\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n",
    "\n",
    "def evaluate(df, on=['article_id', 'dataset_id']):\n",
    "    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n",
    "    return (\n",
    "        score(df, gt, on),\n",
    "        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n",
    "        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449c3a7d",
   "metadata": {
    "_cell_guid": "98be0899-3cad-423b-a3db-313209068df0",
    "_uuid": "84859532-6acd-4011-b783-d0d24257a19b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.580148Z",
     "iopub.status.busy": "2025-09-07T09:38:57.579877Z",
     "iopub.status.idle": "2025-09-07T09:38:57.586886Z",
     "shell.execute_reply": "2025-09-07T09:38:57.585673Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022593,
     "end_time": "2025-09-07T09:38:57.588721",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.566128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/parse.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from helpers import get_logger, PDF_DIR\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def pdf_to_txt(output_dir: Path):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_files = list(PDF_DIR.glob(\"*.pdf\")) + list(PDF_DIR.glob(\"*.PDF\"))\n",
    "    existing_txt_files = {f.stem for f in output_dir.glob(\"*.txt\")}\n",
    "    for pdf_file in pdf_files:\n",
    "        txt_file = output_dir / f\"{pdf_file.stem}.txt\"\n",
    "        if pdf_file.stem in existing_txt_files:\n",
    "            continue\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pymupdf.open(pdf_file) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            txt_file.write_text(text, encoding='utf-8')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('output_dir', type=Path, help='Directory to save text files')\n",
    "    args = parser.parse_args()\n",
    "    pdf_to_txt(args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcddc09",
   "metadata": {
    "_cell_guid": "01632e8b-2a68-4dec-9606-f91214a8c020",
    "_uuid": "5210e49f-e5ab-45c6-b673-f0bd08dc1877",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.624510Z",
     "iopub.status.busy": "2025-09-07T09:38:57.624215Z",
     "iopub.status.idle": "2025-09-07T09:38:57.629791Z",
     "shell.execute_reply": "2025-09-07T09:38:57.629088Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030461,
     "end_time": "2025-09-07T09:38:57.631573",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.601112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/check_parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/check_parse.py\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from helpers import *\n",
    "\n",
    "l=get_logger()\n",
    "\n",
    "def gt_dataset_id_normalization(name:str) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(is_doi_link(name))\n",
    "        .then(pl.col(name).str.split(DOI_LINK).list.last())\n",
    "        .otherwise(name)\n",
    "        .str.to_lowercase()\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    if IS_KAGGLE_SUBMISSION:\n",
    "        l.debug('skipping check_parse for submission')\n",
    "        return\n",
    "    df = (\n",
    "        get_df('/tmp/train_parse')\n",
    "        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n",
    "    )\n",
    "\n",
    "    gt = (\n",
    "        pl.read_csv(COMP_DIR/'train_labels.csv')\n",
    "        .filter(pl.col('article_id').is_in(df['article_id']))\n",
    "        .filter(pl.col('type')!='Missing')\n",
    "        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n",
    "    )\n",
    "\n",
    "    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6499bbe",
   "metadata": {
    "_cell_guid": "83084a3e-ab3e-4c24-9045-7bc81df72e34",
    "_uuid": "5a79e391-1a5c-4264-9aa5-747cb657a266",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.668762Z",
     "iopub.status.busy": "2025-09-07T09:38:57.668378Z",
     "iopub.status.idle": "2025-09-07T09:38:57.683579Z",
     "shell.execute_reply": "2025-09-07T09:38:57.682539Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.035807,
     "end_time": "2025-09-07T09:38:57.684916",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.649109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/getid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/getid.py\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "COMPILED_PATTERNS = {\n",
    "    'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],    \n",
    "    'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n",
    "    'first_citation_patterns': [\n",
    "        re.compile(r'^\\s*\\[1\\]\\s*'),\n",
    "        re.compile(r'^\\s*\\(1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1\\.\\s*'),\n",
    "        re.compile(r'^\\s*1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1(?=\\s|$)'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n",
    "    last_match_idx = None\n",
    "    for pattern in header_patterns:\n",
    "        matches = list(pattern.finditer(text))\n",
    "        if matches:\n",
    "            last_match_idx = matches[-1].start()\n",
    "    return last_match_idx\n",
    "\n",
    "def find_last_first_citation(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_match_line = None\n",
    "    for line_num, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n",
    "            if pattern.match(line):\n",
    "                next_lines = lines[line_num:line_num+3]\n",
    "                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n",
    "                    last_match_line = line_num\n",
    "                break\n",
    "    return last_match_line\n",
    "\n",
    "def find_reference_start(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_first_citation = find_last_first_citation(text)\n",
    "    if last_first_citation is not None:\n",
    "        return last_first_citation\n",
    "    start_search_idx = int(len(lines) * 0.5)\n",
    "    for i in range(start_search_idx, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if COMPILED_PATTERNS['citation_pattern'].match(line):\n",
    "            next_lines = lines[i:i+3]\n",
    "            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n",
    "                for j in range(i, max(-1, i-10), -1):\n",
    "                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()):\n",
    "                        return j + 1\n",
    "                return max(0, i-10)\n",
    "    return None\n",
    "\n",
    "def split_text_and_references(text: str) -> Tuple[str, str]:\n",
    "    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n",
    "    if header_idx is not None:\n",
    "        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "        if header_idx2 is not None:\n",
    "            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "            if header_idx3 is not None:\n",
    "                return text[:header_idx3].strip(), text[header_idx3:].strip()\n",
    "            return text[:header_idx2].strip(), text[header_idx2:].strip()\n",
    "        return text[:header_idx].strip(), text[header_idx:].strip()\n",
    "    ref_start_line = find_reference_start(text)\n",
    "    if ref_start_line is not None:\n",
    "        lines = text.splitlines()\n",
    "        body = '\\n'.join(lines[:ref_start_line])\n",
    "        refs = '\\n'.join(lines[ref_start_line:])\n",
    "        return body.strip(), refs.strip()\n",
    "    return text.strip(), ''\n",
    "\n",
    "def get_splits(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    bodies, refs = [], []\n",
    "    for raw_text in df['text']:\n",
    "        main, ref = split_text_and_references(raw_text)\n",
    "        bodies.append(main)\n",
    "        refs.append(ref)\n",
    "    return df.with_columns(pl.Series('body', bodies), pl.Series('ref', refs))\n",
    "\n",
    "def doi_gbif_ids(df):\n",
    "    doi_pattern = re.compile(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+')\n",
    "    DOI_LINK=\"https://doi.org/\"\n",
    "    records = []\n",
    "    for article_id, ref_text in df.select([\"article_id\",\"ref\"]).rows():\n",
    "        idx = ref_text.find(\"GBIF Occurrence\")\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        context_window = []\n",
    "        context_len = 110\n",
    "        rem_ref = ref_text[idx:]\n",
    "        while idx != -1 :\n",
    "            if len(rem_ref) < context_len:\n",
    "                context_window.append(rem_ref)\n",
    "                doi_matches = doi_pattern.findall(rem_ref)\n",
    "            else:\n",
    "                context_window.append(rem_ref[:context_len])\n",
    "                doi_matches = doi_pattern.findall(rem_ref[:context_len])\n",
    "    \n",
    "            for match in doi_matches:\n",
    "                cleaned = re.sub(r'\\s+', '', match)         \n",
    "                cleaned = re.sub(r'[^A-Za-z0-9]+$', '', cleaned)  \n",
    "                cleaned = cleaned.lower()\n",
    "                dataset_id = DOI_LINK + cleaned\n",
    "                records.append((article_id, dataset_id, match))\n",
    "            rem_ref = rem_ref[1:]\n",
    "            idx = rem_ref.find(\"GBIF Occurrence\")\n",
    "            if idx != -1 :\n",
    "                rem_ref = rem_ref[idx:]\n",
    "    doi_df = pl.DataFrame(records, schema=[\"article_id\", \"dataset_id\", \"match\"], orient=\"row\").unique(subset=[\"article_id\", \"dataset_id\"])\n",
    "    doi_df= doi_df.with_columns(\n",
    "        pl.col(\"match\").map_elements(lambda x: [x] if x is not None else [], return_dtype=pl.List(pl.Utf8))\n",
    "    )\n",
    "    return doi_df\n",
    "\n",
    "def tidy_extraction(df) -> pl.DataFrame:\n",
    "    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n",
    "\n",
    "    doi_df = (\n",
    "        df.with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match'))\n",
    "          .explode('match')\n",
    "          .drop_nulls('match')\n",
    "          .with_columns(\n",
    "              pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                             .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                             .str.to_lowercase()\n",
    "                             .alias('dataset_id')\n",
    "          )\n",
    "          .group_by('article_id', 'dataset_id')\n",
    "          .agg('match')\n",
    "          .with_columns((DOI_LINK + pl.col('dataset_id')).alias('dataset_id'))\n",
    "    )\n",
    "\n",
    "    extra_doi = doi_gbif_ids(df)\n",
    "    doi_df = pl.concat([doi_df,extra_doi])\n",
    "\n",
    "    ref_doi = (\n",
    "        df.with_columns(pl.col('ref').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match'))\n",
    "          .explode('match')\n",
    "          .drop_nulls('match')\n",
    "          .with_columns(\n",
    "              pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                             .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                             .str.to_lowercase()\n",
    "                             .alias('dataset_id')\n",
    "          )\n",
    "          .group_by('article_id', 'dataset_id')\n",
    "          .agg('match')\n",
    "          .with_columns((DOI_LINK + pl.col('dataset_id')).alias('dataset_id'))\n",
    "    )\n",
    "\n",
    "    doi_df = pl.concat([doi_df,ref_doi])\n",
    "\n",
    "    # REGEX_IDS = (\n",
    "    #     r\"(?i)\\b(?:\"\n",
    "    #     r\"CHEMBL\\d+|\"\n",
    "    #     r\"E-GEOD-\\d+|E-PROT-\\d+|E-MTAB-\\d+|E-MEXP-\\d+|EMPIAR-\\d+|\"\n",
    "    #     r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n",
    "    #     r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n",
    "    #     r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|BX\\d{6}|KX\\d{6}|K0\\d{4}|CAB\\d{6}|\"\n",
    "    #     r\"NC_\\d{6}\\.\\d{1}|NM_\\d{9}|\"\n",
    "    #     r\"PRJNA\\d+|PRJEB\\d+|PRJDB\\d+|PXD\\d+|SAMN\\d+|\"\n",
    "    #     r\"GSE\\d+|GSM\\d+|GPL\\d+|\"\n",
    "    #     r\"PDB\\s?[1-9][A-Z0-9]{3}|HMDB\\d+|\"\n",
    "    #     r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n",
    "    #     r\"(?:SR[PRX]|STH|ERR|DRR|DRX|DRP|ERP|ERX)\\d+\"\n",
    "    #     r\")\"\n",
    "    # )  \n",
    "\n",
    "    REGEX_IDS = (\n",
    "        r\"(?i)\\b(?:\"\n",
    "        r\"CHEMBL\\d+|\"\n",
    "        r\"E-GEOD-\\d+|E-PROT-\\d+|E-MTAB-\\d+|E-MEXP-\\d+|EMPIAR-\\d+|\"\n",
    "        r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n",
    "        r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n",
    "        r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|BX\\d{6}|KX\\d{6}|K0\\d{4}|CAB\\d{6}|\"\n",
    "        r\"NC_\\d{6}\\.\\d{1}|NM_\\d{9}|\"\n",
    "        r\"PRJNA\\d+|PRJEB\\d+|PRJDB\\d+|PXD\\d+|SAMN\\d+|\"\n",
    "        r\"GSE\\d+|GSM\\d+|GPL\\d+|\"\n",
    "        r\"PDB\\s?[1-9][A-Z0-9]{3}|HMDB\\d+|\"\n",
    "        r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n",
    "        r\"(?:SR[PRX]|STH|ERR|DRR|DRX|DRP|ERP|ERX)\\d+|\"\n",
    "        r\"CVCL_[A-Z0-9]{4}|\"\n",
    "        r\"[1-5]\\.(?:10|20|30|40|50|60|70|80|90)\\.\\d{2,4}\\.\\d{2,4}\"\n",
    "        r\")\"\n",
    "    )\n",
    "\n",
    "    # REGEX_IDS = (\n",
    "    #     r\"(?i)\\b(?:\"\n",
    "    #     r\"CHEMBL\\d+|\"\n",
    "    #     r\"E-GEOD-\\d+|E-PROT-\\d+|EMPIAR-\\d+|\"\n",
    "    #     r\"ENSBTAG\\d+|ENSOARG\\d+|ENSMMUT\\d+|\"\n",
    "    #     r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n",
    "    #     r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|KX\\d{6}|K0\\d{4}|\"\n",
    "    #     r\"PRJNA\\d+|PRJEB\\d+|PXD\\d+|SAMN\\d+|\"\n",
    "    #     r\"GSE\\d+|GSM\\d+|GPL\\d+|\"\n",
    "    #     r\"E-MTAB-\\d+|E-MEXP-\\d+|\"\n",
    "    #     r\"PDB\\s?[1-9][A-Z0-9]{3}|HMDB\\d+|\"\n",
    "    #     r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n",
    "    #     r\"(?:SRR|SRX|SRP|ERR|DRR|DRX|DRP|ERP|ERX)\\d+|\"\n",
    "    #     r\"(?:[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9]{5}|CAB\\d{6})|\"\n",
    "    #     r\"CVCL_[A-Z0-9]{4}|\"\n",
    "    #     r\"\\d+\\.\\d+\\.\\d+\\.\\d+|\"              # SCOP classification\n",
    "    #     r\"NM_\\d+|\"                           # RefSeq mRNA\n",
    "    #     r\"[A-Z]{2}\\d{6}|\"                    # GenBank 2-letter\n",
    "    #     r\"STH\\d{5}\"                          # STH IDs\n",
    "    #     r\")\"\n",
    "    # )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    acc_df = (\n",
    "        df.with_columns(\n",
    "            pl.col('text').str.extract_all(REGEX_IDS).alias('match')\n",
    "        )\n",
    "        .explode('match')\n",
    "        .drop_nulls('match')\n",
    "        .with_columns(\n",
    "            pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                           .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                           .str.replace(r'(?i)^PDB', '')\n",
    "                           .alias('dataset_id')\n",
    "        )\n",
    "        .group_by('article_id', 'dataset_id')\n",
    "        .agg('match')\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('dryad.'))\n",
    "              .then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('pasta/'))\n",
    "              .then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = pl.concat([doi_df, acc_df])\n",
    "\n",
    "    df = (\n",
    "        df.unique(['article_id', 'dataset_id'])  # CHANGED\n",
    "          .filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains('figshare', literal=True))\n",
    "          .filter(~pl.col('dataset_id').is_in(bad_ids))\n",
    "          .filter(\n",
    "              pl.when(is_doi_link('dataset_id') &\n",
    "                      (pl.col('dataset_id').str.split('/').list.last().str.len_chars() < 5))\n",
    "               .then(False)\n",
    "               .otherwise(True)\n",
    "          )\n",
    "          .with_columns(pl.col('match').list.unique())\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 100) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        raise ValueError\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end]\n",
    "\n",
    "def get_window_df(text_df, ids_df):\n",
    "    df = ids_df.join(text_df, on='article_id')\n",
    "    windows = []\n",
    "    for text, match_ids in df.select('text', 'match').rows():\n",
    "        windows.append(get_context_window(text, match_ids[0]))\n",
    "    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n",
    "\n",
    "def write_the_match(text_df,id_df):\n",
    "    df = id_df.join(text_df,on=['article_id'])\n",
    "    records=[]\n",
    "    for art_id,dataset_id,match_ids,text in df.select('article_id','dataset_id','match','text').rows():\n",
    "        records.append({'article_id':art_id,'dataset_id':dataset_id,'match':match_ids[0],'text':text})\n",
    "\n",
    "    pl.DataFrame(records).write_parquet('/tmp/context_data.parquet')\n",
    "\n",
    "def main():\n",
    "    text_df = get_df('/tmp/train_parse')\n",
    "    df = get_splits(text_df)\n",
    "    df = tidy_extraction(df)\n",
    "        \n",
    "    write_the_match(text_df,df)\n",
    "    \n",
    "    df = get_window_df(text_df, df)\n",
    "    df.write_parquet('/tmp/extracted.parquet')\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r)\n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0988fe4a",
   "metadata": {
    "_cell_guid": "af30506a-e45e-4a3b-ba81-23dd154bde8d",
    "_uuid": "94fc4779-0b43-4058-ac64-f54a7ebd8a76",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.711090Z",
     "iopub.status.busy": "2025-09-07T09:38:57.710711Z",
     "iopub.status.idle": "2025-09-07T09:38:57.719308Z",
     "shell.execute_reply": "2025-09-07T09:38:57.718393Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023023,
     "end_time": "2025-09-07T09:38:57.720521",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.697498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/llm_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/llm_validate.py\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n",
    "1. Priority Rules (highest → lowest)\n",
    "1.1 Always classify as A (Data) if:\n",
    "DOI prefix matches a known data repository:\n",
    "\n",
    "Dryad: 10.5061\n",
    "\n",
    "Zenodo: 10.5281\n",
    "\n",
    "Dl: 10.15468\n",
    "\n",
    "ICPSR: 10.3886\n",
    "\n",
    "USGS data: 10.5066\n",
    "\n",
    "Mendeley Data: 10.17632\n",
    "\n",
    "Dataverse: 10.7910/DVN\n",
    "\n",
    "OpenNeuro: 10.18112/openneuro.\n",
    "\n",
    "PANGAEA: 10.1594/PANGAEA.\n",
    "\n",
    "\n",
    "2. Classify as B (Literature) if:\n",
    "DOI prefix belongs to a publisher (e.g., 10.1038, 10.1007, 10.1126, 10.1016, 10.1101, 10.1021, 10.1145, 10.1177, 10.1093, 10.1080, 10.1111, etc.).\n",
    "\n",
    "Context indicates a journal article, book, conference paper, preprint, protocol, or method paper, without any repository/data storage signal.\n",
    "\n",
    "Mentions only “supplementary material” or “supplementary information” without a repository.\n",
    "\n",
    "3. Ambiguous cases\n",
    "No repository prefix and no clear context → default to B.\n",
    "\n",
    "\n",
    "4. Output\n",
    "Only output:\n",
    "\n",
    "A → data repository / dataset\n",
    "\n",
    "B → literature / non-data resource\n",
    "\n",
    "Few-shot examples\n",
    "\n",
    "“Raw images are stored on Figshare (DOI 10.6084/m9.figshare.1234567).” → A\n",
    "\n",
    "“Sequence reads available under BioProject accession PRJNA765432.” → A\n",
    "\n",
    "“As described in Nature Methods (DOI 10.1038/s41592-020-0793-2).” → B\n",
    "\n",
    "“See Supplementary Data at Zenodo (10.5281/zenodo.987654).” → A\n",
    "\n",
    "“Method details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.” → B\n",
    "\n",
    "“Data uploaded to Dryad (10.5061/dryad.x1y2z3).” → A\n",
    "\n",
    "“Referenced paper: DOI 10.1101/2020.01.01.123456 (bioRxiv preprint).” → B\n",
    "\n",
    "“Metabolomics data in MetaboLights MTBLS1234.” → A\n",
    "\n",
    "“The MRI scans are deposited at OpenNeuro (DOI 10.18112/openneuro.ds000001.v1.0.0).” → A\n",
    "\n",
    "“Protein structure described in Science (DOI 10.1126/science.abc1234).” → B\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_df():\n",
    "    df = pl.read_parquet('/tmp/extracted.parquet')\n",
    "    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n",
    "    return df.filter(is_doi_link('dataset_id'))\n",
    "\n",
    "def build_prompt(tokenizer, df):\n",
    "    prompts = []\n",
    "    for doi, text in df.select('dataset_id', 'window').rows():\n",
    "        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI}, {'role':'user', 'content': text}]\n",
    "        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False))\n",
    "    return df.with_columns(pl.Series('prompt', prompts))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    llm = vllm.LLM(model_path, quantization='awq', tensor_parallel_size=2, gpu_memory_utilization=0.9, trust_remote_code=True, dtype=\"half\", enforce_eager=True, max_model_len=2048, disable_log_stats=True, disable_custom_all_reduce=True, enable_prefix_caching=True, task='generate')\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    df = build_df()\n",
    "    df = build_prompt(tokenizer, df)\n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.2, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        del llm, tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    import gc, torch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd809a64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.742993Z",
     "iopub.status.busy": "2025-09-07T09:38:57.742731Z",
     "iopub.status.idle": "2025-09-07T09:38:57.748836Z",
     "shell.execute_reply": "2025-09-07T09:38:57.748005Z"
    },
    "papermill": {
     "duration": 0.020889,
     "end_time": "2025-09-07T09:38:57.750574",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.729685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/post_filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/post_filter.py\n",
    "import polars as pl\n",
    "from helpers import *\n",
    "\n",
    "\"\"\"\n",
    "Fourth essence: Post-filter to cut FP DOIs that look like literature.\n",
    "- Read /kaggle/working/submission.csv (output of llm_validate.py)\n",
    "- Join with /tmp/extracted.parquet to get context window\n",
    "- Drop DOI rows that (1) start with typical publisher prefixes AND (2) have no data-ish words nearby\n",
    "- Keep accessions untouched\n",
    "\"\"\"\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "PAPER_PREFIXES = [\n",
    "    \"10.5061\",\"10.5281\",\"10.17632\",\"10.1594\",\"10.15468\",\"10.17882\",\"10.7937\",\"10.7910\",\"10.6073\",\n",
    "    \"10.3886\",\"10.3334\",\"10.4121\",\"10.5066\",\"10.5067\",\"10.18150\",\"10.25377\",\"10.25387\",\"10.23642\",\"10.24381\",\"10.22033\"\n",
    "]\n",
    "\n",
    "CONTEXT_RE = r\"(?i)\\b(data(?:set)?|repository|archive|deposited|available|supplementary|raw(?:\\s+data)?|uploaded|hosted|stored|accession)\\b\"\n",
    "\n",
    "def is_paper_prefix(col: str = \"dataset_id\") -> pl.Expr:\n",
    "    expr = pl.lit(False)\n",
    "    for p in PAPER_PREFIXES:\n",
    "        expr = expr | pl.col(col).str.starts_with(f\"{DOI_LINK}{p}\")\n",
    "    return expr\n",
    "\n",
    "def main():\n",
    "    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "    # Normalize columns: drop row_id if present so concat widths match\n",
    "    if \"row_id\" in sub.columns:\n",
    "        sub = sub.drop(\"row_id\")\n",
    "\n",
    "    # Context windows\n",
    "    win = pl.read_parquet(\"/tmp/extracted.parquet\").select(\"article_id\", \"dataset_id\", \"window\")\n",
    "\n",
    "    # DOI & ACC split\n",
    "    doi_rows = sub.filter(is_doi_link(\"dataset_id\")).join(win, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "    acc_rows = sub.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "    keep_mask = (\n",
    "        (~is_paper_prefix(\"dataset_id\"))  # not a known paper prefix\n",
    "        | doi_rows[\"window\"].fill_null(\"\").str.contains(CONTEXT_RE)\n",
    "    )\n",
    "\n",
    "    kept_doi = doi_rows.filter(keep_mask).select(\"article_id\", \"dataset_id\", \"type\")\n",
    "    final = pl.concat([kept_doi, acc_rows.select(\"article_id\", \"dataset_id\", \"type\")])\n",
    "\n",
    "    # Re-eval & save\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        for r in evaluate(final): l.info(r)\n",
    "        for r in evaluate(final, on=[\"article_id\", \"dataset_id\", \"type\"]): l.info(r)\n",
    "\n",
    "    final.with_row_index(\"row_id\").write_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e8389b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.778163Z",
     "iopub.status.busy": "2025-09-07T09:38:57.777833Z",
     "iopub.status.idle": "2025-09-07T09:38:57.787227Z",
     "shell.execute_reply": "2025-09-07T09:38:57.786395Z"
    },
    "papermill": {
     "duration": 0.023323,
     "end_time": "2025-09-07T09:38:57.788626",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.765303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/post_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/post_validate.py\n",
    "\n",
    "from helpers import *\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "\n",
    "PROMPT_CLASSIFY_CITATION_TYPE = '''\n",
    "# Role & Task\n",
    "You are an expert data citation analyst. Your task is to classify a given citation from a scientific paper into one of two categories: **A** (Data) or **B** (Not Data). Base your decision strictly on the provided abstract and the context of the citation.\n",
    "\n",
    "## Instructions\n",
    "1.  **Read the provided abstract** to understand the research context.\n",
    "2.  **Analyze the citation context** for key linguistic cues.\n",
    "3.  **Classify the citation** as either **A** or **B** based on the definitions below.\n",
    "4.  **Output only a single letter: A or B.** Do not output any other text, explanation, or formatting.\n",
    "\n",
    "## Category Definitions\n",
    "\n",
    "### **Category A: DATA**\n",
    "The citation points to a dataset. This includes:\n",
    "*   **Primary Data:** Raw or processed data that the current study's authors collected, generated, or created.\n",
    "*   **Secondary Data:** Data that was originally produced by other researchers but is being *used as a dataset* in the current study.\n",
    "*   **Key Phrases:** \"data are available at\", \"we collected\", \"we measured\", \"data were obtained from\", \"dataset\", \"downloaded from\", \"deposited in\", repository names (e.g., GenBank, Zenodo, Figshare, TCIA).\n",
    "\n",
    "### **Category B: NOT DATA**\n",
    "The citation points to a traditional scholarly publication or other non-data resource. This includes:\n",
    "*   Journal articles, books, conference proceedings, preprints, protocols, methods papers.\n",
    "*   **Key Phrases:** \"as described in\", \"according to\", \"previous study\", \"et al.\", \"paper\", \"article\", \"methodology\", \"was used for analysis\" (without indicating data access).\n",
    "*   Citations that provide background context or methodological description but do not serve as the source of the data used in the analysis.\n",
    "\n",
    "## Input Format\n",
    "You will be provided with the following three pieces of information:\n",
    "Paper Abstract: {abstract}\n",
    "Citation: {dataset_id}\n",
    "Citation Context: {context}\n",
    "\n",
    "## Critical Thinking Guidelines\n",
    "*   A DOI or URL can point to either data (A) or a paper (B). The context determines the classification.\n",
    "*   If the citation is used to describe the *source* of the data for the current study's analysis, it is likely **A**.\n",
    "*   If the citation is used to provide background, justify a method, or compare results, it is likely **B** (a reference to another paper).\n",
    "*   When in doubt, rely on the linguistic cues in the \"Citation Context\".\n",
    "\n",
    "## Examples for Pattern Recognition\n",
    "\n",
    "**Example 1 (Classify as A):**\n",
    "*   Context: \"Three out of four cohorts used in this study can be found on The Cancer Imaging Archive (TCIA)24: Canadian benchmark dataset23: https://doi.org/10.7937/K9/TCIA.2017.8oje5q00.\"\n",
    "*   **Reasoning:** The text states cohorts are \"used in this study\" and provides direct repository links. This is a clear case of citing external data for use.\n",
    "*   **Output:** A\n",
    "\n",
    "**Example 2 (Classify as B):**\n",
    "*   Context: \"data presented here are available at the SEANOE dataportal: https://doi.org/10.17882/94052 (ZooScan dataset Grandremy et al. 2023c)\"\n",
    "*   **Reasoning:** The phrase \"data presented here\" indicates this is the authors' own data being deposited, not a citation to an external source they are using. The \"(Author et al. Year)\" format is a classic literature citation style.\n",
    "*   **Output:** B\n",
    "\n",
    "**Example 3 (Classify as A):**\n",
    "*   Context: \"GBIF occurrence data: Vulpes vulpes: https://doi.org/10.15468/dl.wgtneb (28 May 2021).\"\n",
    "*   **Reasoning:** Explicitly names the data source (GBIF) and provides a direct access link/DOI for the specific dataset used.\n",
    "*   **Output:** A\n",
    "\n",
    "**Example 4 (Classify as B):**\n",
    "*   Context: \"North American soil NCBI SRA SRP035367 Smith & Peay [36] ITS2-Soil\"\n",
    "*   **Reasoning:** While it mentions a data repository ID (SRP035367), it couples it with a standard literature citation \"[36]\". The context suggests it is referencing the *paper* by Smith & Peay that describes the data, not directly citing the dataset itself for use.\n",
    "*   **Output:** B\n",
    "\n",
    "## Ready for Input\n",
    "Begin your analysis. Remember: Output only **A** or **B**.\n",
    "'''\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 600) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        return \"no context\", \"no abstraction\"\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end] , text[:1000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_context_win(tokenizer,df):\n",
    "    text_df = pl.read_parquet('/tmp/context_data.parquet')\n",
    "    # print(text_df)\n",
    "    df = df.join(text_df, on=[\"article_id\",\"dataset_id\"], how=\"inner\")\n",
    "    df = df.drop(\"type\")\n",
    "    print(df)\n",
    "\n",
    "    prompts = []\n",
    "    \n",
    "    for article_id,dataset_id,text,match in df.select([\"article_id\",\"dataset_id\",\"text\",'match']).rows():\n",
    "\n",
    "        context, abstract = get_context_window(text,match)\n",
    "        user_content = f\"\"\"\n",
    "        Paper Abstract: {abstract}\n",
    "        \n",
    "        Citation: {dataset_id}\n",
    "\n",
    "        \n",
    "        Citation Context: {context}\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": PROMPT_CLASSIFY_CITATION_TYPE},\n",
    "            {\"role\": \"user\", \"content\": user_content.strip()}\n",
    "        ]\n",
    "        prompts.append(\n",
    "            tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        )\n",
    "        \n",
    "    return df.with_columns(pl.Series(\"prompt\", prompts))\n",
    "\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        MODEL_PATH,\n",
    "        quantization='awq',\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.9,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=16384,\n",
    "        disable_log_stats=True, \n",
    "        disable_custom_all_reduce=True,\n",
    "        enable_prefix_caching=True,\n",
    "        task='generate')\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "\n",
    "    df=pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "    \n",
    "    if \"row_id\" in df.columns:\n",
    "        df = df.drop(\"row_id\")\n",
    "\n",
    "    # print(df)\n",
    "\n",
    "    doi_df = df.filter(is_doi_link(\"dataset_id\"))\n",
    "    acc_df = df.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "    # print(doi_df)\n",
    "\n",
    "    df = find_context_win(tokenizer,doi_df)\n",
    "\n",
    "    \n",
    "    \n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\",\"C\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.7, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    # print(df)\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        del llm, tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    import gc, torch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64237903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.813772Z",
     "iopub.status.busy": "2025-09-07T09:38:57.813458Z",
     "iopub.status.idle": "2025-09-07T09:38:57.828939Z",
     "shell.execute_reply": "2025-09-07T09:38:57.827987Z"
    },
    "papermill": {
     "duration": 0.029774,
     "end_time": "2025-09-07T09:38:57.830316",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.800542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/predict.py\n",
    "\n",
    "from helpers import *\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "\n",
    "PROMPT_CLASSIFY_CITATION_TYPE = '''\n",
    "# Role & Task\n",
    "You are an expert data citation analyst. Your task is to classify a given citation from a scientific paper into one of two categories based on the context: **A (Primary Data)** or **B (Secondary Data)**.\n",
    "\n",
    "## Instructions\n",
    "1.  **Read the provided abstract** to understand the research context.\n",
    "2.  **Analyze the citation context** for key linguistic cues.\n",
    "3.  **Classify the citation** as either **A** or **B** based on the definitions below.\n",
    "4.  **Output only a single letter: A or B.** Do not output any other text, explanation, or formatting.\n",
    "\n",
    "## Category Definitions\n",
    "\n",
    "### **Category A: PRIMARY DATA**\n",
    "The data was generated, collected, or created by the **authors of the current study**. This is *their* data.\n",
    "*   **Key Phrases:** \"we collected\", \"we generated\", \"our data\", \"data are available at [URL/DOI]\", \"data have been deposited\", \"this study presents\", \"supplementary data\".\n",
    "\n",
    "### **Category B: SECONDARY DATA**\n",
    "The data was produced by **other researchers** or external sources and is being reused or analyzed by the current study's authors.\n",
    "*   **Key Phrases:** \"data were obtained from\", \"publicly available data\", \"previously published data\", \"retrieved from\", \"downloaded from\", \"[Dataset Name] dataset\", \"database\", citing a specific external source.\n",
    "\n",
    "## Input Format\n",
    "You will be provided with the following three pieces of information:\n",
    "Paper Abstract: {abstract}\n",
    "Citation: {dataset_id}\n",
    "Citation Context: {context}\n",
    "\n",
    "\n",
    "## Decision Framework\n",
    "Answer these questions based on the **Citation Context**:\n",
    "\n",
    "1.  **Who is the source of the data?**\n",
    "    *   If the context implies the **authors themselves** are the source (e.g., \"we,\" \"our\"), classify as **A**.\n",
    "    *   If the context names an **external source** (e.g., a repository, another study, a database), classify as **B**.\n",
    "\n",
    "2.  **What is the action being described?**\n",
    "    *   **A (Primary)** actions: *depositing, making available, presenting* their own data.\n",
    "    *   **B (Secondary)** actions: *using, obtaining, accessing, downloading, analyzing* existing data from elsewhere.\n",
    "\n",
    "## Examples for Pattern Recognition\n",
    "\n",
    "**Example 1 (Classify as B):**\n",
    "*   Context: \"Three out of four cohorts **used in this study** can be found on The Cancer Imaging Archive (TCIA)24: Canadian benchmark dataset23: https://doi.org/10.7937/K9/TCIA.2017.8oje5q00.\"\n",
    "*   **Reasoning:** The authors are describing external datasets they **used** (a Secondary action). The source is TCIA, not themselves.\n",
    "*   **Output:** B\n",
    "\n",
    "**Example 2 (Classify as A):**\n",
    "*   Context: \"Additional research data **supporting this publication are available** at 10.25377/sussex.21184705.\"\n",
    "*   **Reasoning:** The authors are stating the availability of data that **supports their own publication**. The source is implied to be themselves.\n",
    "*   **Output:** A\n",
    "\n",
    "**Example 3 (Classify as B):**\n",
    "*   Context: \"GBIF occurrence data: Vulpes vulpes: https://doi.org/10.15468/dl.wgtneb (28 May 2021).\"\n",
    "*   **Reasoning:** The data is explicitly sourced from an external repository (GBIF). The authors are referring to data they reused.\n",
    "*   **Output:** B\n",
    "\n",
    "**Example 4 (Classify as A):**\n",
    "*   Context: \"Data referring to Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE.\"\n",
    "*   **Reasoning:** This is a tricky case. The citation format \"(Author et al. Year)\" suggests a literature reference. However, the phrase \"Data referring to\" and the direct data DOI indicate the authors are citing **their own previously published dataset** (from a 2017 paper) that is now available. This is their Primary data.\n",
    "*   **Output:** A\n",
    "\n",
    "## Ready for Input\n",
    "Begin your analysis. Remember: Output only **A** or **B**.\n",
    "\n",
    "'''\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 600) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        return \"no context\", \"no abstraction\"\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end] , text[:1000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_context_win(tokenizer,df):\n",
    "    text_df = pl.read_parquet('/tmp/context_data.parquet')\n",
    "    # print(text_df)\n",
    "    df = df.join(text_df, on=[\"article_id\",\"dataset_id\"], how=\"inner\")\n",
    "    df = df.drop(\"type\")\n",
    "    print(df)\n",
    "\n",
    "    prompts = []\n",
    "    \n",
    "    for article_id,dataset_id,text,match in df.select([\"article_id\",\"dataset_id\",\"text\",'match']).rows():\n",
    "\n",
    "        context, abstract = get_context_window(text,match)\n",
    "        user_content = f\"\"\"\n",
    "        Paper Abstract: {abstract}\n",
    "        \n",
    "        Citation: {dataset_id}\n",
    "\n",
    "        \n",
    "        Citation Context: {context}\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": PROMPT_CLASSIFY_CITATION_TYPE},\n",
    "            {\"role\": \"user\", \"content\": user_content.strip()}\n",
    "        ]\n",
    "        prompts.append(\n",
    "            tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        )\n",
    "        \n",
    "    return df.with_columns(pl.Series(\"prompt\", prompts))\n",
    "\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        MODEL_PATH,\n",
    "        quantization='awq',\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.9,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=16384,\n",
    "        disable_log_stats=True, \n",
    "        disable_custom_all_reduce=True,\n",
    "        enable_prefix_caching=True,\n",
    "        task='generate')\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "\n",
    "    df=pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "    \n",
    "    if \"row_id\" in df.columns:\n",
    "        df = df.drop(\"row_id\")\n",
    "\n",
    "\n",
    "    doi_df = df.filter(is_doi_link(\"dataset_id\"))\n",
    "    acc_df = df.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "\n",
    "\n",
    "    df = find_context_win(tokenizer,doi_df)\n",
    "\n",
    "    \n",
    "    \n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.8, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A':'Primary', 'B':'Secondary'}\n",
    "    choices = [types[c] for c in choices]\n",
    "\n",
    "\n",
    "    \n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.select('article_id', 'dataset_id','type').write_csv('/tmp/doi_sub.csv')\n",
    "\n",
    "    acc_df = assume_type(acc_df)\n",
    "    acc_df.select('article_id','dataset_id','type').write_csv(\"/tmp/accid_sub.csv\")\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    \n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    # print(df)\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        del llm, tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    import gc, torch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f08e75",
   "metadata": {
    "_cell_guid": "c6a12705-737a-4b21-9bf2-125b3d1ab724",
    "_kg_hide-output": true,
    "_uuid": "bc8e0d68-3097-4ce9-99a1-536921913550",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-07T09:38:57.855122Z",
     "iopub.status.busy": "2025-09-07T09:38:57.854804Z",
     "iopub.status.idle": "2025-09-07T10:22:20.394137Z",
     "shell.execute_reply": "2025-09-07T10:22:20.392991Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2602.552213,
     "end_time": "2025-09-07T10:22:20.395919",
     "exception": false,
     "start_time": "2025-09-07T09:38:57.843706",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "INFO 2025-09-07 09:40:46  [check_parse.py:31 - main()] pymupdf misses: 43 dataset_ids\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:299 - main()] all - f1: 0.1220 [636/9075/83]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:299 - main()] doi - f1: 0.0521 [241/8718/46]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:299 - main()] acc - f1: 0.6672 [395/357/37]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:301 - main()] all - f1: 0.0963 [502/9209/217]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:301 - main()] doi - f1: 0.0355 [164/8795/123]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:301 - main()] acc - f1: 0.5709 [338/414/94]\r\n",
      "INFO 09-07 09:41:19 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "WARNING 09-07 09:41:39 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 09-07 09:41:39 [config.py:1770] Defaulting to use mp for distributed inference\r\n",
      "WARNING 09-07 09:41:39 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 09-07 09:41:39 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \r\n",
      "WARNING 09-07 09:41:39 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:41:39 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\r\n",
      "INFO 09-07 09:41:39 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-07 09:41:39 [cuda.py:289] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:41:39 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:41:39 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 09-07 09:41:41 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:41:41 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "INFO 09-07 09:41:41 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:41:41 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 09-07 09:41:42 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_f8c520a1'), local_subscribe_addr='ipc:///tmp/b7646ece-511f-47ae-bcb8-b17009acc049', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:41:42 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\r\n",
      "INFO 09-07 09:41:42 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 09-07 09:41:42 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:41:42 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:29<01:57, 29.47s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [01:04<01:37, 32.61s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:50<01:17, 38.68s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [02:35<00:41, 41.29s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [03:20<00:00, 42.69s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [03:20<00:00, 40.13s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:45:03 [loader.py:458] Loading weights took 200.73 seconds\r\n",
      "INFO 09-07 09:45:03 [loader.py:458] Loading weights took 201.00 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:45:03 [model_runner.py:1140] Model loading took 9.0935 GiB and 201.253214 seconds\r\n",
      "INFO 09-07 09:45:04 [model_runner.py:1140] Model loading took 9.0935 GiB and 201.527043 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 09:45:19 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 3.63GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-07 09:45:19 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 2.66GiB.\r\n",
      "INFO 09-07 09:45:19 [executor_base.py:112] # cuda blocks: 1363, # CPU blocks: 2048\r\n",
      "INFO 09-07 09:45:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 10.65x\r\n",
      "INFO 09-07 09:45:23 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 19.79 seconds\r\n",
      "Processed prompts: 100%|█| 8959/8959 [36:28<00:00,  4.09it/s, est. speed input: \r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:108 - <module>()] all - f1: 0.6582 [598/500/121]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:108 - <module>()] doi - f1: 0.6414 [203/143/84]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:108 - <module>()] acc - f1: 0.6672 [395/357/37]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:110 - <module>()] all - f1: 0.5195 [472/626/247]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:110 - <module>()] doi - f1: 0.4234 [134/212/153]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:110 - <module>()] acc - f1: 0.5709 [338/414/94]\r\n",
      "INFO 09-07 10:22:12 [multiproc_worker_utils.py:137] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 09-07 10:22:12 [multiproc_worker_utils.py:259] Worker exiting\r\n",
      "[rank0]:[W907 10:22:15.739409981 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp\n",
    "!LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n",
    "! python src/check_parse.py\n",
    "! python src/getid.py\n",
    "! python src/llm_validate.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9432b79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T10:22:20.455979Z",
     "iopub.status.busy": "2025-09-07T10:22:20.455711Z",
     "iopub.status.idle": "2025-09-07T10:37:00.960773Z",
     "shell.execute_reply": "2025-09-07T10:37:00.959896Z"
    },
    "papermill": {
     "duration": 880.536557,
     "end_time": "2025-09-07T10:37:00.962414",
     "exception": false,
     "start_time": "2025-09-07T10:22:20.425857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-07 10:22:28 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "WARNING 09-07 10:22:42 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 09-07 10:22:42 [config.py:1770] Defaulting to use mp for distributed inference\r\n",
      "WARNING 09-07 10:22:42 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 09-07 10:22:43 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \r\n",
      "WARNING 09-07 10:22:43 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:22:43 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\r\n",
      "INFO 09-07 10:22:43 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-07 10:22:43 [cuda.py:289] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:22:43 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:22:43 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 09-07 10:22:45 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:22:45 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "INFO 09-07 10:22:45 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:22:45 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 09-07 10:22:45 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_ae6235c4'), local_subscribe_addr='ipc:///tmp/0580711e-0eb0-4295-b0eb-b47d56f248f3', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:22:45 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\r\n",
      "INFO 09-07 10:22:45 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 09-07 10:22:45 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:22:45 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:12<00:50, 12.73s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:26<00:40, 13.48s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:37<00:24, 12.46s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:40<00:08,  8.72s/it]\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:23:28 [loader.py:458] Loading weights took 42.85 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:42<00:00,  6.26s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:42<00:00,  8.58s/it]\r\n",
      "\r\n",
      "INFO 09-07 10:23:28 [loader.py:458] Loading weights took 43.08 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:23:29 [model_runner.py:1140] Model loading took 9.0935 GiB and 43.168217 seconds\r\n",
      "INFO 09-07 10:23:29 [model_runner.py:1140] Model loading took 9.0935 GiB and 43.394043 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:24:04 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 2.01GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-07 10:24:05 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 2.01GiB.\r\n",
      "INFO 09-07 10:24:05 [executor_base.py:112] # cuda blocks: 1031, # CPU blocks: 2048\r\n",
      "INFO 09-07 10:24:05 [executor_base.py:117] Maximum concurrency for 16384 tokens per request: 1.01x\r\n",
      "INFO 09-07 10:24:09 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.32 seconds\r\n",
      "shape: (346, 4)\r\n",
      "┌───────────────────┬───────────────────┬───────────────────┬──────────────────┐\r\n",
      "│ article_id        ┆ dataset_id        ┆ match             ┆ text             │\r\n",
      "│ ---               ┆ ---               ┆ ---               ┆ ---              │\r\n",
      "│ str               ┆ str               ┆ str               ┆ str              │\r\n",
      "╞═══════════════════╪═══════════════════╪═══════════════════╪══════════════════╡\r\n",
      "│ 10.1186_s12862-01 ┆ https://doi.org/1 ┆ 10.1007/BF0245957 ┆ RESEARCH ARTICLE │\r\n",
      "│ 8-1213-2          ┆ 0.1007/bf0245…    ┆ 5.                ┆ Open Access      │\r\n",
      "│                   ┆                   ┆                   ┆ D…               │\r\n",
      "│ 10.3389_feart.202 ┆ https://doi.org/1 ┆ 10.5066/P9F7K9F2  ┆ Diving deeper    │\r\n",
      "│ 3.1205211         ┆ 0.5066/p9f7k9…    ┆                   ┆ into seep        │\r\n",
      "│                   ┆                   ┆                   ┆ distri…          │\r\n",
      "│ 10.3390_s23094491 ┆ https://doi.org/1 ┆ 10.5281/Zenodo.78 ┆ Citation:        │\r\n",
      "│                   ┆ 0.5281/zenodo…    ┆ 71636,            ┆ Andria, G.;      │\r\n",
      "│                   ┆                   ┆                   ┆ Scarpett…        │\r\n",
      "│ 10.12688_f1000res ┆ https://doi.org/1 ┆ 10.5256/f1000rese ┆ RESEARCH NOTE    │\r\n",
      "│ earch.10556.1     ┆ 0.5256/f1000r…    ┆ arch.10556.d1…    ┆ Healthcare       │\r\n",
      "│                   ┆                   ┆                   ┆ benef…           │\r\n",
      "│ 10.1002_ece3.9627 ┆ https://doi.org/1 ┆ 10.5061/dryad.b8g ┆ Ecology and      │\r\n",
      "│                   ┆ 0.5061/dryad.…    ┆ tht7h3.           ┆ Evolution.       │\r\n",
      "│                   ┆                   ┆                   ┆ 2022;12…         │\r\n",
      "│ …                 ┆ …                 ┆ …                 ┆ …                │\r\n",
      "│ 10.1039_d2cc00847 ┆ https://doi.org/1 ┆ 10.5281/zenodo.60 ┆ This journal is  │\r\n",
      "│ e                 ┆ 0.5281/zenodo…    ┆ 10342.            ┆ The Royal Soc…   │\r\n",
      "│ 10.21105_joss.042 ┆ https://doi.org/1 ┆ 10.5067/MODIS/    ┆ rabpro: global   │\r\n",
      "│ 37                ┆ 0.5067/modis      ┆                   ┆ watershed bound… │\r\n",
      "│ 10.12688_f1000res ┆ https://doi.org/1 ┆ 10.5256/f1000rese ┆ DATA NOTE        │\r\n",
      "│ earch.11698.1     ┆ 0.5256/f1000r…    ┆ arch.11698.d1…    ┆ Data of          │\r\n",
      "│                   ┆                   ┆                   ┆ physiologica…    │\r\n",
      "│ 10.3389_fpls.2023 ┆ https://doi.org/1 ┆ 10.5281/zenodo.13 ┆ How sweet are    │\r\n",
      "│ .1160645          ┆ 0.5281/zenodo…    ┆ 16618             ┆ your             │\r\n",
      "│                   ┆                   ┆                   ┆ strawberrie…     │\r\n",
      "│ 10.5194_acp-22-57 ┆ https://doi.org/1 ┆ 10.24381/cds.adbb ┆ Atmos. Chem.     │\r\n",
      "│ 01-2022           ┆ 0.24381/cds.a…    ┆ 2d47,             ┆ Phys., 22,       │\r\n",
      "│                   ┆                   ┆                   ┆ 570157…          │\r\n",
      "└───────────────────┴───────────────────┴───────────────────┴──────────────────┘\r\n",
      "Processed prompts: 100%|█| 346/346 [12:41<00:00,  2.20s/it, est. speed input: 80\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:168 - <module>()] all - f1: 0.6559 [567/443/152]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:168 - <module>()] doi - f1: 0.6312 [172/86/115]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:168 - <module>()] acc - f1: 0.6672 [395/357/37]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:170 - <module>()] all - f1: 0.5182 [448/562/271]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:170 - <module>()] doi - f1: 0.4037 [110/148/177]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:170 - <module>()] acc - f1: 0.5709 [338/414/94]\r\n",
      "INFO 09-07 10:36:55 [multiproc_worker_utils.py:137] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=330)\u001b[0;0m INFO 09-07 10:36:55 [multiproc_worker_utils.py:259] Worker exiting\r\n",
      "[rank0]:[W907 10:36:58.495966550 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "! python src/post_validate.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ddc551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T10:37:01.031411Z",
     "iopub.status.busy": "2025-09-07T10:37:01.031121Z",
     "iopub.status.idle": "2025-09-07T10:48:30.268800Z",
     "shell.execute_reply": "2025-09-07T10:48:30.267925Z"
    },
    "papermill": {
     "duration": 689.273412,
     "end_time": "2025-09-07T10:48:30.270405",
     "exception": false,
     "start_time": "2025-09-07T10:37:00.996993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-07 10:37:10 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "WARNING 09-07 10:37:26 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 09-07 10:37:26 [config.py:1770] Defaulting to use mp for distributed inference\r\n",
      "WARNING 09-07 10:37:26 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 09-07 10:37:26 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \r\n",
      "WARNING 09-07 10:37:26 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:37:26 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\r\n",
      "INFO 09-07 10:37:26 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-07 10:37:26 [cuda.py:289] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:37:26 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:37:26 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 09-07 10:37:28 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:37:28 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "INFO 09-07 10:37:28 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:37:28 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 09-07 10:37:28 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_a460a5f0'), local_subscribe_addr='ipc:///tmp/2760f2f0-767e-4706-96eb-8587637fd222', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:37:28 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\r\n",
      "INFO 09-07 10:37:28 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 09-07 10:37:28 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:37:28 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:04,  1.09s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.28s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:35<00:31, 15.87s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:50<00:15, 15.43s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:52<00:00, 10.61s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:52<00:00, 10.51s/it]\r\n",
      "\r\n",
      "INFO 09-07 10:38:21 [loader.py:458] Loading weights took 52.65 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:38:21 [loader.py:458] Loading weights took 52.72 seconds\r\n",
      "INFO 09-07 10:38:21 [model_runner.py:1140] Model loading took 9.0935 GiB and 52.959281 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:38:21 [model_runner.py:1140] Model loading took 9.0935 GiB and 53.001064 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:38:55 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 2.01GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-07 10:38:55 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 2.01GiB.\r\n",
      "INFO 09-07 10:38:55 [executor_base.py:112] # cuda blocks: 1031, # CPU blocks: 2048\r\n",
      "INFO 09-07 10:38:55 [executor_base.py:117] Maximum concurrency for 16384 tokens per request: 1.01x\r\n",
      "INFO 09-07 10:38:59 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 37.28 seconds\r\n",
      "shape: (258, 4)\r\n",
      "┌───────────────────┬───────────────────┬───────────────────┬──────────────────┐\r\n",
      "│ article_id        ┆ dataset_id        ┆ match             ┆ text             │\r\n",
      "│ ---               ┆ ---               ┆ ---               ┆ ---              │\r\n",
      "│ str               ┆ str               ┆ str               ┆ str              │\r\n",
      "╞═══════════════════╪═══════════════════╪═══════════════════╪══════════════════╡\r\n",
      "│ 10.12688_f1000res ┆ https://doi.org/1 ┆ 10.5256/f1000rese ┆ RESEARCH NOTE    │\r\n",
      "│ earch.10556.1     ┆ 0.5256/f1000r…    ┆ arch.10556.d1…    ┆ Healthcare       │\r\n",
      "│                   ┆                   ┆                   ┆ benef…           │\r\n",
      "│ 10.1002_ece3.9627 ┆ https://doi.org/1 ┆ 10.5061/dryad.b8g ┆ Ecology and      │\r\n",
      "│                   ┆ 0.5061/dryad.…    ┆ tht7h3.           ┆ Evolution.       │\r\n",
      "│                   ┆                   ┆                   ┆ 2022;12…         │\r\n",
      "│ 10.5194_se-13-154 ┆ https://doi.org/1 ┆ 10.2312/gfz.b103- ┆ Solid Earth, 13, │\r\n",
      "│ 1-2022            ┆ 0.2312/gfz.b1…    ┆ 08095,            ┆ 15411567, 202…   │\r\n",
      "│ 10.1111_1365-2745 ┆ https://doi.org/1 ┆ 10.5061/dryad.r7s ┆ Journal of       │\r\n",
      "│ .13449            ┆ 0.5061/dryad.…    ┆ qv9s8n            ┆ Ecology.         │\r\n",
      "│                   ┆                   ┆                   ┆ 2020;108:2…      │\r\n",
      "│ 10.1186_s12862-01 ┆ https://doi.org/1 ┆ 10.6070/H4T43R3V, ┆ RESEARCH ARTICLE │\r\n",
      "│ 5-0558-z          ┆ 0.6070/h4t43r…    ┆                   ┆ Open Access      │\r\n",
      "│                   ┆                   ┆                   ┆ E…               │\r\n",
      "│ …                 ┆ …                 ┆ …                 ┆ …                │\r\n",
      "│ 10.5194_tc-17-361 ┆ https://doi.org/1 ┆ 10.24381/cds.32b0 ┆ The Cryosphere,  │\r\n",
      "│ 7-2023            ┆ 0.24381/cds.3…    ┆ 4ec5              ┆ 17, 36173660, …  │\r\n",
      "│ 10.3133_cir1497   ┆ https://doi.org/1 ┆ 10.5065/D6N014HK. ┆ U.S. Department  │\r\n",
      "│                   ┆ 0.5065/d6n014…    ┆                   ┆ of the Interio…  │\r\n",
      "│ 10.1039_d2cc00847 ┆ https://doi.org/1 ┆ 10.5281/zenodo.60 ┆ This journal is  │\r\n",
      "│ e                 ┆ 0.5281/zenodo…    ┆ 10342.            ┆ The Royal Soc…   │\r\n",
      "│ 10.12688_f1000res ┆ https://doi.org/1 ┆ 10.5256/f1000rese ┆ DATA NOTE        │\r\n",
      "│ earch.11698.1     ┆ 0.5256/f1000r…    ┆ arch.11698.d1…    ┆ Data of          │\r\n",
      "│                   ┆                   ┆                   ┆ physiologica…    │\r\n",
      "│ 10.5194_acp-22-57 ┆ https://doi.org/1 ┆ 10.24381/cds.adbb ┆ Atmos. Chem.     │\r\n",
      "│ 01-2022           ┆ 0.24381/cds.a…    ┆ 2d47,             ┆ Phys., 22,       │\r\n",
      "│                   ┆                   ┆                   ┆ 570157…          │\r\n",
      "└───────────────────┴───────────────────┴───────────────────┴──────────────────┘\r\n",
      "Processed prompts: 100%|█| 258/258 [09:23<00:00,  2.18s/it, est. speed input: 77\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:176 - <module>()] all - f1: 0.6559 [567/443/152]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:176 - <module>()] doi - f1: 0.6312 [172/86/115]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:176 - <module>()] acc - f1: 0.6672 [395/357/37]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:178 - <module>()] all - f1: 0.5518 [477/533/242]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:178 - <module>()] doi - f1: 0.5101 [139/119/148]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:178 - <module>()] acc - f1: 0.5709 [338/414/94]\r\n",
      "INFO 09-07 10:48:24 [multiproc_worker_utils.py:137] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=458)\u001b[0;0m INFO 09-07 10:48:24 [multiproc_worker_utils.py:259] Worker exiting\r\n",
      "[rank0]:[W907 10:48:27.726932929 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "! python src/predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "662b8571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T10:48:30.338906Z",
     "iopub.status.busy": "2025-09-07T10:48:30.338174Z",
     "iopub.status.idle": "2025-09-07T10:48:30.457390Z",
     "shell.execute_reply": "2025-09-07T10:48:30.456387Z"
    },
    "papermill": {
     "duration": 0.154121,
     "end_time": "2025-09-07T10:48:30.458868",
     "exception": false,
     "start_time": "2025-09-07T10:48:30.304747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-09-07 09:40:55  [getid.py:299 - main()] all - f1: 0.1220 [636/9075/83]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:299 - main()] doi - f1: 0.0521 [241/8718/46]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:299 - main()] acc - f1: 0.6672 [395/357/37]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:301 - main()] all - f1: 0.0963 [502/9209/217]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:301 - main()] doi - f1: 0.0355 [164/8795/123]\r\n",
      "INFO 2025-09-07 09:40:55  [getid.py:301 - main()] acc - f1: 0.5709 [338/414/94]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:108 - <module>()] all - f1: 0.6582 [598/500/121]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:108 - <module>()] doi - f1: 0.6414 [203/143/84]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:108 - <module>()] acc - f1: 0.6672 [395/357/37]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:110 - <module>()] all - f1: 0.5195 [472/626/247]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:110 - <module>()] doi - f1: 0.4234 [134/212/153]\r\n",
      "INFO 2025-09-07 10:22:12  [llm_validate.py:110 - <module>()] acc - f1: 0.5709 [338/414/94]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:168 - <module>()] all - f1: 0.6559 [567/443/152]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:168 - <module>()] doi - f1: 0.6312 [172/86/115]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:168 - <module>()] acc - f1: 0.6672 [395/357/37]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:170 - <module>()] all - f1: 0.5182 [448/562/271]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:170 - <module>()] doi - f1: 0.4037 [110/148/177]\r\n",
      "INFO 2025-09-07 10:36:55  [post_validate.py:170 - <module>()] acc - f1: 0.5709 [338/414/94]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:176 - <module>()] all - f1: 0.6559 [567/443/152]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:176 - <module>()] doi - f1: 0.6312 [172/86/115]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:176 - <module>()] acc - f1: 0.6672 [395/357/37]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:178 - <module>()] all - f1: 0.5518 [477/533/242]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:178 - <module>()] doi - f1: 0.5101 [139/119/148]\r\n",
      "INFO 2025-09-07 10:48:24  [predict.py:178 - <module>()] acc - f1: 0.5709 [338/414/94]\r\n"
     ]
    }
   ],
   "source": [
    "! grep \"f1:\" /tmp/logs/project.log"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "sourceId": 248118764,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141565,
     "sourceId": 166368,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4204.50802,
   "end_time": "2025-09-07T10:48:30.813437",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-07T09:38:26.305417",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
