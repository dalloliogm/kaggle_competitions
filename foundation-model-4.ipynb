{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b305bdf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-30T14:06:47.252024Z",
     "iopub.status.busy": "2025-03-30T14:06:47.251555Z",
     "iopub.status.idle": "2025-03-30T14:33:40.286309Z",
     "shell.execute_reply": "2025-03-30T14:33:40.285200Z"
    },
    "papermill": {
     "duration": 1613.056402,
     "end_time": "2025-03-30T14:33:40.303821",
     "exception": false,
     "start_time": "2025-03-30T14:06:47.247419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (8349, 38)\n",
      "Training Spatial Foundation Autoencoder...\n",
      "Spatial Autoencoder Epoch 1/200, Loss: 40.4327\n",
      "Spatial Autoencoder Epoch 2/200, Loss: 13.2733\n",
      "Spatial Autoencoder Epoch 3/200, Loss: 11.2352\n",
      "Spatial Autoencoder Epoch 4/200, Loss: 9.4801\n",
      "Spatial Autoencoder Epoch 5/200, Loss: 8.1366\n",
      "Spatial Autoencoder Epoch 6/200, Loss: 7.1390\n",
      "Spatial Autoencoder Epoch 7/200, Loss: 6.5800\n",
      "Spatial Autoencoder Epoch 8/200, Loss: 6.1916\n",
      "Spatial Autoencoder Epoch 9/200, Loss: 5.9855\n",
      "Spatial Autoencoder Epoch 10/200, Loss: 5.8222\n",
      "Spatial Autoencoder Epoch 11/200, Loss: 5.6645\n",
      "Spatial Autoencoder Epoch 12/200, Loss: 5.5342\n",
      "Spatial Autoencoder Epoch 13/200, Loss: 5.4220\n",
      "Spatial Autoencoder Epoch 14/200, Loss: 5.3279\n",
      "Spatial Autoencoder Epoch 15/200, Loss: 5.2535\n",
      "Spatial Autoencoder Epoch 16/200, Loss: 5.1719\n",
      "Spatial Autoencoder Epoch 17/200, Loss: 5.1097\n",
      "Spatial Autoencoder Epoch 18/200, Loss: 5.0419\n",
      "Spatial Autoencoder Epoch 19/200, Loss: 4.9750\n",
      "Spatial Autoencoder Epoch 20/200, Loss: 4.9119\n",
      "Spatial Autoencoder Epoch 21/200, Loss: 4.8593\n",
      "Spatial Autoencoder Epoch 22/200, Loss: 4.8300\n",
      "Spatial Autoencoder Epoch 23/200, Loss: 4.7801\n",
      "Spatial Autoencoder Epoch 24/200, Loss: 4.7491\n",
      "Spatial Autoencoder Epoch 25/200, Loss: 4.6900\n",
      "Spatial Autoencoder Epoch 26/200, Loss: 4.6569\n",
      "Spatial Autoencoder Epoch 27/200, Loss: 4.6216\n",
      "Spatial Autoencoder Epoch 28/200, Loss: 4.5827\n",
      "Spatial Autoencoder Epoch 29/200, Loss: 4.5578\n",
      "Spatial Autoencoder Epoch 30/200, Loss: 4.5281\n",
      "Spatial Autoencoder Epoch 31/200, Loss: 4.4893\n",
      "Spatial Autoencoder Epoch 32/200, Loss: 4.4793\n",
      "Spatial Autoencoder Epoch 33/200, Loss: 4.4545\n",
      "Spatial Autoencoder Epoch 34/200, Loss: 4.4303\n",
      "Spatial Autoencoder Epoch 35/200, Loss: 4.4209\n",
      "Spatial Autoencoder Epoch 36/200, Loss: 4.3703\n",
      "Spatial Autoencoder Epoch 37/200, Loss: 4.3651\n",
      "Spatial Autoencoder Epoch 38/200, Loss: 4.3488\n",
      "Spatial Autoencoder Epoch 39/200, Loss: 4.3078\n",
      "Spatial Autoencoder Epoch 40/200, Loss: 4.3092\n",
      "Spatial Autoencoder Epoch 41/200, Loss: 4.2759\n",
      "Spatial Autoencoder Epoch 42/200, Loss: 4.2600\n",
      "Spatial Autoencoder Epoch 43/200, Loss: 4.2408\n",
      "Spatial Autoencoder Epoch 44/200, Loss: 4.2281\n",
      "Spatial Autoencoder Epoch 45/200, Loss: 4.2227\n",
      "Spatial Autoencoder Epoch 46/200, Loss: 4.1913\n",
      "Spatial Autoencoder Epoch 47/200, Loss: 4.1962\n",
      "Spatial Autoencoder Epoch 48/200, Loss: 4.1623\n",
      "Spatial Autoencoder Epoch 49/200, Loss: 4.1530\n",
      "Spatial Autoencoder Epoch 50/200, Loss: 4.1414\n",
      "Spatial Autoencoder Epoch 51/200, Loss: 4.1244\n",
      "Spatial Autoencoder Epoch 52/200, Loss: 4.1062\n",
      "Spatial Autoencoder Epoch 53/200, Loss: 4.0969\n",
      "Spatial Autoencoder Epoch 54/200, Loss: 4.0793\n",
      "Spatial Autoencoder Epoch 55/200, Loss: 4.0678\n",
      "Spatial Autoencoder Epoch 56/200, Loss: 4.0617\n",
      "Spatial Autoencoder Epoch 57/200, Loss: 4.0340\n",
      "Spatial Autoencoder Epoch 58/200, Loss: 4.0334\n",
      "Spatial Autoencoder Epoch 59/200, Loss: 4.0121\n",
      "Spatial Autoencoder Epoch 60/200, Loss: 4.0014\n",
      "Spatial Autoencoder Epoch 61/200, Loss: 4.0089\n",
      "Spatial Autoencoder Epoch 62/200, Loss: 3.9865\n",
      "Spatial Autoencoder Epoch 63/200, Loss: 3.9641\n",
      "Spatial Autoencoder Epoch 64/200, Loss: 3.9605\n",
      "Spatial Autoencoder Epoch 65/200, Loss: 3.9525\n",
      "Spatial Autoencoder Epoch 66/200, Loss: 3.9368\n",
      "Spatial Autoencoder Epoch 67/200, Loss: 3.9173\n",
      "Spatial Autoencoder Epoch 68/200, Loss: 3.9225\n",
      "Spatial Autoencoder Epoch 69/200, Loss: 3.9068\n",
      "Spatial Autoencoder Epoch 70/200, Loss: 3.8941\n",
      "Spatial Autoencoder Epoch 71/200, Loss: 3.8870\n",
      "Spatial Autoencoder Epoch 72/200, Loss: 3.8748\n",
      "Spatial Autoencoder Epoch 73/200, Loss: 3.8788\n",
      "Spatial Autoencoder Epoch 74/200, Loss: 3.8494\n",
      "Spatial Autoencoder Epoch 75/200, Loss: 3.8507\n",
      "Spatial Autoencoder Epoch 76/200, Loss: 3.8338\n",
      "Spatial Autoencoder Epoch 77/200, Loss: 3.8257\n",
      "Spatial Autoencoder Epoch 78/200, Loss: 3.8372\n",
      "Spatial Autoencoder Epoch 79/200, Loss: 3.8185\n",
      "Spatial Autoencoder Epoch 80/200, Loss: 3.8148\n",
      "Spatial Autoencoder Epoch 81/200, Loss: 3.7941\n",
      "Spatial Autoencoder Epoch 82/200, Loss: 3.7988\n",
      "Spatial Autoencoder Epoch 83/200, Loss: 3.7766\n",
      "Spatial Autoencoder Epoch 84/200, Loss: 3.7725\n",
      "Spatial Autoencoder Epoch 85/200, Loss: 3.7686\n",
      "Spatial Autoencoder Epoch 86/200, Loss: 3.7634\n",
      "Spatial Autoencoder Epoch 87/200, Loss: 3.7461\n",
      "Spatial Autoencoder Epoch 88/200, Loss: 3.7504\n",
      "Spatial Autoencoder Epoch 89/200, Loss: 3.7368\n",
      "Spatial Autoencoder Epoch 90/200, Loss: 3.7349\n",
      "Spatial Autoencoder Epoch 91/200, Loss: 3.7307\n",
      "Spatial Autoencoder Epoch 92/200, Loss: 3.7175\n",
      "Spatial Autoencoder Epoch 93/200, Loss: 3.6991\n",
      "Spatial Autoencoder Epoch 94/200, Loss: 3.6895\n",
      "Spatial Autoencoder Epoch 95/200, Loss: 3.7045\n",
      "Spatial Autoencoder Epoch 96/200, Loss: 3.6942\n",
      "Spatial Autoencoder Epoch 97/200, Loss: 3.6749\n",
      "Spatial Autoencoder Epoch 98/200, Loss: 3.6708\n",
      "Spatial Autoencoder Epoch 99/200, Loss: 3.6618\n",
      "Spatial Autoencoder Epoch 100/200, Loss: 3.6615\n",
      "Spatial Autoencoder Epoch 101/200, Loss: 3.6466\n",
      "Spatial Autoencoder Epoch 102/200, Loss: 3.6361\n",
      "Spatial Autoencoder Epoch 103/200, Loss: 3.6359\n",
      "Spatial Autoencoder Epoch 104/200, Loss: 3.6347\n",
      "Spatial Autoencoder Epoch 105/200, Loss: 3.6258\n",
      "Spatial Autoencoder Epoch 106/200, Loss: 3.6140\n",
      "Spatial Autoencoder Epoch 107/200, Loss: 3.6080\n",
      "Spatial Autoencoder Epoch 108/200, Loss: 3.6025\n",
      "Spatial Autoencoder Epoch 109/200, Loss: 3.6032\n",
      "Spatial Autoencoder Epoch 110/200, Loss: 3.5922\n",
      "Spatial Autoencoder Epoch 111/200, Loss: 3.5825\n",
      "Spatial Autoencoder Epoch 112/200, Loss: 3.5748\n",
      "Spatial Autoencoder Epoch 113/200, Loss: 3.5668\n",
      "Spatial Autoencoder Epoch 114/200, Loss: 3.5688\n",
      "Spatial Autoencoder Epoch 115/200, Loss: 3.5704\n",
      "Spatial Autoencoder Epoch 116/200, Loss: 3.5551\n",
      "Spatial Autoencoder Epoch 117/200, Loss: 3.5595\n",
      "Spatial Autoencoder Epoch 118/200, Loss: 3.5373\n",
      "Spatial Autoencoder Epoch 119/200, Loss: 3.5449\n",
      "Spatial Autoencoder Epoch 120/200, Loss: 3.5295\n",
      "Spatial Autoencoder Epoch 121/200, Loss: 3.5316\n",
      "Spatial Autoencoder Epoch 122/200, Loss: 3.5154\n",
      "Spatial Autoencoder Epoch 123/200, Loss: 3.5241\n",
      "Spatial Autoencoder Epoch 124/200, Loss: 3.5023\n",
      "Spatial Autoencoder Epoch 125/200, Loss: 3.5061\n",
      "Spatial Autoencoder Epoch 126/200, Loss: 3.4944\n",
      "Spatial Autoencoder Epoch 127/200, Loss: 3.4988\n",
      "Spatial Autoencoder Epoch 128/200, Loss: 3.4817\n",
      "Spatial Autoencoder Epoch 129/200, Loss: 3.4817\n",
      "Spatial Autoencoder Epoch 130/200, Loss: 3.4876\n",
      "Spatial Autoencoder Epoch 131/200, Loss: 3.4704\n",
      "Spatial Autoencoder Epoch 132/200, Loss: 3.4645\n",
      "Spatial Autoencoder Epoch 133/200, Loss: 3.4576\n",
      "Spatial Autoencoder Epoch 134/200, Loss: 3.4575\n",
      "Spatial Autoencoder Epoch 135/200, Loss: 3.4429\n",
      "Spatial Autoencoder Epoch 136/200, Loss: 3.4341\n",
      "Spatial Autoencoder Epoch 137/200, Loss: 3.4379\n",
      "Spatial Autoencoder Epoch 138/200, Loss: 3.4331\n",
      "Spatial Autoencoder Epoch 139/200, Loss: 3.4358\n",
      "Spatial Autoencoder Epoch 140/200, Loss: 3.4254\n",
      "Spatial Autoencoder Epoch 141/200, Loss: 3.4129\n",
      "Spatial Autoencoder Epoch 142/200, Loss: 3.4224\n",
      "Spatial Autoencoder Epoch 143/200, Loss: 3.4236\n",
      "Spatial Autoencoder Epoch 144/200, Loss: 3.3984\n",
      "Spatial Autoencoder Epoch 145/200, Loss: 3.4129\n",
      "Spatial Autoencoder Epoch 146/200, Loss: 3.3996\n",
      "Spatial Autoencoder Epoch 147/200, Loss: 3.3953\n",
      "Spatial Autoencoder Epoch 148/200, Loss: 3.3835\n",
      "Spatial Autoencoder Epoch 149/200, Loss: 3.3958\n",
      "Spatial Autoencoder Epoch 150/200, Loss: 3.3792\n",
      "Spatial Autoencoder Epoch 151/200, Loss: 3.3755\n",
      "Spatial Autoencoder Epoch 152/200, Loss: 3.3672\n",
      "Spatial Autoencoder Epoch 153/200, Loss: 3.3764\n",
      "Spatial Autoencoder Epoch 154/200, Loss: 3.3649\n",
      "Spatial Autoencoder Epoch 155/200, Loss: 3.3592\n",
      "Spatial Autoencoder Epoch 156/200, Loss: 3.3453\n",
      "Spatial Autoencoder Epoch 157/200, Loss: 3.3459\n",
      "Spatial Autoencoder Epoch 158/200, Loss: 3.3401\n",
      "Spatial Autoencoder Epoch 159/200, Loss: 3.3350\n",
      "Spatial Autoencoder Epoch 160/200, Loss: 3.3269\n",
      "Spatial Autoencoder Epoch 161/200, Loss: 3.3222\n",
      "Spatial Autoencoder Epoch 162/200, Loss: 3.3194\n",
      "Spatial Autoencoder Epoch 163/200, Loss: 3.3189\n",
      "Spatial Autoencoder Epoch 164/200, Loss: 3.3132\n",
      "Spatial Autoencoder Epoch 165/200, Loss: 3.3150\n",
      "Spatial Autoencoder Epoch 166/200, Loss: 3.3112\n",
      "Spatial Autoencoder Epoch 167/200, Loss: 3.3096\n",
      "Spatial Autoencoder Epoch 168/200, Loss: 3.3042\n",
      "Spatial Autoencoder Epoch 169/200, Loss: 3.2885\n",
      "Spatial Autoencoder Epoch 170/200, Loss: 3.2875\n",
      "Spatial Autoencoder Epoch 171/200, Loss: 3.2871\n",
      "Spatial Autoencoder Epoch 172/200, Loss: 3.2736\n",
      "Spatial Autoencoder Epoch 173/200, Loss: 3.2719\n",
      "Spatial Autoencoder Epoch 174/200, Loss: 3.2805\n",
      "Spatial Autoencoder Epoch 175/200, Loss: 3.2859\n",
      "Spatial Autoencoder Epoch 176/200, Loss: 3.2703\n",
      "Spatial Autoencoder Epoch 177/200, Loss: 3.2669\n",
      "Spatial Autoencoder Epoch 178/200, Loss: 3.2625\n",
      "Spatial Autoencoder Epoch 179/200, Loss: 3.2663\n",
      "Spatial Autoencoder Epoch 180/200, Loss: 3.2531\n",
      "Spatial Autoencoder Epoch 181/200, Loss: 3.2577\n",
      "Spatial Autoencoder Epoch 182/200, Loss: 3.2570\n",
      "Spatial Autoencoder Epoch 183/200, Loss: 3.2556\n",
      "Spatial Autoencoder Epoch 184/200, Loss: 3.2517\n",
      "Spatial Autoencoder Epoch 185/200, Loss: 3.2484\n",
      "Spatial Autoencoder Epoch 186/200, Loss: 3.2390\n",
      "Spatial Autoencoder Epoch 187/200, Loss: 3.2510\n",
      "Spatial Autoencoder Epoch 188/200, Loss: 3.2346\n",
      "Spatial Autoencoder Epoch 189/200, Loss: 3.2406\n",
      "Spatial Autoencoder Epoch 190/200, Loss: 3.2259\n",
      "Spatial Autoencoder Epoch 191/200, Loss: 3.2381\n",
      "Spatial Autoencoder Epoch 192/200, Loss: 3.2332\n",
      "Spatial Autoencoder Epoch 193/200, Loss: 3.2340\n",
      "Spatial Autoencoder Epoch 194/200, Loss: 3.2206\n",
      "Spatial Autoencoder Epoch 195/200, Loss: 3.2196\n",
      "Spatial Autoencoder Epoch 196/200, Loss: 3.2177\n",
      "Spatial Autoencoder Epoch 197/200, Loss: 3.2198\n",
      "Spatial Autoencoder Epoch 198/200, Loss: 3.2135\n",
      "Spatial Autoencoder Epoch 199/200, Loss: 3.2200\n",
      "Spatial Autoencoder Epoch 200/200, Loss: 3.2082\n",
      "Training Main Mapping With Image Model...\n",
      "Main Mapping With Image Epoch 1/100, Loss: 62.1028\n",
      "Main Mapping With Image Epoch 2/100, Loss: 56.3183\n",
      "Main Mapping With Image Epoch 3/100, Loss: 53.1661\n",
      "Main Mapping With Image Epoch 4/100, Loss: 49.5229\n",
      "Main Mapping With Image Epoch 5/100, Loss: 47.6684\n",
      "Main Mapping With Image Epoch 6/100, Loss: 46.1695\n",
      "Main Mapping With Image Epoch 7/100, Loss: 44.7183\n",
      "Main Mapping With Image Epoch 8/100, Loss: 43.6261\n",
      "Main Mapping With Image Epoch 9/100, Loss: 42.3447\n",
      "Main Mapping With Image Epoch 10/100, Loss: 41.9757\n",
      "Main Mapping With Image Epoch 11/100, Loss: 40.7497\n",
      "Main Mapping With Image Epoch 12/100, Loss: 40.3344\n",
      "Main Mapping With Image Epoch 13/100, Loss: 39.7055\n",
      "Main Mapping With Image Epoch 14/100, Loss: 38.5608\n",
      "Main Mapping With Image Epoch 15/100, Loss: 38.4143\n",
      "Main Mapping With Image Epoch 16/100, Loss: 37.7444\n",
      "Main Mapping With Image Epoch 17/100, Loss: 37.3811\n",
      "Main Mapping With Image Epoch 18/100, Loss: 37.1020\n",
      "Main Mapping With Image Epoch 19/100, Loss: 36.9659\n",
      "Main Mapping With Image Epoch 20/100, Loss: 36.4959\n",
      "Main Mapping With Image Epoch 21/100, Loss: 36.0675\n",
      "Main Mapping With Image Epoch 22/100, Loss: 36.1648\n",
      "Main Mapping With Image Epoch 23/100, Loss: 35.8819\n",
      "Main Mapping With Image Epoch 24/100, Loss: 35.5050\n",
      "Main Mapping With Image Epoch 25/100, Loss: 35.1184\n",
      "Main Mapping With Image Epoch 26/100, Loss: 34.8742\n",
      "Main Mapping With Image Epoch 27/100, Loss: 34.8052\n",
      "Main Mapping With Image Epoch 28/100, Loss: 34.3439\n",
      "Main Mapping With Image Epoch 29/100, Loss: 34.8721\n",
      "Main Mapping With Image Epoch 30/100, Loss: 34.1009\n",
      "Main Mapping With Image Epoch 31/100, Loss: 33.9030\n",
      "Main Mapping With Image Epoch 32/100, Loss: 33.6502\n",
      "Main Mapping With Image Epoch 33/100, Loss: 33.7118\n",
      "Main Mapping With Image Epoch 34/100, Loss: 33.5585\n",
      "Main Mapping With Image Epoch 35/100, Loss: 33.3742\n",
      "Main Mapping With Image Epoch 36/100, Loss: 33.1444\n",
      "Main Mapping With Image Epoch 37/100, Loss: 32.8390\n",
      "Main Mapping With Image Epoch 38/100, Loss: 32.6551\n",
      "Main Mapping With Image Epoch 39/100, Loss: 32.6997\n",
      "Main Mapping With Image Epoch 40/100, Loss: 32.4455\n",
      "Main Mapping With Image Epoch 41/100, Loss: 32.1250\n",
      "Main Mapping With Image Epoch 42/100, Loss: 32.2138\n",
      "Main Mapping With Image Epoch 43/100, Loss: 31.9655\n",
      "Main Mapping With Image Epoch 44/100, Loss: 31.7610\n",
      "Main Mapping With Image Epoch 45/100, Loss: 31.9221\n",
      "Main Mapping With Image Epoch 46/100, Loss: 31.7708\n",
      "Main Mapping With Image Epoch 47/100, Loss: 31.8324\n",
      "Main Mapping With Image Epoch 48/100, Loss: 31.2652\n",
      "Main Mapping With Image Epoch 49/100, Loss: 31.2464\n",
      "Main Mapping With Image Epoch 50/100, Loss: 30.8795\n",
      "Main Mapping With Image Epoch 51/100, Loss: 30.6257\n",
      "Main Mapping With Image Epoch 52/100, Loss: 30.7076\n",
      "Main Mapping With Image Epoch 53/100, Loss: 30.5082\n",
      "Main Mapping With Image Epoch 54/100, Loss: 30.5751\n",
      "Main Mapping With Image Epoch 55/100, Loss: 30.3606\n",
      "Main Mapping With Image Epoch 56/100, Loss: 30.2114\n",
      "Main Mapping With Image Epoch 57/100, Loss: 30.1147\n",
      "Main Mapping With Image Epoch 58/100, Loss: 30.3948\n",
      "Main Mapping With Image Epoch 59/100, Loss: 29.8341\n",
      "Main Mapping With Image Epoch 60/100, Loss: 29.8109\n",
      "Main Mapping With Image Epoch 61/100, Loss: 29.6573\n",
      "Main Mapping With Image Epoch 62/100, Loss: 29.5417\n",
      "Main Mapping With Image Epoch 63/100, Loss: 29.5014\n",
      "Main Mapping With Image Epoch 64/100, Loss: 29.3399\n",
      "Main Mapping With Image Epoch 65/100, Loss: 29.1874\n",
      "Main Mapping With Image Epoch 66/100, Loss: 29.1047\n",
      "Main Mapping With Image Epoch 67/100, Loss: 28.7105\n",
      "Main Mapping With Image Epoch 68/100, Loss: 28.7299\n",
      "Main Mapping With Image Epoch 69/100, Loss: 28.6347\n",
      "Main Mapping With Image Epoch 70/100, Loss: 28.5833\n",
      "Main Mapping With Image Epoch 71/100, Loss: 28.5723\n",
      "Main Mapping With Image Epoch 72/100, Loss: 28.4245\n",
      "Main Mapping With Image Epoch 73/100, Loss: 28.2138\n",
      "Main Mapping With Image Epoch 74/100, Loss: 28.0191\n",
      "Main Mapping With Image Epoch 75/100, Loss: 27.9681\n",
      "Main Mapping With Image Epoch 76/100, Loss: 27.8538\n",
      "Main Mapping With Image Epoch 77/100, Loss: 27.9483\n",
      "Main Mapping With Image Epoch 78/100, Loss: 27.6664\n",
      "Main Mapping With Image Epoch 79/100, Loss: 27.4703\n",
      "Main Mapping With Image Epoch 80/100, Loss: 27.2424\n",
      "Main Mapping With Image Epoch 81/100, Loss: 27.4839\n",
      "Main Mapping With Image Epoch 82/100, Loss: 27.0649\n",
      "Main Mapping With Image Epoch 83/100, Loss: 26.9916\n",
      "Main Mapping With Image Epoch 84/100, Loss: 26.8934\n",
      "Main Mapping With Image Epoch 85/100, Loss: 26.7917\n",
      "Main Mapping With Image Epoch 86/100, Loss: 26.8108\n",
      "Main Mapping With Image Epoch 87/100, Loss: 26.5910\n",
      "Main Mapping With Image Epoch 88/100, Loss: 26.6742\n",
      "Main Mapping With Image Epoch 89/100, Loss: 26.2828\n",
      "Main Mapping With Image Epoch 90/100, Loss: 26.3544\n",
      "Main Mapping With Image Epoch 91/100, Loss: 26.0222\n",
      "Main Mapping With Image Epoch 92/100, Loss: 26.1044\n",
      "Main Mapping With Image Epoch 93/100, Loss: 25.8299\n",
      "Main Mapping With Image Epoch 94/100, Loss: 25.8613\n",
      "Main Mapping With Image Epoch 95/100, Loss: 25.7498\n",
      "Main Mapping With Image Epoch 96/100, Loss: 25.8418\n",
      "Main Mapping With Image Epoch 97/100, Loss: 25.2902\n",
      "Main Mapping With Image Epoch 98/100, Loss: 25.4108\n",
      "Main Mapping With Image Epoch 99/100, Loss: 25.2011\n",
      "Main Mapping With Image Epoch 100/100, Loss: 25.0460\n",
      "Test data shape: (2088, 3)\n",
      "Submission file 'submission.csv' created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial import KDTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "##############################\n",
    "# PART A: SPATIAL FOUNDATION AUTOENCODER\n",
    "##############################\n",
    "\n",
    "# 1. Load Training Data (spots/Train) with Slice Information\n",
    "h5_file_path = \"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\"\n",
    "\n",
    "with h5py.File(h5_file_path, \"r\") as f:\n",
    "    train_spots = f[\"spots/Train\"]\n",
    "    # Load each slide and tag with its slice name.\n",
    "    train_spot_tables = {\n",
    "        slide: pd.DataFrame(np.array(train_spots[slide])).assign(slice_name=slide)\n",
    "        for slide in train_spots.keys()\n",
    "    }\n",
    "# Concatenate all slides.\n",
    "train_df = pd.concat(train_spot_tables.values(), ignore_index=True)\n",
    "\n",
    "# Assume first two columns are coordinates, next 35 are cell abundances.\n",
    "cell_types = [f\"C{i+1}\" for i in range(35)]\n",
    "train_df.columns = [\"x\", \"y\"] + cell_types + [\"slice_name\"]\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "\n",
    "# 2. Compute Descending Ranks for Each Spot (highest abundance gets rank 1)\n",
    "# This uses the pandas rank function row-wise.\n",
    "ranks = train_df[cell_types].rank(axis=1, method=\"dense\", ascending=False).values  # shape (N,35)\n",
    "\n",
    "# 3. Compute Neighbor-Aggregated Ranks (using a KDTree within each slice)\n",
    "def compute_neighbor_aggregated_ranks(df, rank_array, radius=100):\n",
    "    agg = np.zeros_like(rank_array)\n",
    "    for slice_name in df['slice_name'].unique():\n",
    "        slice_idx = df.index[df['slice_name'] == slice_name].tolist()\n",
    "        coords = df.loc[slice_idx, [\"x\", \"y\"]].values\n",
    "        tree = KDTree(coords)\n",
    "        for i, spot in enumerate(coords):\n",
    "            neighbor_local = tree.query_ball_point(spot, r=radius)\n",
    "            neighbor_global = [slice_idx[j] for j in neighbor_local]\n",
    "            agg[slice_idx[i]] = rank_array[neighbor_global].mean(axis=0)\n",
    "    return agg\n",
    "\n",
    "neighbor_agg = compute_neighbor_aggregated_ranks(train_df, ranks, radius=100)\n",
    "\n",
    "# 4. Concatenate Own Ranks with Neighbor Aggregated Ranks -> 70-dim features.\n",
    "spatial_features = np.concatenate([ranks, neighbor_agg], axis=1)  # shape (N,70)\n",
    "\n",
    "# 5. Define a Spatial Foundation Autoencoder that learns a latent (16-dim) embedding.\n",
    "class SpatialFoundationAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=70, embed_dim=16, hidden_dim=128):\n",
    "        super(SpatialFoundationAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)\n",
    "        recon = self.decoder(emb)\n",
    "        return emb, recon\n",
    "\n",
    "# Dataset for spatial autoencoder training.\n",
    "class SpatialFoundationDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.data = torch.tensor(features, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def train_spatial_autoencoder(model, dataloader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for data in dataloader:\n",
    "            data = data.to(device)\n",
    "            emb, recon = model(data)\n",
    "            loss = criterion(recon, data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * data.size(0)\n",
    "        print(f\"Spatial Autoencoder Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader.dataset):.4f}\")\n",
    "    return model\n",
    "\n",
    "spatial_dataset = SpatialFoundationDataset(spatial_features)\n",
    "spatial_loader = DataLoader(spatial_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "spatial_model = SpatialFoundationAutoencoder(input_dim=70, embed_dim=16, hidden_dim=128)\n",
    "print(\"Training Spatial Foundation Autoencoder...\")\n",
    "spatial_model = train_spatial_autoencoder(spatial_model, spatial_loader, num_epochs=200, lr=0.001, device=device)\n",
    "\n",
    "# Precompute the spatial embeddings for each training spot:\n",
    "with torch.no_grad():\n",
    "    spatial_model.eval()\n",
    "    spatial_tensor = torch.tensor(spatial_features, dtype=torch.float32).to(device)\n",
    "    spatial_embeddings = spatial_model.encoder(spatial_tensor).cpu().numpy()  # shape (N,16)\n",
    "\n",
    "##############################\n",
    "# PART B: MAIN MODEL TO PREDICT SPATIAL EMBEDDINGS\n",
    "##############################\n",
    "\n",
    "# For this main model we assume that at test time we don't have cell abundances,\n",
    "# so we use available inputs (e.g. coordinates and image patches from the H5 file).\n",
    "# The target for training is the spatial embedding computed above.\n",
    "\n",
    "# We'll build dataset classes that load images from the H5 file.\n",
    "\n",
    "# Dataset for training main model (loading images from H5)\n",
    "class MainMappingWithImageH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    For training: maps (x, y) coordinates and corresponding image patch (from the H5 file)\n",
    "    to the precomputed spatial embedding.\n",
    "    Expects a DataFrame with columns: \"x\", \"y\", and \"slice_name\".\n",
    "    \"\"\"\n",
    "    def __init__(self, df, target_embeddings, h5_file_path, patch_size=64, transform=None, train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.targets = target_embeddings  # should be in the same order as df.\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.train = train\n",
    "        self.images = {}\n",
    "        group = \"Train\" if train else \"Test\"\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            for slice_name in self.df['slice_name'].unique():\n",
    "                img_array = np.array(f[f\"images/{group}\"][slice_name])\n",
    "                # Normalize and convert to uint8 if needed.\n",
    "                if img_array.dtype != np.uint8:\n",
    "                    img_array = img_array - img_array.min()\n",
    "                    if img_array.max() > 0:\n",
    "                        img_array = img_array / img_array.max()\n",
    "                    img_array = (img_array * 255).astype(np.uint8)\n",
    "                if img_array.ndim > 3:\n",
    "                    img_array = np.squeeze(img_array)\n",
    "                if img_array.ndim == 2:\n",
    "                    img_array = np.stack([img_array]*3, axis=-1)\n",
    "                if img_array.shape[-1] != 3:\n",
    "                    raise ValueError(f\"Unexpected number of channels in image for slice {slice_name}: {img_array.shape}\")\n",
    "                self.images[slice_name] = Image.fromarray(img_array, mode=\"RGB\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        coord = np.array([row['x'], row['y']], dtype=np.float32)\n",
    "        slice_name = row['slice_name']\n",
    "        image = self.images[slice_name]\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "        half_patch = self.patch_size // 2\n",
    "        left = max(x - half_patch, 0)\n",
    "        upper = max(y - half_patch, 0)\n",
    "        right = left + self.patch_size\n",
    "        lower = upper + self.patch_size\n",
    "        patch = image.crop((left, upper, right, lower))\n",
    "        patch = self.transform(patch)\n",
    "        target = self.targets[idx]\n",
    "        return torch.tensor(coord, dtype=torch.float32), patch, torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "# Dataset for test (similarly loads images from H5)\n",
    "class TestMappingWithImageH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    For testing: maps (x, y) coordinates and corresponding image patch (from the H5 file).\n",
    "    Expects a DataFrame with columns: \"x\", \"y\", and \"slice_name\".\n",
    "    \"\"\"\n",
    "    def __init__(self, df, h5_file_path, patch_size=64, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.images = {}\n",
    "        group = \"Test\"\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            for slice_name in self.df['slice_name'].unique():\n",
    "                img_array = np.array(f[f\"images/{group}\"][slice_name])\n",
    "                if img_array.dtype != np.uint8:\n",
    "                    img_array = img_array - img_array.min()\n",
    "                    if img_array.max() > 0:\n",
    "                        img_array = img_array / img_array.max()\n",
    "                    img_array = (img_array * 255).astype(np.uint8)\n",
    "                if img_array.ndim > 3:\n",
    "                    img_array = np.squeeze(img_array)\n",
    "                if img_array.ndim == 2:\n",
    "                    img_array = np.stack([img_array]*3, axis=-1)\n",
    "                if img_array.shape[-1] != 3:\n",
    "                    raise ValueError(f\"Unexpected number of channels in image for slice {slice_name}: {img_array.shape}\")\n",
    "                self.images[slice_name] = Image.fromarray(img_array, mode=\"RGB\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        coord = np.array([row['x'], row['y']], dtype=np.float32)\n",
    "        slice_name = row['slice_name']\n",
    "        image = self.images[slice_name]\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "        half_patch = self.patch_size // 2\n",
    "        left = max(x - half_patch, 0)\n",
    "        upper = max(y - half_patch, 0)\n",
    "        right = left + self.patch_size\n",
    "        lower = upper + self.patch_size\n",
    "        patch = image.crop((left, upper, right, lower))\n",
    "        patch = self.transform(patch)\n",
    "        return torch.tensor(coord, dtype=torch.float32), patch\n",
    "\n",
    "# Define a main model that predicts the spatial embedding from coordinates and image patch.\n",
    "class MainModelMappingWithImage(nn.Module):\n",
    "    def __init__(self, coord_input_dim=2, patch_channels=3, patch_size=64, embed_dim=16, hidden_dim=64):\n",
    "        super(MainModelMappingWithImage, self).__init__()\n",
    "        # CNN for image patch encoding.\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(patch_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        img_feat_dim = 32 * (patch_size // 4) * (patch_size // 4)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(coord_input_dim + img_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "    def forward(self, coords, patches):\n",
    "        img_features = self.image_encoder(patches)\n",
    "        combined = torch.cat([coords, img_features], dim=1)\n",
    "        emb = self.fc(combined)\n",
    "        return emb\n",
    "\n",
    "def train_main_mapping_with_image(model, dataloader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for coords, patches, target_emb in dataloader:\n",
    "            coords = coords.to(device)\n",
    "            patches = patches.to(device)\n",
    "            target_emb = target_emb.to(device)\n",
    "            pred_emb = model(coords, patches)\n",
    "            loss = criterion(pred_emb, target_emb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * coords.size(0)\n",
    "        print(f\"Main Mapping With Image Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader.dataset):.4f}\")\n",
    "    return model\n",
    "\n",
    "# Prepare training dataset for the main model.\n",
    "patch_size = 64\n",
    "# Use the H5-based dataset for training. (Images from \"images/Train\")\n",
    "main_train_dataset = MainMappingWithImageH5Dataset(\n",
    "    train_df, spatial_embeddings, h5_file_path, patch_size=patch_size, transform=transforms.ToTensor(), train=True\n",
    ")\n",
    "main_train_loader = DataLoader(main_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "main_model_img = MainModelMappingWithImage(coord_input_dim=2, patch_channels=3, patch_size=patch_size, embed_dim=16, hidden_dim=64)\n",
    "print(\"Training Main Mapping With Image Model...\")\n",
    "main_model_img = train_main_mapping_with_image(main_model_img, main_train_loader, num_epochs=100, lr=0.001, device=device)\n",
    "\n",
    "##############################\n",
    "# PART C: INFERENCE AND SUBMISSION\n",
    "##############################\n",
    "\n",
    "# Load test spots for slide \"S_7\" from the H5 file.\n",
    "with h5py.File(h5_file_path, \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_array = np.array(test_spots[\"S_7\"])\n",
    "    test_df = pd.DataFrame(test_array)\n",
    "# Test file has three columns: x, y, Test_set. Drop the third column.\n",
    "if test_df.shape[1] == 3:\n",
    "    test_df.columns = [\"x\", \"y\", \"Test_set\"]\n",
    "    test_df = test_df[[\"x\", \"y\"]]\n",
    "# Add slice_name column for image lookup.\n",
    "test_df[\"slice_name\"] = \"S_7\"\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "# Create the test dataset using the H5 file (images from \"images/Test\").\n",
    "test_dataset = TestMappingWithImageH5Dataset(test_df, h5_file_path, patch_size=patch_size, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Use the main model to predict the spatial embedding for test spots.\n",
    "predicted_embeddings_list = []\n",
    "main_model_img.eval()\n",
    "with torch.no_grad():\n",
    "    for coords, patches in test_loader:\n",
    "        coords = coords.to(device)\n",
    "        patches = patches.to(device)\n",
    "        pred_emb = main_model_img(coords, patches)\n",
    "        predicted_embeddings_list.append(pred_emb)\n",
    "    predicted_embeddings = torch.cat(predicted_embeddings_list, dim=0)\n",
    "\n",
    "# Use the foundation decoder (spatial_model.decoder) to reconstruct the full 70-d vector.\n",
    "# Here, we assume that the first 35 values correspond to the spot's own rank vector.\n",
    "spatial_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_reconstruction = spatial_model.decoder(predicted_embeddings)\n",
    "    pred_reconstruction = pred_reconstruction.cpu().numpy()\n",
    "\n",
    "# For submission, we extract the first 35 columns (the predicted ranks for the spot).\n",
    "predicted_ranks = pred_reconstruction[:, :35]\n",
    "submission_df = pd.DataFrame(predicted_ranks, columns=cell_types)\n",
    "submission_df.insert(0, 'ID', test_df.index)\n",
    "submission_file = \"submission.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "print(f\"Submission file '{submission_file}' created!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11390004,
     "sourceId": 94147,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1618.545481,
   "end_time": "2025-03-30T14:33:42.965227",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-30T14:06:44.419746",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
