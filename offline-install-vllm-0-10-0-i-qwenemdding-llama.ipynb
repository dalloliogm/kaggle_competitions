{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef888e60",
   "metadata": {
    "papermill": {
     "duration": 0.005005,
     "end_time": "2025-09-27T16:49:53.464349",
     "exception": false,
     "start_time": "2025-09-27T16:49:53.459344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### References\n",
    "\n",
    "*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n",
    "*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n",
    "*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n",
    "*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch\n",
    "*   [https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference](https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference)\n",
    "*   https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed62583",
   "metadata": {
    "papermill": {
     "duration": 0.004306,
     "end_time": "2025-09-27T16:49:53.472963",
     "exception": false,
     "start_time": "2025-09-27T16:49:53.468657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### I want to say thanks to @neibyr for your interesting idea: [Retrieve by Qwen3Embedding](http://https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226a233",
   "metadata": {
    "papermill": {
     "duration": 0.003785,
     "end_time": "2025-09-27T16:49:53.480704",
     "exception": false,
     "start_time": "2025-09-27T16:49:53.476919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Original Notebook(1)\n",
    "https://www.kaggle.com/code/kishanvavdara/test-on-testdataset-qwenemdding-llama-lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72dcfad",
   "metadata": {
    "papermill": {
     "duration": 0.003797,
     "end_time": "2025-09-27T16:49:53.488419",
     "exception": false,
     "start_time": "2025-09-27T16:49:53.484622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# abstruct\n",
    "This notebook is offline install ver of original notebook(1).\n",
    "\n",
    "vllm==0.10.0 is installed internet-offline by jigsaw-packages2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8c5965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:49:53.496975Z",
     "iopub.status.busy": "2025-09-27T16:49:53.496756Z",
     "iopub.status.idle": "2025-09-27T16:50:39.140130Z",
     "shell.execute_reply": "2025-09-27T16:50:39.139429Z"
    },
    "papermill": {
     "duration": 45.649017,
     "end_time": "2025-09-27T16:50:39.141351",
     "exception": false,
     "start_time": "2025-09-27T16:49:53.492334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m168 packages\u001b[0m \u001b[2min 519ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m deepspeed\u001b[2m==0.17.4\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m71 packages\u001b[0m \u001b[2min 37.17s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m27 packages\u001b[0m \u001b[2min 1.86s\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m71 packages\u001b[0m \u001b[2min 404ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mauto-gptq\u001b[0m\u001b[2m==0.7.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.46.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mcbor2\u001b[0m\u001b[2m==5.7.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.10.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdeepspeed\u001b[0m\u001b[2m==0.17.4\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.116.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.10\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.1.5\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mgekko\u001b[0m\u001b[2m==1.3.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.9\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhjson\u001b[0m\u001b[2m==3.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.31.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.34.4\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.12\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogits-processor-zoo\u001b[0m\u001b[2m==0.2.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.8.4\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.9.0.13\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.6.4.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.6.80\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.4.0.6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.0.4\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.11.1.6\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.10.19\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.7.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.4.40\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.1.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.9.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.4.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.26.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.9.41\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.6.85\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.70.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.90.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moptimum\u001b[0m\u001b[2m==1.27.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.2.10\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-extra-types\u001b[0m\u001b[2m==2.10.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==24.0.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.15.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.6.4\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrouge\u001b[0m\u001b[2m==1.0.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.47.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.7.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0+cu124 (from https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.51.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.21.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.35.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.10.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.31\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.21\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 3.04s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m4 packages\u001b[0m \u001b[2min 558ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m emoji\u001b[2m==1.7.0\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m emoji\u001b[2m==1.7.0\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m emoji\u001b[2m==1.7.0\u001b[0m\r\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m emoji\u001b[2m==1.7.0\u001b[0m\r\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m emoji\u001b[2m==1.7.0\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 617ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mclean-text\u001b[0m\u001b[2m==0.6.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1memoji\u001b[0m\u001b[2m==2.14.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1memoji\u001b[0m\u001b[2m==1.7.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mftfy\u001b[0m\u001b[2m==6.3.1\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m3 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 36ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 60ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.5.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.10.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.6.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.0.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.14.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.17.1\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e48811",
   "metadata": {
    "papermill": {
     "duration": 0.013279,
     "end_time": "2025-09-27T16:50:39.171239",
     "exception": false,
     "start_time": "2025-09-27T16:50:39.157960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Test time train Qwen 2.5 0.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79532a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:50:39.198686Z",
     "iopub.status.busy": "2025-09-27T16:50:39.198224Z",
     "iopub.status.idle": "2025-09-27T16:50:39.203856Z",
     "shell.execute_reply": "2025-09-27T16:50:39.203268Z"
    },
    "papermill": {
     "duration": 0.020566,
     "end_time": "2025-09-27T16:50:39.204875",
     "exception": false,
     "start_time": "2025-09-27T16:50:39.184309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\n",
    "LORA_PATH = \"output/\"\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7638bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:50:39.232299Z",
     "iopub.status.busy": "2025-09-27T16:50:39.232106Z",
     "iopub.status.idle": "2025-09-27T16:50:39.237621Z",
     "shell.execute_reply": "2025-09-27T16:50:39.237010Z"
    },
    "papermill": {
     "duration": 0.020514,
     "end_time": "2025-09-27T16:50:39.238660",
     "exception": false,
     "start_time": "2025-09-27T16:50:39.218146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "import random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "\n",
    "    # ---------- 处理训练集 ----------\n",
    "    train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                              \"positive_example_1\",\"positive_example_2\",\n",
    "                              \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # 随机选 positive_example 和 negative_example\n",
    "    train_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"positive_example_1\"],\n",
    "        train_df[\"positive_example_2\"]\n",
    "    )\n",
    "    train_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"negative_example_1\"],\n",
    "        train_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # 删除原来的候选列\n",
    "    train_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "    flatten.append(train_df)\n",
    "\n",
    "    # ---------- 处理测试集 ----------\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                # body 用当前 positive_example\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"  # 另一个 positive\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                # negative_example 随机选\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "            # 删除原来的候选列\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # 合并所有 DataFrame\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618b9e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:50:39.266033Z",
     "iopub.status.busy": "2025-09-27T16:50:39.265845Z",
     "iopub.status.idle": "2025-09-27T16:50:39.270450Z",
     "shell.execute_reply": "2025-09-27T16:50:39.269745Z"
    },
    "papermill": {
     "duration": 0.019428,
     "end_time": "2025-09-27T16:50:39.271538",
     "exception": false,
     "start_time": "2025-09-27T16:50:39.252110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import pandas as pd\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_dataframe_to_train\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,\n",
    "        \n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=1e-4, #keep high, lora usually likes high. \n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        BASE_MODEL_PATH,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(LORA_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a551156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:50:39.299477Z",
     "iopub.status.busy": "2025-09-27T16:50:39.299280Z",
     "iopub.status.idle": "2025-09-27T16:50:39.304616Z",
     "shell.execute_reply": "2025-09-27T16:50:39.303908Z"
    },
    "papermill": {
     "duration": 0.020479,
     "end_time": "2025-09-27T16:50:39.305667",
     "exception": false,
     "start_time": "2025-09-27T16:50:39.285188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils import build_dataset\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def run_inference_on_device(df_slice):\n",
    "    \"\"\"在当前进程可见的 GPU 上跑 vLLM 推理\"\"\"\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization=\"gptq\",\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.98,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "\n",
    "    test_dataset = build_dataset(df_slice)\n",
    "    texts = test_dataset[\"prompt\"]\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "\n",
    "    log_probs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    predictions = pd.DataFrame(log_probs)[[POSITIVE_ANSWER, NEGATIVE_ANSWER]]\n",
    "    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def worker(device_id, df_slice, return_dict):\n",
    "    # 限制该进程只看到一张 GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n",
    "    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n",
    "\n",
    "    preds = run_inference_on_device(df_slice)\n",
    "    return_dict[device_id] = preds\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "\n",
    "    # 随机选择例子\n",
    "    test_dataframe[\"positive_example\"] = test_dataframe.apply(\n",
    "        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]),\n",
    "        axis=1\n",
    "    )\n",
    "    test_dataframe[\"negative_example\"] = test_dataframe.apply(\n",
    "        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]),\n",
    "        axis=1\n",
    "    )\n",
    "    test_dataframe = test_dataframe.drop(\n",
    "        columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # 切分数据\n",
    "    mid = len(test_dataframe) // 2\n",
    "    df0 = test_dataframe.iloc[:mid].reset_index(drop=True)\n",
    "    df1 = test_dataframe.iloc[mid:].reset_index(drop=True)\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "\n",
    "    # 两个进程并行\n",
    "    p0 = mp.Process(target=worker, args=(0, df0, return_dict))\n",
    "    p1 = mp.Process(target=worker, args=(1, df1, return_dict))\n",
    "    p0.start()\n",
    "    p1.start()\n",
    "    p0.join()\n",
    "    p1.join()\n",
    "\n",
    "    # 合并结果\n",
    "    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\n",
    "\n",
    "    # 构建 submission\n",
    "    submission = predictions[[\"row_id\", POSITIVE_ANSWER]].rename(columns={POSITIVE_ANSWER: \"rule_violation\"})\n",
    "    rq = submission['rule_violation'].rank(method='average') / (len(submission) + 1)\n",
    "    submission['rule_violation'] = rq\n",
    "\n",
    "    submission.to_csv(\"submission_qwen.csv\", index=False)\n",
    "    print(\"✅ Saved submission_qwen.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6295b5c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:50:39.333127Z",
     "iopub.status.busy": "2025-09-27T16:50:39.332694Z",
     "iopub.status.idle": "2025-09-27T16:50:39.337263Z",
     "shell.execute_reply": "2025-09-27T16:50:39.336593Z"
    },
    "papermill": {
     "duration": 0.019336,
     "end_time": "2025-09-27T16:50:39.338292",
     "exception": false,
     "start_time": "2025-09-27T16:50:39.318956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 64\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3b1b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:50:39.365898Z",
     "iopub.status.busy": "2025-09-27T16:50:39.365563Z",
     "iopub.status.idle": "2025-09-27T17:01:10.504802Z",
     "shell.execute_reply": "2025-09-27T17:01:10.504048Z"
    },
    "papermill": {
     "duration": 631.154598,
     "end_time": "2025-09-27T17:01:10.506292",
     "exception": false,
     "start_time": "2025-09-27T16:50:39.351694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-27 16:50:54,807] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-27 16:50:59,040] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "2025-09-27 16:51:00.449344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758991860.643492     190 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758991860.701365     190 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0927 16:51:12.804000 190 torch/distributed/run.py:766] \r\n",
      "W0927 16:51:12.804000 190 torch/distributed/run.py:766] *****************************************\r\n",
      "W0927 16:51:12.804000 190 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0927 16:51:12.804000 190 torch/distributed/run.py:766] *****************************************\r\n",
      "[W927 16:51:20.118553955 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 16:51:28.120869273 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "2025-09-27 16:51:35.277764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-09-27 16:51:35.277777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758991895.299177     247 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758991895.299179     248 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758991895.306430     247 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1758991895.307056     248 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "[2025-09-27 16:51:42,459] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-27 16:51:42,459] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-27 16:51:43,737] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "[2025-09-27 16:51:43,746] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "[2025-09-27 16:51:43,760] [INFO] [comm.py:821:init_distributed] cdb=None\r\n",
      "[2025-09-27 16:51:43,760] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n",
      "[2025-09-27 16:51:43,769] [INFO] [comm.py:821:init_distributed] cdb=None\r\n",
      "[W927 16:51:51.071539126 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 16:51:51.081487236 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 16:51:59.073814762 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 16:52:07.076259269 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_fwd\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_bwd\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_fwd(cast_inputs=torch.float16)\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_fwd\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_bwd\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_fwd(cast_inputs=torch.float16)\r\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\r\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\r\n",
      "Adding EOS to train dataset: 100%|█| 2049/2049 [00:00<00:00, 14020.91 examples/s\r\n",
      "Tokenizing train dataset: 100%|█████| 2049/2049 [00:03<00:00, 562.19 examples/s]\r\n",
      "Truncating train dataset: 100%|██| 2049/2049 [00:00<00:00, 105626.99 examples/s]\r\n",
      "Adding EOS to train dataset:   0%|              | 0/2049 [00:00<?, ? examples/s]The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\r\n",
      "Adding EOS to train dataset: 100%|█| 2049/2049 [00:00<00:00, 22144.00 examples/s\r\n",
      "Tokenizing train dataset: 100%|█████| 2049/2049 [00:02<00:00, 700.49 examples/s]\r\n",
      "Truncating train dataset: 100%|██| 2049/2049 [00:00<00:00, 135983.05 examples/s]\r\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\r\n",
      "[2025-09-27 16:52:19,986] [WARNING] [engine.py:1373:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\r\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\r\n",
      "  0%|                                                    | 0/65 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\r\n",
      "{'loss': 5.8852, 'grad_norm': 2.524470806121826, 'learning_rate': 9.698463103929542e-05, 'num_tokens': 68410.0, 'mean_token_accuracy': 0.3234375, 'epoch': 0.16}\r\n",
      "{'loss': 0.4486, 'grad_norm': 10.13871955871582, 'learning_rate': 8.308429187984297e-05, 'num_tokens': 136025.0, 'mean_token_accuracy': 0.7578125, 'epoch': 0.31}\r\n",
      "{'loss': 0.3504, 'grad_norm': 2.3450212478637695, 'learning_rate': 6.112604669781572e-05, 'num_tokens': 204188.0, 'mean_token_accuracy': 0.7890625, 'epoch': 0.47}\r\n",
      "{'loss': 0.3167, 'grad_norm': 1.7738429307937622, 'learning_rate': 3.6457976592849754e-05, 'num_tokens': 271365.0, 'mean_token_accuracy': 0.828125, 'epoch': 0.62}\r\n",
      "{'loss': 0.3279, 'grad_norm': 2.9931368827819824, 'learning_rate': 1.5088159095696363e-05, 'num_tokens': 338855.0, 'mean_token_accuracy': 0.81875, 'epoch': 0.78}\r\n",
      "{'loss': 0.3066, 'grad_norm': 3.1622869968414307, 'learning_rate': 2.221359710692961e-06, 'num_tokens': 407756.0, 'mean_token_accuracy': 0.8203125, 'epoch': 0.93}\r\n",
      "{'train_runtime': 520.8561, 'train_samples_per_second': 3.934, 'train_steps_per_second': 0.125, 'train_loss': 1.198622855773339, 'num_tokens': 434309.0, 'mean_token_accuracy': 0.8161764705882353, 'epoch': 1.0}\r\n",
      "100%|███████████████████████████████████████████| 65/65 [08:40<00:00,  8.01s/it]\r\n",
      "[rank0]:[W927 17:01:05.692703811 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file accelerate_config.yaml train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292ebba6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:01:10.546840Z",
     "iopub.status.busy": "2025-09-27T17:01:10.546566Z",
     "iopub.status.idle": "2025-09-27T17:02:23.551490Z",
     "shell.execute_reply": "2025-09-27T17:02:23.550736Z"
    },
    "papermill": {
     "duration": 73.026043,
     "end_time": "2025-09-27T17:02:23.552908",
     "exception": false,
     "start_time": "2025-09-27T17:01:10.526865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Worker 0] Running on GPU 0, data size=5\r\n",
      "[Worker 1] Running on GPU 1, data size=5\r\n",
      "2025-09-27 17:01:16.575739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-09-27 17:01:16.577720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758992476.609106     425 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758992476.610707     427 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758992476.622328     427 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1758992476.622514     425 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 09-27 17:01:20 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "INFO 09-27 17:01:20 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "INFO 09-27 17:01:36 [config.py:1604] Using max model len 2836\r\n",
      "INFO 09-27 17:01:36 [config.py:1604] Using max model len 2836\r\n",
      "WARNING 09-27 17:01:38 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 09-27 17:01:38 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 09-27 17:01:39 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "WARNING 09-27 17:01:39 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 09-27 17:01:39 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2836, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "INFO 09-27 17:01:39 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2836, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "INFO 09-27 17:01:39 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-27 17:01:39 [cuda.py:395] Using XFormers backend.\r\n",
      "INFO 09-27 17:01:39 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-27 17:01:39 [cuda.py:395] Using XFormers backend.\r\n",
      "[W927 17:01:50.029241184 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 17:01:50.029439331 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 17:02:00.039735211 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 17:02:00.039932708 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 09-27 17:02:00 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "INFO 09-27 17:02:00 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1...\r\n",
      "INFO 09-27 17:02:00 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "INFO 09-27 17:02:00 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.89it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.91it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.89it/s]\r\n",
      "\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.90it/s]\r\n",
      "\r\n",
      "INFO 09-27 17:02:01 [default_loader.py:262] Loading weights took 0.27 seconds\r\n",
      "INFO 09-27 17:02:01 [default_loader.py:262] Loading weights took 0.27 seconds\r\n",
      "INFO 09-27 17:02:01 [logger.py:65] Using PunicaWrapperGPU.\r\n",
      "INFO 09-27 17:02:01 [logger.py:65] Using PunicaWrapperGPU.\r\n",
      "INFO 09-27 17:02:02 [model_runner.py:1115] Model loading took 0.4969 GiB and 0.521374 seconds\r\n",
      "INFO 09-27 17:02:02 [model_runner.py:1115] Model loading took 0.4969 GiB and 0.526043 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-27 17:02:09 [worker.py:295] model weights take 0.50GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 12.51GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-27 17:02:09 [worker.py:295] model weights take 0.50GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 12.51GiB.\r\n",
      "INFO 09-27 17:02:10 [executor_base.py:113] # cuda blocks: 68319, # CPU blocks: 21845\r\n",
      "INFO 09-27 17:02:10 [executor_base.py:118] Maximum concurrency for 2836 tokens per request: 385.44x\r\n",
      "INFO 09-27 17:02:10 [executor_base.py:113] # cuda blocks: 68319, # CPU blocks: 21845\r\n",
      "INFO 09-27 17:02:10 [executor_base.py:118] Maximum concurrency for 2836 tokens per request: 385.44x\r\n",
      "INFO 09-27 17:02:15 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 12.77 seconds\r\n",
      "INFO 09-27 17:02:15 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 12.89 seconds\r\n",
      "Adding requests: 100%|████████████████████████████| 5/5 [00:00<00:00,  7.76it/s]\r\n",
      "Processed prompts: 100%|█| 5/5 [00:00<00:00, 21.41it/s, est. speed input: 4275.5\r\n",
      "Adding requests: 100%|████████████████████████████| 5/5 [00:00<00:00,  7.46it/s]\r\n",
      "Processed prompts: 100%|█| 5/5 [00:00<00:00, 21.59it/s, est. speed input: 5034.8\r\n",
      "✅ Saved submission_qwen.csv\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12dc1a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:02:23.596091Z",
     "iopub.status.busy": "2025-09-27T17:02:23.595857Z",
     "iopub.status.idle": "2025-09-27T17:02:23.713959Z",
     "shell.execute_reply": "2025-09-27T17:02:23.713298Z"
    },
    "papermill": {
     "duration": 0.141044,
     "end_time": "2025-09-27T17:02:23.715086",
     "exception": false,
     "start_time": "2025-09-27T17:02:23.574042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id,rule_violation\r\n",
      "2029,0.18181818181818182\r\n",
      "2030,0.8181818181818182\r\n",
      "2031,0.36363636363636365\r\n",
      "2032,0.2727272727272727\r\n",
      "2033,0.7272727272727273\r\n",
      "2034,0.09090909090909091\r\n",
      "2035,0.6363636363636364\r\n",
      "2036,0.5454545454545454\r\n",
      "2037,0.45454545454545453\r\n"
     ]
    }
   ],
   "source": [
    "!head submission_qwen.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962ebf3",
   "metadata": {
    "papermill": {
     "duration": 0.019835,
     "end_time": "2025-09-27T17:02:23.755009",
     "exception": false,
     "start_time": "2025-09-27T17:02:23.735174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Qwen2.5 14B GPTQ Int4 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece927ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:02:23.796096Z",
     "iopub.status.busy": "2025-09-27T17:02:23.795876Z",
     "iopub.status.idle": "2025-09-27T17:02:23.799358Z",
     "shell.execute_reply": "2025-09-27T17:02:23.798665Z"
    },
    "papermill": {
     "duration": 0.025545,
     "end_time": "2025-09-27T17:02:23.800507",
     "exception": false,
     "start_time": "2025-09-27T17:02:23.774962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0a284f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:02:23.841776Z",
     "iopub.status.busy": "2025-09-27T17:02:23.841585Z",
     "iopub.status.idle": "2025-09-27T17:02:23.847200Z",
     "shell.execute_reply": "2025-09-27T17:02:23.846615Z"
    },
    "papermill": {
     "duration": 0.02751,
     "end_time": "2025-09-27T17:02:23.848233",
     "exception": false,
     "start_time": "2025-09-27T17:02:23.820723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing infer_qwen.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile infer_qwen.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import torch\n",
    "import vllm\n",
    "import numpy as np\n",
    "from vllm.lora.request import LoRARequest\n",
    "import argparse\n",
    "from scipy.special import softmax\n",
    "df = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n",
    "\n",
    "MODEL_NAME = \"/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1\"\n",
    "LORA_PATH = \"/kaggle/input/lora_14b_gptq_1epoch_r32/keras/default/1\"\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        MODEL_NAME,\n",
    "        # quantization='awq',\n",
    "        quantization='gptq',\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        gpu_memory_utilization=0.95,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=32\n",
    "    )\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    SYS_PROMPT = \"\"\"\n",
    "You are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\n",
    "\"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = f\"\"\"\n",
    "    r/{row.subreddit}\n",
    "    Rule: {row.rule}\n",
    "    \n",
    "    1) {row.positive_example_1}\n",
    "    Violation: Yes\n",
    "    \n",
    "    2) {row.positive_example_2}\n",
    "    Violation: Yes\n",
    "    \n",
    "    3) {row.negative_example_1}\n",
    "    Violation: No\n",
    "    \n",
    "    4) {row.negative_example_2}\n",
    "    Violation: No\n",
    "    \n",
    "    5) {row.body}\n",
    "    \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    \n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        ) + \"Answer:\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    df[\"prompt\"] = prompts\n",
    "    \n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=['Yes','No'])\n",
    "    outputs = llm.generate(\n",
    "        prompts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "    logprobs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    logit_matrix = pd.DataFrame(logprobs)[['Yes','No']]\n",
    "    df = pd.concat([df, logit_matrix], axis=1)\n",
    "    \n",
    "    df[['Yes',\"No\"]] = df[['Yes',\"No\"]].apply(lambda x: softmax(x.values), axis=1, result_type=\"expand\")\n",
    "    df[\"pred\"] = df[\"Yes\"]\n",
    "    df['rule_violation'] = df[\"pred\"]\n",
    "    df[['row_id', 'rule_violation']].to_csv(\"submission_qwen14b.csv\",index=False)\n",
    "    pd.read_csv('submission_qwen14b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc32c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:02:23.889891Z",
     "iopub.status.busy": "2025-09-27T17:02:23.889691Z",
     "iopub.status.idle": "2025-09-27T17:05:28.482800Z",
     "shell.execute_reply": "2025-09-27T17:05:28.482053Z"
    },
    "papermill": {
     "duration": 184.615773,
     "end_time": "2025-09-27T17:05:28.484253",
     "exception": false,
     "start_time": "2025-09-27T17:02:23.868480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-27 17:02:29.669298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758992549.692472     655 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758992549.699291     655 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 09-27 17:02:33 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "INFO 09-27 17:02:48 [config.py:1604] Using max model len 2836\r\n",
      "WARNING 09-27 17:02:49 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 09-27 17:02:49 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 09-27 17:02:49 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2836, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "WARNING 09-27 17:02:50 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\r\n",
      "WARNING 09-27 17:02:50 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 09-27 17:02:50 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-27 17:02:50 [cuda.py:395] Using XFormers backend.\r\n",
      "2025-09-27 17:02:55.752567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758992575.778503     685 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758992575.785251     685 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 09-27 17:02:59 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:03:00 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:03:00 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:03:00 [cuda.py:395] Using XFormers backend.\r\n",
      "[W927 17:03:11.882852024 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 17:03:11.238038284 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 17:03:21.893139561 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W927 17:03:31.903222526 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:03:31 [__init__.py:1375] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:03:31 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 09-27 17:03:31 [__init__.py:1375] Found nccl from library libnccl.so.2\r\n",
      "INFO 09-27 17:03:31 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 09-27 17:03:31 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 09-27 17:03:55 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:03:55 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 09-27 17:03:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_43ca182b'), local_subscribe_addr='ipc:///tmp/e44b915a-2067-4173-b83f-52cceaf9132f', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "INFO 09-27 17:03:55 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:03:55 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\r\n",
      "INFO 09-27 17:03:55 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:03:56 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:08<00:16,  8.45s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:27<00:14, 14.60s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:50<00:00, 18.42s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:50<00:00, 16.77s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:04:46 [default_loader.py:262] Loading weights took 49.99 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:04:46 [logger.py:65] Using PunicaWrapperGPU.\r\n",
      "INFO 09-27 17:04:47 [default_loader.py:262] Loading weights took 50.64 seconds\r\n",
      "INFO 09-27 17:04:47 [logger.py:65] Using PunicaWrapperGPU.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:04:47 [model_runner.py:1115] Model loading took 4.8516 GiB and 50.514284 seconds\r\n",
      "INFO 09-27 17:04:48 [model_runner.py:1115] Model loading took 4.8516 GiB and 51.124931 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 09-27 17:04:59 [worker.py:295] model weights take 4.85GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.30GiB; the rest of the memory reserved for KV Cache is 8.73GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-27 17:04:59 [worker.py:295] model weights take 4.85GiB; non_torch_memory takes 0.13GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 7.61GiB.\r\n",
      "INFO 09-27 17:04:59 [executor_base.py:113] # cuda blocks: 5195, # CPU blocks: 2730\r\n",
      "INFO 09-27 17:04:59 [executor_base.py:118] Maximum concurrency for 2836 tokens per request: 29.31x\r\n",
      "INFO 09-27 17:05:04 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 16.69 seconds\r\n",
      "Adding requests: 100%|██████████████████████████| 10/10 [00:00<00:00, 10.98it/s]\r\n",
      "Processed prompts: 100%|█| 10/10 [00:14<00:00,  1.41s/it, est. speed input: 247.\r\n",
      "INFO 09-27 17:05:23 [multiproc_worker_utils.py:125] Killing local vLLM worker processes\r\n",
      "[rank0]:[W927 17:05:24.804151726 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "# %cd /tmp\n",
    "!python infer_qwen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa9bff",
   "metadata": {
    "papermill": {
     "duration": 0.02181,
     "end_time": "2025-09-27T17:05:28.531278",
     "exception": false,
     "start_time": "2025-09-27T17:05:28.509468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Qwen3 0.6b Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d8d8e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:05:28.575886Z",
     "iopub.status.busy": "2025-09-27T17:05:28.575593Z",
     "iopub.status.idle": "2025-09-27T17:05:28.896129Z",
     "shell.execute_reply": "2025-09-27T17:05:28.895509Z"
    },
    "papermill": {
     "duration": 0.344627,
     "end_time": "2025-09-27T17:05:28.897488",
     "exception": false,
     "start_time": "2025-09-27T17:05:28.552861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "753d77df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:05:28.945321Z",
     "iopub.status.busy": "2025-09-27T17:05:28.944878Z",
     "iopub.status.idle": "2025-09-27T17:05:28.950091Z",
     "shell.execute_reply": "2025-09-27T17:05:28.949427Z"
    },
    "papermill": {
     "duration": 0.030438,
     "end_time": "2025-09-27T17:05:28.951272",
     "exception": false,
     "start_time": "2025-09-27T17:05:28.920834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "EMBDEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\n",
    "MODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/blob/main/config_sentence_transformers.json\n",
    "EMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:\"\n",
    "\n",
    "CLEAN_TEXT = True\n",
    "TOP_K = 2000\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfccd5f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:05:28.997889Z",
     "iopub.status.busy": "2025-09-27T17:05:28.997658Z",
     "iopub.status.idle": "2025-09-27T17:05:29.002761Z",
     "shell.execute_reply": "2025-09-27T17:05:29.002113Z"
    },
    "papermill": {
     "duration": 0.029495,
     "end_time": "2025-09-27T17:05:29.003839",
     "exception": false,
     "start_time": "2025-09-27T17:05:28.974344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "import torch.distributed as dist\n",
    "\n",
    "from datasets import Dataset\n",
    "from cleantext import clean\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from constants import CLEAN_TEXT\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n",
    "\n",
    "\n",
    "def cleaner(text):\n",
    "    return clean(\n",
    "        text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=True,\n",
    "        lower=False,\n",
    "        no_line_breaks=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=False,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        lang=\"en\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.6, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "    flatten.append(train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]])\n",
    "    \n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[f\"{violation_type}_example_{i}\", \"rule\", \"subreddit\"]].copy()\n",
    "            sub_dataset = sub_dataset.rename(columns={f\"{violation_type}_example_{i}\": \"body\"})\n",
    "            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    dataframe = pd.concat(flatten, axis=0)    \n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def prepare_dataframe(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    \n",
    "    if CLEAN_TEXT:\n",
    "        tqdm.pandas(desc=\"cleaner\")\n",
    "        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n",
    "\n",
    "    if \"rule_violation\" in dataframe.columns:\n",
    "        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: 1,\n",
    "                0: -1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d30ac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:05:29.049877Z",
     "iopub.status.busy": "2025-09-27T17:05:29.049690Z",
     "iopub.status.idle": "2025-09-27T17:05:29.054869Z",
     "shell.execute_reply": "2025-09-27T17:05:29.054274Z"
    },
    "papermill": {
     "duration": 0.029621,
     "end_time": "2025-09-27T17:05:29.055816",
     "exception": false,
     "start_time": "2025-09-27T17:05:29.026195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing semantic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile semantic.py\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search, dot_score\n",
    "from tqdm.auto import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "from utils import get_dataframe_to_train, prepare_dataframe\n",
    "from constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH\n",
    "\n",
    "\n",
    "\n",
    "def get_scores(test_dataframe):\n",
    "    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "    \n",
    "    # Load adapter configuration and model\n",
    "    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "\n",
    "    # 4. Tạo lại SentenceTransformer từ encoder đã merge\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n",
    "\n",
    "    print('Done loading model!')\n",
    "\n",
    "    result = []\n",
    "    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=f\"Generate scores for each rule\"):\n",
    "        test_dataframe_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_dataframe_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_dataframe_part = corpus_dataframe_part.reset_index(names=\"row_id\")\n",
    "        \n",
    "        query_embeddings = embedding_model.encode(\n",
    "            sentences=test_dataframe_part[\"prompt\"].tolist(),\n",
    "            prompt=EMBEDDING_MODEL_QUERY,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        document_embeddings = embedding_model.encode(\n",
    "            sentences=corpus_dataframe_part[\"prompt\"].tolist(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        test_dataframe_part[\"semantic\"] = semantic_search(\n",
    "            query_embeddings,\n",
    "            document_embeddings,\n",
    "            top_k=TOP_K,\n",
    "            score_function=dot_score,\n",
    "        )\n",
    "        def get_score(semantic):\n",
    "            semantic = pd.DataFrame(semantic)\n",
    "            semantic = semantic.merge(\n",
    "                corpus_dataframe_part[[\"row_id\", \"rule_violation\"]],\n",
    "                how=\"left\",\n",
    "                left_on=\"corpus_id\",\n",
    "                right_on=\"row_id\",\n",
    "            )\n",
    "            semantic[\"score\"] = semantic[\"score\"]*semantic[\"rule_violation\"]\n",
    "            return semantic[\"score\"].sum()\n",
    "            \n",
    "        tqdm.pandas(desc=f\"Add label for {rule=}\")\n",
    "        test_dataframe_part[\"rule_violation\"] = test_dataframe_part[\"semantic\"].progress_apply(get_score)\n",
    "        result.append(test_dataframe_part[[\"row_id\", \"rule_violation\"]].copy())\n",
    "        \n",
    "    submission = pd.concat(result, axis=0)\n",
    "    return submission\n",
    "\n",
    "\n",
    "def generate_submission():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    test_dataframe = prepare_dataframe(test_dataframe)\n",
    "    \n",
    "    submission = get_scores(test_dataframe)\n",
    "    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n",
    "    submission.to_csv(\"submission_qwen3.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_submission()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3312f2c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:05:29.103192Z",
     "iopub.status.busy": "2025-09-27T17:05:29.102990Z",
     "iopub.status.idle": "2025-09-27T17:07:46.819244Z",
     "shell.execute_reply": "2025-09-27T17:07:46.818342Z"
    },
    "papermill": {
     "duration": 137.74259,
     "end_time": "2025-09-27T17:07:46.821351",
     "exception": false,
     "start_time": "2025-09-27T17:05:29.078761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-27 17:05:35.889507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758992735.913024     948 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758992735.919973     948 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "cleaner: 100%|████████████████████████████████| 10/10 [00:00<00:00, 1247.12it/s]\r\n",
      "cleaner: 100%|████████████████████████████| 1969/1969 [00:00<00:00, 4642.44it/s]\r\n",
      "[2025-09-27 17:05:49,652] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-27 17:05:51,062] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "Done loading model!\r\n",
      "Generate scores for each rule:   0%|                      | 0/2 [00:00<?, ?it/s]\r\n",
      "Batches:   0%|                                            | 0/1 [00:00<?, ?it/s]\u001b[A\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  1.51it/s]\r\n",
      "\r\n",
      "Batches:   0%|                                            | 0/8 [00:00<?, ?it/s]\u001b[A\r\n",
      "Batches:  12%|████▌                               | 1/8 [00:02<00:17,  2.48s/it]\u001b[A\r\n",
      "Batches:  25%|█████████                           | 2/8 [00:08<00:27,  4.59s/it]\u001b[A\r\n",
      "Batches:  38%|█████████████▌                      | 3/8 [00:11<00:18,  3.76s/it]\u001b[A\r\n",
      "Batches:  50%|██████████████████                  | 4/8 [00:13<00:12,  3.06s/it]\u001b[A\r\n",
      "Batches:  62%|██████████████████████▌             | 5/8 [00:14<00:07,  2.55s/it]\u001b[A\r\n",
      "Batches:  75%|███████████████████████████         | 6/8 [00:16<00:04,  2.13s/it]\u001b[A\r\n",
      "Batches:  88%|███████████████████████████████▌    | 7/8 [00:17<00:01,  1.78s/it]\u001b[A\r\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:18<00:00,  2.29s/it]\r\n",
      "\r\n",
      "Add label for rule='No Advertising: Spam, referral links, unsolicited advertisin\r\n",
      "Generate scores for each rule:  50%|███████       | 1/2 [00:19<00:19, 19.24s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 12.30it/s]\r\n",
      "\r\n",
      "Batches:   0%|                                            | 0/8 [00:00<?, ?it/s]\u001b[A\r\n",
      "Batches:  12%|████▌                               | 1/8 [00:01<00:13,  1.90s/it]\u001b[A\r\n",
      "Batches:  25%|█████████                           | 2/8 [00:07<00:23,  3.99s/it]\u001b[A\r\n",
      "Batches:  38%|█████████████▌                      | 3/8 [00:11<00:20,  4.19s/it]\u001b[A\r\n",
      "Batches:  50%|██████████████████                  | 4/8 [00:14<00:15,  3.76s/it]\u001b[A\r\n",
      "Batches:  62%|██████████████████████▌             | 5/8 [00:17<00:10,  3.33s/it]\u001b[A\r\n",
      "Batches:  75%|███████████████████████████         | 6/8 [00:19<00:05,  2.89s/it]\u001b[A\r\n",
      "Batches:  88%|███████████████████████████████▌    | 7/8 [00:21<00:02,  2.57s/it]\u001b[A\r\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:22<00:00,  2.86s/it]\r\n",
      "\r\n",
      "Add label for rule='No legal advice: Do not offer or request legal advice.': 100\r\n",
      "Generate scores for each rule: 100%|██████████████| 2/2 [00:43<00:00, 21.54s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python semantic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478c963",
   "metadata": {
    "papermill": {
     "duration": 0.023337,
     "end_time": "2025-09-27T17:07:46.869862",
     "exception": false,
     "start_time": "2025-09-27T17:07:46.846525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. ENSEMBLE RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8b9d0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:07:46.917811Z",
     "iopub.status.busy": "2025-09-27T17:07:46.917535Z",
     "iopub.status.idle": "2025-09-27T17:07:46.937734Z",
     "shell.execute_reply": "2025-09-27T17:07:46.937012Z"
    },
    "papermill": {
     "duration": 0.045832,
     "end_time": "2025-09-27T17:07:46.938982",
     "exception": false,
     "start_time": "2025-09-27T17:07:46.893150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "q = pd.read_csv('submission_qwen.csv')\n",
    "l = pd.read_csv('submission_qwen3.csv')\n",
    "m = pd.read_csv('submission_qwen14b.csv')\n",
    "\n",
    "\n",
    "rq = q['rule_violation'].rank(method='average') / (len(q)+1)\n",
    "rl = l['rule_violation'].rank(method='average') / (len(l)+1)\n",
    "rm = m['rule_violation'].rank(method='average') / (len(m)+1)\n",
    "\n",
    "#blend = 0.5*rq + 0.3*rl + 0.2*rm 　#original \n",
    "blend = 0.5*rq + 0.3*rl + 0.2*rm   # or tune the rank-weights with a tiny grid using OOF\n",
    "q['rule_violation'] = blend\n",
    "q.to_csv('/kaggle/working/submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e313e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:07:46.987963Z",
     "iopub.status.busy": "2025-09-27T17:07:46.987717Z",
     "iopub.status.idle": "2025-09-27T17:07:47.007916Z",
     "shell.execute_reply": "2025-09-27T17:07:47.007082Z"
    },
    "papermill": {
     "duration": 0.045964,
     "end_time": "2025-09-27T17:07:47.009129",
     "exception": false,
     "start_time": "2025-09-27T17:07:46.963165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2029</td>\n",
       "      <td>0.190909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2030</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2031</td>\n",
       "      <td>0.554545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2032</td>\n",
       "      <td>0.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2033</td>\n",
       "      <td>0.763636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2034</td>\n",
       "      <td>0.127273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2035</td>\n",
       "      <td>0.672727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2036</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2037</td>\n",
       "      <td>0.372727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2038</td>\n",
       "      <td>0.736364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  rule_violation\n",
       "0    2029        0.190909\n",
       "1    2030        0.772727\n",
       "2    2031        0.554545\n",
       "3    2032        0.409091\n",
       "4    2033        0.763636\n",
       "5    2034        0.127273\n",
       "6    2035        0.672727\n",
       "7    2036        0.400000\n",
       "8    2037        0.372727\n",
       "9    2038        0.736364"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('/kaggle/working/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01055e7c",
   "metadata": {
    "papermill": {
     "duration": 0.023255,
     "end_time": "2025-09-27T17:07:47.056325",
     "exception": false,
     "start_time": "2025-09-27T17:07:47.033070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 8044304,
     "sourceId": 12726948,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8067935,
     "sourceId": 12762469,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 252850661,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 252853424,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 259545323,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 145960,
     "sourceId": 171496,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 146086,
     "sourceId": 171638,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 368803,
     "modelInstanceId": 347541,
     "sourceId": 426330,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 429004,
     "modelInstanceId": 411182,
     "sourceId": 523492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1078.130191,
   "end_time": "2025-09-27T17:07:47.397394",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-27T16:49:49.267203",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
