{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389f1e94",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-14T16:37:32.260612Z",
     "iopub.status.busy": "2025-11-14T16:37:32.260326Z",
     "iopub.status.idle": "2025-11-14T16:37:34.127685Z",
     "shell.execute_reply": "2025-11-14T16:37:34.126730Z"
    },
    "papermill": {
     "duration": 1.872116,
     "end_time": "2025-11-14T16:37:34.129025",
     "exception": false,
     "start_time": "2025-11-14T16:37:32.256909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/esm-2/keras/esm2_t6_8m/1/config.json\n",
      "/kaggle/input/esm-2/keras/esm2_t6_8m/1/preprocessor.json\n",
      "/kaggle/input/esm-2/keras/esm2_t6_8m/1/tokenizer.json\n",
      "/kaggle/input/esm-2/keras/esm2_t6_8m/1/metadata.json\n",
      "/kaggle/input/esm-2/keras/esm2_t6_8m/1/model.weights.h5\n",
      "/kaggle/input/esm-2/keras/esm2_t6_8m/1/task.json\n",
      "/kaggle/input/esm-2/keras/esm2_t6_8m/1/task.weights.h5\n",
      "/kaggle/input/esm-2/keras/esm2_t6_8m/1/assets/tokenizer/vocabulary.txt\n",
      "/kaggle/input/cafa-6-protein-function-prediction/sample_submission.tsv\n",
      "/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\n",
      "/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\n",
      "/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\n",
      "/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\n",
      "/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\n",
      "/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\n",
      "/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe6b32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T16:37:34.135265Z",
     "iopub.status.busy": "2025-11-14T16:37:34.134921Z",
     "iopub.status.idle": "2025-11-14T17:04:35.708869Z",
     "shell.execute_reply": "2025-11-14T17:04:35.708153Z"
    },
    "papermill": {
     "duration": 1621.57864,
     "end_time": "2025-11-14T17:04:35.710178",
     "exception": false,
     "start_time": "2025-11-14T16:37:34.131538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 16:37:40.498898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763138260.952377      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763138261.057032      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CAFA-6 PROTEIN FUNCTION PREDICTION - V2 (INCREMENTAL IMPROVEMENTS)\n",
      "================================================================================\n",
      "[    0.0s] INFO  | Loading FASTA sequences...\n",
      "[    3.2s] INFO  | Loaded 82,404 training sequences\n",
      "[    3.2s] INFO  | Loaded 224,309 test sequences\n",
      "[    3.7s] INFO  | Loaded 537,027 annotations for 82,404 proteins\n",
      "[    3.7s] INFO  | Loaded 40,122 IA weights\n",
      "[    3.7s] INFO  | Parsing GO ontology hierarchy...\n",
      "[    4.3s] INFO  | Parsed 40,121 GO terms\n",
      "[    4.3s] INFO  | Propagating labels...\n",
      "[   24.7s] PROG  | Propagated 25,000/82,404 proteins\n",
      "[   25.7s] PROG  | Propagated 50,000/82,404 proteins\n",
      "[   26.6s] PROG  | Propagated 75,000/82,404 proteins\n",
      "[   26.9s] INFO  | Before: 537,027, After: 3,564,990\n",
      "[   26.9s] INFO  | Selecting labels...\n",
      "[   27.4s] INFO  | Labels: MF=2000, BP=2000, CC=2000, Total=6000\n",
      "[  190.4s] INFO  | Training proteins: 82,404\n",
      "[  190.4s] INFO  | Extracting features for training set...\n",
      "[  190.4s] INFO  |   Computing TF-IDF...\n",
      "[  231.0s] INFO  |     TF-IDF: (82404, 8558)\n",
      "[  231.5s] INFO  |   Computing AA composition...\n",
      "[  236.5s] INFO  |   Computing dipeptide composition...\n",
      "[  248.8s] INFO  |   Computing physicochemical properties...\n",
      "[  270.1s] INFO  | Combined features: (82404, 8626)\n",
      "[  283.6s] INFO  | Preparing labels...\n",
      "[  287.9s] INFO  | Label matrix: (82404, 6000), sparsity: 99.35%\n",
      "[  287.9s] INFO  | Building model with IA-weighted loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763138566.171559      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1763138566.172355      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  291.4s] INFO  | Model params: 17,047,536\n",
      "[  291.4s] INFO  | Splitting data and training...\n",
      "[  292.8s] INFO  | Train: (70043, 8626), Val: (12361, 8626)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763138584.026347      64 service.cc:148] XLA service 0x7e7ea40165a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1763138584.027753      64 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1763138584.027775      64 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1763138584.582739      64 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1763138588.041620      64 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2189/2189 - 31s - 14ms/step - loss: 0.0729 - precision: 0.0148 - recall: 0.1212 - val_loss: 0.0428 - val_precision: 0.0152 - val_recall: 0.1127 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0322 - precision: 0.0158 - recall: 0.1175 - val_loss: 0.0320 - val_precision: 0.0179 - val_recall: 0.1332 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0269 - precision: 0.0177 - recall: 0.1320 - val_loss: 0.0246 - val_precision: 0.0208 - val_recall: 0.1560 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0223 - precision: 0.0195 - recall: 0.1457 - val_loss: 0.0206 - val_precision: 0.0221 - val_recall: 0.1658 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0191 - precision: 0.0213 - recall: 0.1593 - val_loss: 0.0185 - val_precision: 0.0228 - val_recall: 0.1714 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0172 - precision: 0.0228 - recall: 0.1709 - val_loss: 0.0173 - val_precision: 0.0238 - val_recall: 0.1791 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0160 - precision: 0.0242 - recall: 0.1818 - val_loss: 0.0167 - val_precision: 0.0243 - val_recall: 0.1828 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0153 - precision: 0.0254 - recall: 0.1913 - val_loss: 0.0163 - val_precision: 0.0246 - val_recall: 0.1858 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0148 - precision: 0.0265 - recall: 0.1996 - val_loss: 0.0162 - val_precision: 0.0250 - val_recall: 0.1886 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0145 - precision: 0.0276 - recall: 0.2080 - val_loss: 0.0161 - val_precision: 0.0254 - val_recall: 0.1916 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0143 - precision: 0.0286 - recall: 0.2156 - val_loss: 0.0161 - val_precision: 0.0255 - val_recall: 0.1926 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0141 - precision: 0.0294 - recall: 0.2224 - val_loss: 0.0162 - val_precision: 0.0258 - val_recall: 0.1942 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0140 - precision: 0.0302 - recall: 0.2282 - val_loss: 0.0162 - val_precision: 0.0262 - val_recall: 0.1977 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0133 - precision: 0.0325 - recall: 0.2463 - val_loss: 0.0158 - val_precision: 0.0260 - val_recall: 0.1963 - learning_rate: 5.0000e-05\n",
      "Epoch 15/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0126 - precision: 0.0345 - recall: 0.2622 - val_loss: 0.0157 - val_precision: 0.0269 - val_recall: 0.2034 - learning_rate: 5.0000e-05\n",
      "Epoch 16/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0122 - precision: 0.0358 - recall: 0.2727 - val_loss: 0.0156 - val_precision: 0.0275 - val_recall: 0.2085 - learning_rate: 5.0000e-05\n",
      "Epoch 17/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0120 - precision: 0.0366 - recall: 0.2792 - val_loss: 0.0155 - val_precision: 0.0278 - val_recall: 0.2108 - learning_rate: 5.0000e-05\n",
      "Epoch 18/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0118 - precision: 0.0373 - recall: 0.2847 - val_loss: 0.0156 - val_precision: 0.0278 - val_recall: 0.2109 - learning_rate: 5.0000e-05\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0116 - precision: 0.0380 - recall: 0.2905 - val_loss: 0.0155 - val_precision: 0.0279 - val_recall: 0.2116 - learning_rate: 5.0000e-05\n",
      "Epoch 20/20\n",
      "2189/2189 - 15s - 7ms/step - loss: 0.0113 - precision: 0.0395 - recall: 0.3022 - val_loss: 0.0153 - val_precision: 0.0279 - val_recall: 0.2112 - learning_rate: 2.5000e-05\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "[  623.3s] INFO  | Trained 20 epochs\n",
      "[  623.9s] INFO  | Optimizing threshold...\n",
      "[  702.1s] INFO  | Best threshold: 0.055, Weighted F1: 0.0698\n",
      "[  702.7s] INFO  | Loading CAFA5 baseline...\n",
      "[  702.7s] WARN  | CAFA5 not loaded: [Errno 2] No such file or directory: '/kaggle/input/cafa5-055923-pred/submission.tsv'\n",
      "[  702.7s] INFO  | Predicting test set (streaming mode)...\n",
      "[  702.7s] PROG  | Processing batch 0-5,000/224,309\n",
      "[  901.1s] PROG  | Processing batch 50,000-55,000/224,309\n",
      "[ 1088.7s] PROG  | Processing batch 100,000-105,000/224,309\n",
      "[ 1270.1s] PROG  | Processing batch 150,000-155,000/224,309\n",
      "[ 1445.4s] PROG  | Processing batch 200,000-205,000/224,309\n",
      "[ 1528.0s] INFO  | Submission saved to: /kaggle/working/submission.tsv\n",
      "[ 1528.0s] INFO  | Total predictions written: 179,447,200\n",
      "[ 1528.0s] INFO  | Analyzing submission...\n",
      "[ 1582.4s] INFO  | Final score check - Min: 0.5360, Max: 1.0000\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ IMPROVEMENTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "CHANGES FROM BASELINE:\n",
      "  1. âœ… More labels: 4000 â†’ 6,000\n",
      "  2. âœ… IA-weighted loss: Enabled\n",
      "  3. âœ… Better threshold search: 0.01-0.5 â†’ 0.005-0.8 (found: 0.055)\n",
      "  4. âœ… Fixed score clipping bug (max was 142, now: 1.0000)\n",
      "  5. âœ… More predictions per protein: 500 â†’ 800\n",
      "  6. âœ… Better CAFA5 ensemble weight: 0.3 â†’ 0.4\n",
      "\n",
      "RESULTS:\n",
      "  - Training proteins: 82,404\n",
      "  - Test proteins: 224,309\n",
      "  - Total predictions: 179,447,200\n",
      "  - Avg per protein: 800.0\n",
      "  - Weighted F1: 0.0698\n",
      "\n",
      "OUTPUT: /kaggle/working/submission.tsv\n",
      "\n",
      "================================================================================\n",
      "âœ… READY FOR SUBMISSION!\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CAFA-6 PROTEIN FUNCTION PREDICTION - INCREMENTAL IMPROVEMENTS\n",
    "Base Score: 2.2 â†’ Target: 2.5+\n",
    "\n",
    "Changes from baseline:\n",
    "1. FIX: Score clipping bug (max 142 â†’ max 1.0)\n",
    "2. IMPROVE: Better threshold search range\n",
    "3. IMPROVE: More labels (4000 â†’ 6000)\n",
    "4. IMPROVE: IA-weighted loss function\n",
    "5. IMPROVE: Better CAFA5 ensemble weights\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, time, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CAFA-6 PROTEIN FUNCTION PREDICTION - V2 (INCREMENTAL IMPROVEMENTS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "CONFIG = {\n",
    "    'BASE': \"/kaggle/input/cafa-6-protein-function-prediction\",\n",
    "    'CAFA5_PATH': \"/kaggle/input/cafa5-055923-pred/submission.tsv\",\n",
    "    'OUTPUT': \"/kaggle/working/submission.tsv\",\n",
    "    \n",
    "    # CHANGE 1: More labels\n",
    "    'TOP_K_LABELS': 6000,  # 4000 â†’ 6000\n",
    "    'MIN_FREQ': 2,  # 3 â†’ 2 (include rarer terms)\n",
    "    \n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 20,  # More epochs\n",
    "    'LR': 1e-4,  # Slightly lower LR\n",
    "    'HIDDEN': [1536, 768, 384],\n",
    "    'DROPOUT': 0.4,  # Less dropout\n",
    "    'L2_REG': 5e-6,  # Less regularization\n",
    "    \n",
    "    'TOP_K_PRED': 800,  # 500 â†’ 800 (more predictions per protein)\n",
    "    'PROP_ROUNDS': 15,  # 10 â†’ 15\n",
    "    'TEST_BATCH_SIZE': 5000,\n",
    "    \n",
    "    'USE_TFIDF': True,\n",
    "    'USE_AA_COMP': True,\n",
    "    'USE_DIPEP': True,\n",
    "    'USE_PHYSICHEM': True,\n",
    "    'TFIDF_MAX_FEATURES': 15000,  # 12000 â†’ 15000\n",
    "    'KMER_SIZE': 3,\n",
    "    \n",
    "    # CHANGE 2: IA-weighted loss\n",
    "    'USE_IA_WEIGHTED_LOSS': True,\n",
    "    \n",
    "    # CHANGE 3: Better ensemble\n",
    "    'CAFA5_WEIGHT': 0.4,  # 0.3 â†’ 0.4 (CAFA5 has good performance)\n",
    "    \n",
    "    'SEED': 42,\n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "tf.random.set_seed(CONFIG['SEED'])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def log(msg, level=\"INFO\"):\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[{elapsed:7.1f}s] {level:5s} | {msg}\")\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "def calculate_aa_composition(seq):\n",
    "    aas = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    comp = {aa: 0 for aa in aas}\n",
    "    for aa in seq:\n",
    "        if aa in comp:\n",
    "            comp[aa] += 1\n",
    "    total = len(seq) or 1\n",
    "    return np.array([comp[aa] / total for aa in aas])\n",
    "\n",
    "def calculate_dipeptide_composition(seq):\n",
    "    top_dipeptides = [\n",
    "        'AL', 'LA', 'AA', 'LE', 'EA', 'AS', 'LL', 'EL', 'SA', 'VA',\n",
    "        'AR', 'GA', 'LG', 'AG', 'PA', 'AP', 'GG', 'VS', 'GL', 'LV',\n",
    "        'KA', 'VE', 'AK', 'TA', 'GS', 'RA', 'AT', 'VL', 'AV', 'DA',\n",
    "        'LK', 'SG', 'KL', 'EV', 'TL', 'LT', 'KE', 'LS', 'AD', 'SE'\n",
    "    ]\n",
    "    dipep_counts = defaultdict(int)\n",
    "    for i in range(len(seq) - 1):\n",
    "        dipep = seq[i:i+2]\n",
    "        if len(dipep) == 2:\n",
    "            dipep_counts[dipep] += 1\n",
    "    total = max(len(seq) - 1, 1)\n",
    "    return np.array([dipep_counts[dp] / total for dp in top_dipeptides])\n",
    "\n",
    "def calculate_physicochemical_properties(seq):\n",
    "    hydro = {'A': 1.8, 'C': 2.5, 'D': -3.5, 'E': -3.5, 'F': 2.8,\n",
    "             'G': -0.4, 'H': -3.2, 'I': 4.5, 'K': -3.9, 'L': 3.8,\n",
    "             'M': 1.9, 'N': -3.5, 'P': -1.6, 'Q': -3.5, 'R': -4.5,\n",
    "             'S': -0.8, 'T': -0.7, 'V': 4.2, 'W': -0.9, 'Y': -1.3}\n",
    "    mw = {'A': 89, 'C': 121, 'D': 133, 'E': 147, 'F': 165, 'G': 75,\n",
    "          'H': 155, 'I': 131, 'K': 146, 'L': 131, 'M': 149, 'N': 132,\n",
    "          'P': 115, 'Q': 146, 'R': 174, 'S': 105, 'T': 119, 'V': 117,\n",
    "          'W': 204, 'Y': 181}\n",
    "    \n",
    "    if not seq:\n",
    "        return np.zeros(8)\n",
    "    \n",
    "    avg_hydro = np.mean([hydro.get(aa, 0) for aa in seq])\n",
    "    avg_mw = np.mean([mw.get(aa, 0) for aa in seq])\n",
    "    positive = sum(1 for aa in seq if aa in 'RK')\n",
    "    negative = sum(1 for aa in seq if aa in 'DE')\n",
    "    polar = sum(1 for aa in seq if aa in 'STNQ')\n",
    "    helix_formers = sum(1 for aa in seq if aa in 'AELM')\n",
    "    sheet_formers = sum(1 for aa in seq if aa in 'VIF')\n",
    "    total = len(seq)\n",
    "    \n",
    "    return np.array([\n",
    "        avg_hydro, avg_mw / 150, positive / total, negative / total,\n",
    "        polar / total, helix_formers / total, sheet_formers / total, len(seq) / 1000\n",
    "    ])\n",
    "\n",
    "def get_kmers(seq, k=3):\n",
    "    return ' '.join([seq[i:i+k] for i in range(len(seq) - k + 1)])\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING (SAME)\n",
    "# =============================================================================\n",
    "log(\"Loading FASTA sequences...\")\n",
    "\n",
    "def read_fasta(path):\n",
    "    seqs = {}\n",
    "    pid, seq = None, []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if pid:\n",
    "                    seqs[pid] = ''.join(seq)\n",
    "                parts = line[1:].split('|')\n",
    "                pid = parts[1] if len(parts) > 1 else line[1:].split()[0]\n",
    "                seq = []\n",
    "            else:\n",
    "                seq.append(line)\n",
    "        if pid:\n",
    "            seqs[pid] = ''.join(seq)\n",
    "    return seqs\n",
    "\n",
    "train_seqs = read_fasta(f\"{CONFIG['BASE']}/Train/train_sequences.fasta\")\n",
    "test_seqs = read_fasta(f\"{CONFIG['BASE']}/Test/testsuperset.fasta\")\n",
    "\n",
    "log(f\"Loaded {len(train_seqs):,} training sequences\")\n",
    "log(f\"Loaded {len(test_seqs):,} test sequences\")\n",
    "\n",
    "df_terms = pd.read_csv(f\"{CONFIG['BASE']}/Train/train_terms.tsv\", sep='\\t', header=None,\n",
    "                       names=['protein_id', 'go_term', 'ontology'])\n",
    "df_terms = df_terms[df_terms['protein_id'] != 'EntryID'].reset_index(drop=True)\n",
    "\n",
    "df_ia = pd.read_csv(f\"{CONFIG['BASE']}/IA.tsv\", sep='\\t', header=None, names=['go_term', 'ia'])\n",
    "ia_weights = dict(zip(df_ia['go_term'], df_ia['ia']))\n",
    "\n",
    "log(f\"Loaded {len(df_terms):,} annotations for {df_terms['protein_id'].nunique():,} proteins\")\n",
    "log(f\"Loaded {len(ia_weights):,} IA weights\")\n",
    "\n",
    "# =============================================================================\n",
    "# GO ONTOLOGY PARSING (SAME)\n",
    "# =============================================================================\n",
    "log(\"Parsing GO ontology hierarchy...\")\n",
    "\n",
    "parents = defaultdict(set)\n",
    "term_ontology = {}\n",
    "\n",
    "with open(f\"{CONFIG['BASE']}/Train/go-basic.obo\") as f:\n",
    "    cur_id = None\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line == \"[Term]\":\n",
    "            cur_id = None\n",
    "        elif line.startswith(\"id: \"):\n",
    "            cur_id = line.split(\"id: \")[1]\n",
    "        elif line.startswith(\"namespace: \"):\n",
    "            if cur_id:\n",
    "                term_ontology[cur_id] = line.split(\"namespace: \")[1]\n",
    "        elif line.startswith(\"is_a: \") and cur_id:\n",
    "            parents[cur_id].add(line.split()[1])\n",
    "        elif line.startswith(\"relationship: part_of \") and cur_id:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                parents[cur_id].add(parts[2])\n",
    "\n",
    "log(f\"Parsed {len(parents):,} GO terms\")\n",
    "\n",
    "def get_all_ancestors(term):\n",
    "    ancestors = set()\n",
    "    queue = [term]\n",
    "    while queue:\n",
    "        current = queue.pop(0)\n",
    "        for parent in parents.get(current, []):\n",
    "            if parent not in ancestors:\n",
    "                ancestors.add(parent)\n",
    "                queue.append(parent)\n",
    "    return ancestors\n",
    "\n",
    "# =============================================================================\n",
    "# LABEL PROPAGATION (SAME)\n",
    "# =============================================================================\n",
    "log(\"Propagating labels...\")\n",
    "\n",
    "protein_to_terms = defaultdict(set)\n",
    "for _, row in df_terms.iterrows():\n",
    "    protein_to_terms[row['protein_id']].add(row['go_term'])\n",
    "\n",
    "propagated_terms = {}\n",
    "for i, (protein, terms) in enumerate(protein_to_terms.items()):\n",
    "    all_terms = set(terms)\n",
    "    for term in terms:\n",
    "        all_terms.update(get_all_ancestors(term))\n",
    "    propagated_terms[protein] = all_terms\n",
    "    if (i + 1) % 25000 == 0:\n",
    "        log(f\"Propagated {i+1:,}/{len(protein_to_terms):,} proteins\", \"PROG\")\n",
    "\n",
    "log(f\"Before: {sum(len(v) for v in protein_to_terms.values()):,}, After: {sum(len(v) for v in propagated_terms.values()):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LABEL SELECTION (MORE LABELS)\n",
    "# =============================================================================\n",
    "log(\"Selecting labels...\")\n",
    "\n",
    "term_counts = Counter()\n",
    "for terms in propagated_terms.values():\n",
    "    term_counts.update(terms)\n",
    "\n",
    "frequent_terms = {t for t, c in term_counts.items() if c >= CONFIG['MIN_FREQ']}\n",
    "\n",
    "mf_candidates = [t for t, c in term_counts.most_common() \n",
    "                 if t in frequent_terms and term_ontology.get(t) == 'molecular_function']\n",
    "bp_candidates = [t for t, c in term_counts.most_common() \n",
    "                 if t in frequent_terms and term_ontology.get(t) == 'biological_process']\n",
    "cc_candidates = [t for t, c in term_counts.most_common() \n",
    "                 if t in frequent_terms and term_ontology.get(t) == 'cellular_component']\n",
    "\n",
    "per_ontology = CONFIG['TOP_K_LABELS'] // 3\n",
    "selected_mf = mf_candidates[:per_ontology]\n",
    "selected_bp = bp_candidates[:per_ontology]\n",
    "selected_cc = cc_candidates[:per_ontology]\n",
    "top_terms = selected_mf + selected_bp + selected_cc\n",
    "\n",
    "log(f\"Labels: MF={len(selected_mf)}, BP={len(selected_bp)}, CC={len(selected_cc)}, Total={len(top_terms)}\")\n",
    "\n",
    "valid_proteins = [p for p in propagated_terms.keys() if p in train_seqs]\n",
    "filtered_terms = {p: [t for t in propagated_terms[p] if t in top_terms] for p in valid_proteins}\n",
    "valid_proteins = [p for p in valid_proteins if filtered_terms[p]]\n",
    "\n",
    "log(f\"Training proteins: {len(valid_proteins):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE EXTRACTION (MORE FEATURES)\n",
    "# =============================================================================\n",
    "log(\"Extracting features for training set...\")\n",
    "\n",
    "if CONFIG['USE_TFIDF']:\n",
    "    log(\"  Computing TF-IDF...\")\n",
    "    train_texts = [get_kmers(train_seqs[p], CONFIG['KMER_SIZE']) for p in valid_proteins]\n",
    "    tfidf = TfidfVectorizer(analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                            max_features=CONFIG['TFIDF_MAX_FEATURES'])\n",
    "    X_tfidf = tfidf.fit_transform(train_texts).toarray().astype(np.float32)\n",
    "    log(f\"    TF-IDF: {X_tfidf.shape}\")\n",
    "    del train_texts\n",
    "    gc.collect()\n",
    "else:\n",
    "    X_tfidf = None\n",
    "\n",
    "feature_parts = []\n",
    "if X_tfidf is not None:\n",
    "    feature_parts.append(X_tfidf)\n",
    "\n",
    "if CONFIG['USE_AA_COMP']:\n",
    "    log(\"  Computing AA composition...\")\n",
    "    X_aa = np.array([calculate_aa_composition(train_seqs[p]) for p in valid_proteins], dtype=np.float32)\n",
    "    feature_parts.append(X_aa)\n",
    "    del X_aa\n",
    "    gc.collect()\n",
    "\n",
    "if CONFIG['USE_DIPEP']:\n",
    "    log(\"  Computing dipeptide composition...\")\n",
    "    X_dipep = np.array([calculate_dipeptide_composition(train_seqs[p]) for p in valid_proteins], dtype=np.float32)\n",
    "    feature_parts.append(X_dipep)\n",
    "    del X_dipep\n",
    "    gc.collect()\n",
    "\n",
    "if CONFIG['USE_PHYSICHEM']:\n",
    "    log(\"  Computing physicochemical properties...\")\n",
    "    X_physchem = np.array([calculate_physicochemical_properties(train_seqs[p]) for p in valid_proteins], dtype=np.float32)\n",
    "    feature_parts.append(X_physchem)\n",
    "    del X_physchem\n",
    "    gc.collect()\n",
    "\n",
    "X_combined = np.concatenate(feature_parts, axis=1)\n",
    "log(f\"Combined features: {X_combined.shape}\")\n",
    "\n",
    "del feature_parts\n",
    "if X_tfidf is not None:\n",
    "    del X_tfidf\n",
    "gc.collect()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined).astype(np.float32)\n",
    "del X_combined\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARE LABELS\n",
    "# =============================================================================\n",
    "log(\"Preparing labels...\")\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=sorted(top_terms))\n",
    "y_list = [filtered_terms[p] for p in valid_proteins]\n",
    "Y = mlb.fit_transform(y_list).astype(np.float32)\n",
    "\n",
    "log(f\"Label matrix: {Y.shape}, sparsity: {(1 - Y.mean()) * 100:.2f}%\")\n",
    "\n",
    "# CHANGE 4: Prepare IA weights for loss\n",
    "label_ia = np.array([ia_weights.get(t, 1.0) for t in mlb.classes_])\n",
    "label_ia = label_ia / label_ia.mean()  # Normalize\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD MODEL WITH IA-WEIGHTED LOSS\n",
    "# =============================================================================\n",
    "log(\"Building model with IA-weighted loss...\")\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"IA-weighted binary cross-entropy\"\"\"\n",
    "    weights_tensor = K.constant(label_ia, dtype='float32')\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_bce = bce * weights_tensor\n",
    "    return K.mean(weighted_bce, axis=-1)\n",
    "\n",
    "def build_model(input_dim, output_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    \n",
    "    for i, hidden_size in enumerate(CONFIG['HIDDEN']):\n",
    "        x = layers.Dense(hidden_size, kernel_regularizer=regularizers.l2(CONFIG['L2_REG']))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Dropout(CONFIG['DROPOUT'])(x)\n",
    "    \n",
    "    outputs = layers.Dense(output_dim, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    loss_fn = weighted_binary_crossentropy if CONFIG['USE_IA_WEIGHTED_LOSS'] else 'binary_crossentropy'\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG['LR']),\n",
    "        loss=loss_fn,\n",
    "        metrics=['precision', 'recall']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(X_scaled.shape[1], Y.shape[1])\n",
    "log(f\"Model params: {model.count_params():,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN\n",
    "# =============================================================================\n",
    "log(\"Splitting data and training...\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, Y, test_size=0.15, random_state=CONFIG['SEED']\n",
    ")\n",
    "\n",
    "log(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    epochs=CONFIG['EPOCHS'],\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "log(f\"Trained {len(history.history['loss'])} epochs\")\n",
    "\n",
    "del X_train, y_train, X_scaled, Y\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# THRESHOLD OPTIMIZATION (BETTER SEARCH RANGE)\n",
    "# =============================================================================\n",
    "log(\"Optimizing threshold...\")\n",
    "\n",
    "y_val_pred = model.predict(X_val, batch_size=64, verbose=0)\n",
    "\n",
    "def calc_weighted_f1(y_true, y_pred_bin, weights):\n",
    "    tp = ((y_true == 1) & (y_pred_bin == 1)).sum(0).astype(float)\n",
    "    fp = ((y_true == 0) & (y_pred_bin == 1)).sum(0).astype(float)\n",
    "    fn = ((y_true == 1) & (y_pred_bin == 0)).sum(0).astype(float)\n",
    "    prec = tp / (tp + fp + 1e-12)\n",
    "    rec = tp / (tp + fn + 1e-12)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    return (f1 * weights).sum() / (weights.sum() + 1e-12)\n",
    "\n",
    "best_thr, best_f1 = 0.5, 0\n",
    "# CHANGE: Better threshold search - try lower AND higher thresholds\n",
    "for t in np.arange(0.005, 0.8, 0.01):  # 0.01-0.5 â†’ 0.005-0.8\n",
    "    f1 = calc_weighted_f1(y_val, (y_val_pred >= t).astype(int), label_ia)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, t\n",
    "\n",
    "log(f\"Best threshold: {best_thr:.3f}, Weighted F1: {best_f1:.4f}\")\n",
    "\n",
    "del X_val, y_val, y_val_pred\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD CAFA5 BASELINE\n",
    "# =============================================================================\n",
    "log(\"Loading CAFA5 baseline...\")\n",
    "\n",
    "try:\n",
    "    df_cafa5 = pd.read_csv(CONFIG['CAFA5_PATH'], sep='\\t', header=None,\n",
    "                          names=['protein_id', 'go_term', 'score'])\n",
    "    cafa5_lookup = defaultdict(dict)\n",
    "    for _, row in df_cafa5.iterrows():\n",
    "        try:\n",
    "            cafa5_lookup[row['protein_id']][row['go_term']] = float(row['score'])\n",
    "        except:\n",
    "            pass\n",
    "    log(f\"CAFA5 covers {len(cafa5_lookup):,} proteins, {sum(len(v) for v in cafa5_lookup.values()):,} predictions\")\n",
    "except Exception as e:\n",
    "    log(f\"CAFA5 not loaded: {e}\", \"WARN\")\n",
    "    cafa5_lookup = {}\n",
    "\n",
    "# =============================================================================\n",
    "# STREAMING TEST PREDICTION\n",
    "# =============================================================================\n",
    "log(\"Predicting test set (streaming mode)...\")\n",
    "\n",
    "test_protein_ids = list(test_seqs.keys())\n",
    "n_test = len(test_protein_ids)\n",
    "batch_size = CONFIG['TEST_BATCH_SIZE']\n",
    "\n",
    "term_to_idx = {t: i for i, t in enumerate(mlb.classes_)}\n",
    "restricted_parents = {\n",
    "    t: [p for p in parents.get(t, []) if p in term_to_idx]\n",
    "    for t in mlb.classes_\n",
    "}\n",
    "\n",
    "total_predictions = 0\n",
    "\n",
    "with open(CONFIG['OUTPUT'], 'w') as f_out:\n",
    "    \n",
    "    for batch_start in range(0, n_test, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, n_test)\n",
    "        batch_ids = test_protein_ids[batch_start:batch_end]\n",
    "        \n",
    "        if (batch_start // batch_size) % 10 == 0:\n",
    "            log(f\"Processing batch {batch_start:,}-{batch_end:,}/{n_test:,}\", \"PROG\")\n",
    "        \n",
    "        # Extract features\n",
    "        batch_features = []\n",
    "        \n",
    "        if CONFIG['USE_TFIDF']:\n",
    "            batch_texts = [get_kmers(test_seqs[p], CONFIG['KMER_SIZE']) for p in batch_ids]\n",
    "            batch_tfidf = tfidf.transform(batch_texts).toarray().astype(np.float32)\n",
    "            batch_features.append(batch_tfidf)\n",
    "            del batch_texts, batch_tfidf\n",
    "        \n",
    "        if CONFIG['USE_AA_COMP']:\n",
    "            batch_aa = np.array([calculate_aa_composition(test_seqs[p]) for p in batch_ids], dtype=np.float32)\n",
    "            batch_features.append(batch_aa)\n",
    "            del batch_aa\n",
    "        \n",
    "        if CONFIG['USE_DIPEP']:\n",
    "            batch_dipep = np.array([calculate_dipeptide_composition(test_seqs[p]) for p in batch_ids], dtype=np.float32)\n",
    "            batch_features.append(batch_dipep)\n",
    "            del batch_dipep\n",
    "        \n",
    "        if CONFIG['USE_PHYSICHEM']:\n",
    "            batch_phys = np.array([calculate_physicochemical_properties(test_seqs[p]) for p in batch_ids], dtype=np.float32)\n",
    "            batch_features.append(batch_phys)\n",
    "            del batch_phys\n",
    "        \n",
    "        X_batch = np.concatenate(batch_features, axis=1)\n",
    "        del batch_features\n",
    "        \n",
    "        X_batch = scaler.transform(X_batch).astype(np.float32)\n",
    "        \n",
    "        # Predict\n",
    "        y_batch_pred = model.predict(X_batch, batch_size=64, verbose=0)\n",
    "        del X_batch\n",
    "        \n",
    "        # Propagate\n",
    "        for _ in range(CONFIG['PROP_ROUNDS']):\n",
    "            for child, parent_list in restricted_parents.items():\n",
    "                if not parent_list:\n",
    "                    continue\n",
    "                c_idx = term_to_idx[child]\n",
    "                for parent in parent_list:\n",
    "                    p_idx = term_to_idx[parent]\n",
    "                    mask = y_batch_pred[:, c_idx] > y_batch_pred[:, p_idx]\n",
    "                    if mask.any():\n",
    "                        y_batch_pred[mask, p_idx] = y_batch_pred[mask, c_idx]\n",
    "        \n",
    "        # Write predictions\n",
    "        for i, pid in enumerate(batch_ids):\n",
    "            scores = y_batch_pred[i]\n",
    "            top_idx = np.argsort(scores)[-CONFIG['TOP_K_PRED']:][::-1]\n",
    "            \n",
    "            for idx in top_idx:\n",
    "                score = float(scores[idx])\n",
    "                go_term = mlb.classes_[idx]\n",
    "                \n",
    "                # CHANGE: Fixed ensemble - clip to [0,1]\n",
    "                if pid in cafa5_lookup and go_term in cafa5_lookup[pid]:\n",
    "                    cafa5_score = cafa5_lookup[pid][go_term]\n",
    "                    score = (1 - CONFIG['CAFA5_WEIGHT']) * score + CONFIG['CAFA5_WEIGHT'] * cafa5_score\n",
    "                    score = np.clip(score, 0, 1)  # FIX: Ensure scores are in [0,1]\n",
    "                \n",
    "                if score > 0.001:\n",
    "                    f_out.write(f\"{pid}\\t{go_term}\\t{score:.3g}\\n\")\n",
    "                    total_predictions += 1\n",
    "        \n",
    "        del y_batch_pred\n",
    "        gc.collect()\n",
    "\n",
    "log(f\"Submission saved to: {CONFIG['OUTPUT']}\")\n",
    "log(f\"Total predictions written: {total_predictions:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYZE SUBMISSION\n",
    "# =============================================================================\n",
    "log(\"Analyzing submission...\")\n",
    "\n",
    "df_sub = pd.read_csv(CONFIG['OUTPUT'], sep='\\t', header=None, names=['pid', 'go', 'score'])\n",
    "\n",
    "log(f\"Final score check - Min: {df_sub['score'].min():.4f}, Max: {df_sub['score'].max():.4f}\")\n",
    "\n",
    "if df_sub['score'].max() > 1.0:\n",
    "    log(\"WARNING: Scores exceed 1.0, something is wrong!\", \"ERROR\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ IMPROVEMENTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "CHANGES FROM BASELINE:\n",
    "  1. âœ… More labels: 4000 â†’ {len(top_terms):,}\n",
    "  2. âœ… IA-weighted loss: {'Enabled' if CONFIG['USE_IA_WEIGHTED_LOSS'] else 'Disabled'}\n",
    "  3. âœ… Better threshold search: 0.01-0.5 â†’ 0.005-0.8 (found: {best_thr:.3f})\n",
    "  4. âœ… Fixed score clipping bug (max was 142, now: {df_sub['score'].max():.4f})\n",
    "  5. âœ… More predictions per protein: 500 â†’ {CONFIG['TOP_K_PRED']}\n",
    "  6. âœ… Better CAFA5 ensemble weight: 0.3 â†’ {CONFIG['CAFA5_WEIGHT']}\n",
    "\n",
    "RESULTS:\n",
    "  - Training proteins: {len(valid_proteins):,}\n",
    "  - Test proteins: {df_sub['pid'].nunique():,}\n",
    "  - Total predictions: {len(df_sub):,}\n",
    "  - Avg per protein: {len(df_sub)/df_sub['pid'].nunique():.1f}\n",
    "  - Weighted F1: {best_f1:.4f}\n",
    "\n",
    "OUTPUT: {CONFIG['OUTPUT']}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… READY FOR SUBMISSION!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "modelId": 425816,
     "modelInstanceId": 407954,
     "sourceId": 517313,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1631.674361,
   "end_time": "2025-11-14T17:04:38.633920",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-14T16:37:26.959559",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
