{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102c616b",
   "metadata": {
    "papermill": {
     "duration": 0.003326,
     "end_time": "2026-02-11T11:03:08.345162",
     "exception": false,
     "start_time": "2026-02-11T11:03:08.341836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Qwen3 Next 80B Fewshot Starter (llama.cpp)\n",
    "\n",
    "## Overview\n",
    "\n",
    "A small starter that builds **few-shot prompts via TF-IDF retrieval** over *(transliteration → translation)* pairs, then runs inference through **llama.cpp `llama-server`** (OpenAI-compatible endpoint), tuned for **T4×2**-style environments.\n",
    "\n",
    "## Highlights\n",
    "\n",
    "* **RAG**: TF-IDF retriever for transliteration/translation pair selection (few-shot)\n",
    "* **Model**: `unsloth/Qwen3-Next-80B-A3B-Instruct` (4-bit / GGUF)\n",
    "* **Server**: `llama.cpp` (`llama-server`) optimized for T4×2\n",
    "\n",
    "## Tips\n",
    "\n",
    "* Decoding params (`temperature`, `min_p`, `top_p`, `top_k`, `presence_penalty`) follow the official guide:\n",
    "  [https://unsloth.ai/docs/models/tutorials/qwen3-next](https://unsloth.ai/docs/models/tutorials/qwen3-next)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* `llama.cpp` **with `llama-server`** (OpenAI-compatible `/v1/*` endpoints)\n",
    "* A **GGUF** model file\n",
    "* Python deps: `pandas`, `numpy`, `scikit-learn`, `tqdm`, `requests`\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "```bash\n",
    "# 1) Build few-shot examples for each test item\n",
    "python retriever.py --data_dir ./data --out test_with_few_shot.parquet\n",
    "\n",
    "# 2) Start llama-server (edit paths inside run_server.py)\n",
    "python run_server.py\n",
    "\n",
    "# 3) Run inference and write submission\n",
    "python submit.py \\\n",
    "  --data_dir ./data \\\n",
    "  --test_with_few_shot_path test_with_few_shot.parquet \\\n",
    "  --base_url http://127.0.0.1:8080 \\\n",
    "  --out submission.csv\n",
    "```\n",
    "\n",
    "## Data format\n",
    "\n",
    "* `train.csv` must contain: `transliteration`, `translation`\n",
    "* `test.csv` must contain: `transliteration`\n",
    "* `retriever.py` outputs a parquet where each test row includes:\n",
    "\n",
    "  * `few_shot`: list of `{transliteration, translation}` pairs\n",
    "\n",
    "## Server check (optional)\n",
    "\n",
    "```bash\n",
    "curl http://127.0.0.1:8080/v1/models\n",
    "```\n",
    "\n",
    "## Tuning knobs\n",
    "\n",
    "* Retrieval: `n_shots`, TF-IDF char n-grams, similarity thresholds (`retriever.py`)\n",
    "* Server: `ctx`, `n_gpu_layers`, parallelism (`run_server.py`)\n",
    "* Generation: decoding params + max tokens (`submit.py`)\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "* `503 model is loading`: expected while the model is warming up\n",
    "* Wrong paths: check `run_server.py` (`server_bin`, `model_path`)\n",
    "* Output too long/short: adjust `max_tokens` / context size (`ctx`)\n",
    "\n",
    "## License / Notes\n",
    "\n",
    "Follow the licenses of the model, llama.cpp, and any dataset used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7561415",
   "metadata": {
    "papermill": {
     "duration": 0.002293,
     "end_time": "2026-02-11T11:03:08.349981",
     "exception": false,
     "start_time": "2026-02-11T11:03:08.347688",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee12cf9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:08.355934Z",
     "iopub.status.busy": "2026-02-11T11:03:08.355611Z",
     "iopub.status.idle": "2026-02-11T11:03:15.531392Z",
     "shell.execute_reply": "2026-02-11T11:03:15.530572Z"
    },
    "papermill": {
     "duration": 7.180875,
     "end_time": "2026-02-11T11:03:15.533213",
     "exception": false,
     "start_time": "2026-02-11T11:03:08.352338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/temp/app\n",
    "cp /kaggle/input/llama-b7814-cuda/llama-b7814-cuda/app/* /kaggle/temp/app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1633b6cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:15.540221Z",
     "iopub.status.busy": "2026-02-11T11:03:15.539685Z",
     "iopub.status.idle": "2026-02-11T11:03:15.657968Z",
     "shell.execute_reply": "2026-02-11T11:03:15.657139Z"
    },
    "papermill": {
     "duration": 0.123802,
     "end_time": "2026-02-11T11:03:15.659591",
     "exception": false,
     "start_time": "2026-02-11T11:03:15.535789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libggml-base.so.0.9.5\t    libggml-cpu-ivybridge.so\t   libggml-cpu-zen4.so\r\n",
      "libggml-cpu-alderlake.so    libggml-cpu-piledriver.so\t   libggml-cuda.so\r\n",
      "libggml-cpu-cannonlake.so   libggml-cpu-sandybridge.so\t   libggml.so.0.9.5\r\n",
      "libggml-cpu-cascadelake.so  libggml-cpu-sapphirerapids.so  libllama.so.0.0.7814\r\n",
      "libggml-cpu-cooperlake.so   libggml-cpu-skylakex.so\t   libmtmd.so.0.0.7814\r\n",
      "libggml-cpu-haswell.so\t    libggml-cpu-sse42.so\t   llama-server\r\n",
      "libggml-cpu-icelake.so\t    libggml-cpu-x64.so\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/temp/app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df71e185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:15.666053Z",
     "iopub.status.busy": "2026-02-11T11:03:15.665806Z",
     "iopub.status.idle": "2026-02-11T11:03:15.683469Z",
     "shell.execute_reply": "2026-02-11T11:03:15.682945Z"
    },
    "papermill": {
     "duration": 0.022685,
     "end_time": "2026-02-11T11:03:15.685016",
     "exception": false,
     "start_time": "2026-02-11T11:03:15.662331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/temp/app/\n",
    "chmod +x llama-server\n",
    "test -e libmtmd.so.0 || ln -s libmtmd.so.0.0.7814 libmtmd.so.0\n",
    "test -e libllama.so.0 || ln -s libllama.so.0.0.7814 libllama.so.0\n",
    "test -e libggml.so.0 || ln -s libggml.so.0.9.5 libggml.so.0\n",
    "test -e libggml-base.so.0 || ln -s libggml-base.so.0.9.5 libggml-base.so.0\n",
    "\n",
    "#./llama-server --list-devices\n",
    "#nohup ./llama-server -m \"/kaggle/input/qwen3-next-80b-a3b-instruct-q2-k-gguf/gguf/default/1/Qwen3-Next-80B-A3B-Instruct-Q2_K.gguf\" -ngl 24 -c 8192 -np 1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae9b2ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:15.691220Z",
     "iopub.status.busy": "2026-02-11T11:03:15.690988Z",
     "iopub.status.idle": "2026-02-11T11:03:15.697860Z",
     "shell.execute_reply": "2026-02-11T11:03:15.697106Z"
    },
    "papermill": {
     "duration": 0.011692,
     "end_time": "2026-02-11T11:03:15.699215",
     "exception": false,
     "start_time": "2026-02-11T11:03:15.687523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llama_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama_server.py\n",
    "import os\n",
    "import time\n",
    "import socket\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def wait_port(host: str, port: int, timeout_s: float = 60.0) -> None:\n",
    "    \"\"\"TCP portが開くまで待つ\"\"\"\n",
    "    deadline = time.time() + timeout_s\n",
    "    last_err = None\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            with socket.create_connection((host, port), timeout=1.0):\n",
    "                return\n",
    "        except OSError as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.5)\n",
    "    raise TimeoutError(f\"Port {host}:{port} did not open within {timeout_s}s. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "def wait_http_ready(base_url: str, timeout_s: float = 900.0, proc: subprocess.Popen | None = None) -> None:\n",
    "    \"\"\"\n",
    "    llama.cpp serverの「準備完了」を待つ。\n",
    "    - 200/204: ready\n",
    "    - 503 かつ \"model is loading\": 正常なロード中として待機継続\n",
    "    - proc が死んだら即エラー\n",
    "    \"\"\"\n",
    "    deadline = time.time() + timeout_s\n",
    "    last = None\n",
    "\n",
    "    urls = [\n",
    "        f\"{base_url}/health\",\n",
    "        f\"{base_url}/v1/models\",\n",
    "        f\"{base_url}/\",\n",
    "    ]\n",
    "\n",
    "    while time.time() < deadline:\n",
    "        if proc is not None and proc.poll() is not None:\n",
    "            raise RuntimeError(f\"llama-server exited early with code {proc.returncode}\")\n",
    "\n",
    "        for url in urls:\n",
    "            try:\n",
    "                r = requests.get(url, timeout=2.0)\n",
    "\n",
    "                # Ready\n",
    "                if r.status_code in (200, 204):\n",
    "                    return\n",
    "\n",
    "                # Loading (expected)\n",
    "                if r.status_code == 503 and (\"model is loading\" in r.text.lower()):\n",
    "                    last = (url, 503, \"loading\")\n",
    "                    continue\n",
    "\n",
    "                # Some builds: /health may not exist; treat as \"server up\" hints only\n",
    "                if url.endswith(\"/\") and r.status_code in (404, 405):\n",
    "                    last = (url, r.status_code, \"server up (route missing)\")\n",
    "                    continue\n",
    "\n",
    "                last = (url, r.status_code, r.text[:200])\n",
    "\n",
    "            except Exception as e:\n",
    "                last = (url, \"exception\", repr(e))\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    raise TimeoutError(f\"Server not ready within {timeout_s}s. Last probe: {last}\")\n",
    "\n",
    "\n",
    "class LlamaServer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        server_bin: str,\n",
    "        model_path: str,\n",
    "        host: str = \"127.0.0.1\",\n",
    "        port: int = 8080,\n",
    "        n_gpu_layers: int = 30,\n",
    "        n_cpu_moe: int = 19,\n",
    "        ctx: int = 7168,\n",
    "        parallel: int = 1,\n",
    "        threads: int = 4,\n",
    "        cram: int = 5120,\n",
    "        extra_args=None,\n",
    "        workdir: str | None = None,\n",
    "        ld_library_path: str | None = None,\n",
    "        log_path: str | None = None,\n",
    "    ):\n",
    "        self.server_bin = server_bin\n",
    "        self.model_path = model_path\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.n_gpu_layers = n_gpu_layers\n",
    "        self.n_cpu_moe = n_cpu_moe\n",
    "        self.ctx = ctx\n",
    "        self.parallel = parallel\n",
    "        self.threads = threads\n",
    "        self.cram = cram\n",
    "        self.extra_args = extra_args or []\n",
    "        self.workdir = workdir\n",
    "        self.ld_library_path = ld_library_path\n",
    "        self.log_path = log_path\n",
    "        self.proc: subprocess.Popen | None = None\n",
    "\n",
    "    @property\n",
    "    def base_url(self) -> str:\n",
    "        return f\"http://{self.host}:{self.port}\"\n",
    "\n",
    "    def start(self, timeout_s: float = 900.0) -> None:\n",
    "        if self.proc and self.proc.poll() is None:\n",
    "            return  # already running\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        if self.ld_library_path:\n",
    "            env[\"LD_LIBRARY_PATH\"] = f\"{self.ld_library_path}:{env.get('LD_LIBRARY_PATH','')}\"\n",
    "\n",
    "        # ログはファイルへ（Jupyterのセルが固まらないように）\n",
    "        log_file = None\n",
    "        if self.log_path:\n",
    "            Path(self.log_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            log_file = open(self.log_path, \"ab\", buffering=0)\n",
    "\n",
    "        cmd = [\n",
    "            self.server_bin,\n",
    "            \"-m\", self.model_path,\n",
    "            \"--host\", self.host,\n",
    "            \"--port\", str(self.port),\n",
    "            \"-ngl\", str(self.n_gpu_layers),\n",
    "            \"--n-cpu-moe\", str(self.n_cpu_moe),\n",
    "            \"-c\", str(self.ctx),\n",
    "            \"-np\", str(self.parallel),\n",
    "            \"-t\", str(self.threads),\n",
    "            \"-cram\", str(self.cram),\n",
    "            *self.extra_args,\n",
    "        ]\n",
    "\n",
    "        self.proc = subprocess.Popen(\n",
    "            cmd,\n",
    "            cwd=self.workdir,\n",
    "            env=env,\n",
    "            stdout=log_file if log_file else subprocess.DEVNULL,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            stdin=subprocess.DEVNULL,\n",
    "            start_new_session=True,\n",
    "        )\n",
    "\n",
    "        # まずポートを待つ（プロセス死亡もここで検知してよい）\n",
    "        wait_port(self.host, self.port, timeout_s=min(timeout_s, 60.0))\n",
    "\n",
    "        if self.proc.poll() is not None:\n",
    "            raise RuntimeError(f\"llama-server exited early with code {self.proc.returncode}. Check log: {self.log_path}\")\n",
    "\n",
    "        # HTTPレベルで ready を待つ（503 loading を許容）\n",
    "        wait_http_ready(self.base_url, timeout_s=timeout_s, proc=self.proc)\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        if not self.proc:\n",
    "            return\n",
    "        if self.proc.poll() is None:\n",
    "            self.proc.terminate()\n",
    "            try:\n",
    "                self.proc.wait(timeout=10)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                self.proc.kill()\n",
    "                self.proc.wait(timeout=10)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        self.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a51e7ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:15.705335Z",
     "iopub.status.busy": "2026-02-11T11:03:15.705096Z",
     "iopub.status.idle": "2026-02-11T11:03:15.868415Z",
     "shell.execute_reply": "2026-02-11T11:03:15.867733Z"
    },
    "papermill": {
     "duration": 0.167994,
     "end_time": "2026-02-11T11:03:15.869736",
     "exception": false,
     "start_time": "2026-02-11T11:03:15.701742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.3  0.0   7376  3504 ?        Ss   11:02   0:00 /bin/bash -c \r\n",
      "root           9 26.5  0.4 735492 163772 ?       Sl   11:02   0:04 python3 -c im\r\n",
      "root          11  1.0  0.0 189232 21496 ?        Ssl  11:02   0:00 /opt/bin/nvid\r\n",
      "root          24 16.3  0.3 893652 103820 ?       Ssl  11:03   0:01 /usr/bin/pyth\r\n",
      "root          55  0.0  0.0  10076  3480 pts/0    Rs+  11:03   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "!ps aux\n",
    "#!cat /kaggle/temp/app/llama_server.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea97414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:15.875988Z",
     "iopub.status.busy": "2026-02-11T11:03:15.875735Z",
     "iopub.status.idle": "2026-02-11T11:03:15.881216Z",
     "shell.execute_reply": "2026-02-11T11:03:15.880599Z"
    },
    "papermill": {
     "duration": 0.010327,
     "end_time": "2026-02-11T11:03:15.882585",
     "exception": false,
     "start_time": "2026-02-11T11:03:15.872258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_server.py\n",
    "import importlib\n",
    "from llama_server import LlamaServer\n",
    "import llama_server\n",
    "importlib.reload(llama_server)\n",
    "import inspect\n",
    "import requests\n",
    "\n",
    "def main():\n",
    "    print(\"llama_server file:\", llama_server.__file__)\n",
    "    print(\"LlamaServer.__init__:\", inspect.signature(llama_server.LlamaServer.__init__))\n",
    "    \n",
    "    server = LlamaServer(\n",
    "        server_bin=\"/kaggle/temp/app/llama-server\",\n",
    "        model_path=\"/kaggle/input/qwen3-next-80b-a3b-instruct-q4-k-m-gguf/Qwen3-Next-80B-A3B-Instruct-Q4_K_M.gguf\",\n",
    "        workdir=\"/kaggle/temp/app\",\n",
    "        ld_library_path=\"/kaggle/temp/app\",\n",
    "        log_path=\"/kaggle/temp/app/llama_server.log\",\n",
    "        n_gpu_layers=30,\n",
    "        n_cpu_moe=19,\n",
    "        ctx=7168,\n",
    "        parallel=1,\n",
    "        threads=4,\n",
    "        cram=5120\n",
    "    )\n",
    "    \n",
    "    server.start(timeout_s=1500)   # ここでロード完了まで待つ（503 loading は許容済み）\n",
    "    print(\"server up:\", server.base_url)\n",
    "    \n",
    "    # 以降、何回でも呼べる\n",
    "    models = requests.get(f\"{server.base_url}/v1/models\").json()\n",
    "    print(models)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fb8af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:15.888887Z",
     "iopub.status.busy": "2026-02-11T11:03:15.888441Z",
     "iopub.status.idle": "2026-02-11T11:03:15.893245Z",
     "shell.execute_reply": "2026-02-11T11:03:15.892693Z"
    },
    "papermill": {
     "duration": 0.009379,
     "end_time": "2026-02-11T11:03:15.894521",
     "exception": false,
     "start_time": "2026-02-11T11:03:15.885142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing retriever.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile retriever.py\n",
    "#from rag_utils import FewShotRetriever\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class FewShotRetriever:\n",
    "    def __init__(self, train_df: pd.DataFrame):\n",
    "        self.train_df = train_df\n",
    "        self.vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4), min_df=1, max_features=5000)\n",
    "        translit_data = train_df[\"transliteration\"].values.astype(\"U\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(translit_data)\n",
    "\n",
    "    def get_few_shot_examples(self, query: str, n_shots: int) -> list[dict]:\n",
    "        MAX_TOKENS = 5120\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
    "        top_indices = np.argsort(similarities)[-n_shots * 5:][::-1]\n",
    "        examples = []\n",
    "        for idx in top_indices:  \n",
    "            if len(examples) >= n_shots:\n",
    "                break\n",
    "            transliteration = self.train_df.iloc[idx][\"transliteration\"]\n",
    "            translation = self.train_df.iloc[idx][\"translation\"]\n",
    "            if len(transliteration) / 4 >= MAX_TOKENS or len(translation) / 4 >= MAX_TOKENS:\n",
    "                continue\n",
    "            if similarities[idx] > 0.05:\n",
    "                examples.append({\n",
    "                    \"transliteration\": transliteration,\n",
    "                    \"translation\": translation,\n",
    "                })\n",
    "        return examples\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"/kaggle/input/deep-past-initiative-machine-translation\")\n",
    "    parser.add_argument(\"--out\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    data_dir = Path(args.data_dir)\n",
    "    test_path = data_dir / \"test.csv\"\n",
    "    train_path = data_dir / \"train.csv\"\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    tgt_col = \"translation\"\n",
    "    source_col = \"transliteration\"\n",
    "    fsr = FewShotRetriever(train_df) \n",
    "    all_docs = []\n",
    "    for query in tqdm(test_df[source_col].fillna(\"\").astype(str).tolist()):\n",
    "        docs = fsr.get_few_shot_examples(query, n_shots=1)\n",
    "        all_docs.append(docs)\n",
    "    test_df[\"few_shot\"] = all_docs\n",
    "    test_df.to_parquet(args.out)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d40d39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:15.900886Z",
     "iopub.status.busy": "2026-02-11T11:03:15.900656Z",
     "iopub.status.idle": "2026-02-11T11:03:15.907589Z",
     "shell.execute_reply": "2026-02-11T11:03:15.906908Z"
    },
    "papermill": {
     "duration": 0.011718,
     "end_time": "2026-02-11T11:03:15.908881",
     "exception": false,
     "start_time": "2026-02-11T11:03:15.897163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submit.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert translator of Old Akkadian merchant documents.\n",
    "Your goal: Produce accurate, complete, scholarly translations matching examples.\n",
    "\n",
    "Akkadian determinatives in curly brackets:\n",
    "{d} = dingir ‘god, deity’ — d preceding non-human divine actors\n",
    "{mul} = ‘stars’ — MUL preceding astronomical bodies and constellations\n",
    "{ki} = ‘earth’ — KI following a geographical place name or location\n",
    "{lu₂} = LÚ preceding people and professions\n",
    "{e₂} = {É} preceding buildings and institutions, such as temples and palaces\n",
    "{uru} = (URU) preceding names of settlements, such as villages, towns and cities\n",
    "{kur} = (KUR) preceding lands and territories as well as mountains\n",
    "{mi} = munus (f) preceding feminine personal names\n",
    "{m} = (1 or m) preceding masculine personal names\n",
    "{geš} / {ĝeš) = (GIŠ) preceding trees and things made of wood\n",
    "{tug₂} = (TÚG) preceding textiles and other woven objects\n",
    "{dub} = (DUB) preceding clay tablets, and by extension, documents and legal records\n",
    "{id₂} = (ÍD) (A.ENGUR) preceding names of canals or rivers\n",
    "{mušen} = (MUŠEN) preceding birds\n",
    "{na₄} = (na4) preceding stone\n",
    "{kuš} = (kuš) preceding (animal) skin, fleece, hides\n",
    "{u₂} = (Ú) preceding plants\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Names: Preserve EXACTLY as shown in transliteration (Ashshur-taklaku, Ennam-Ashshur, Ali-ahum)\n",
    "2. Units: Use 'mina' (SINGULAR: '2 mina' not 'minas'), 'shekel' for smaller amounts\n",
    "3. Document: 'tablet' for records, 'letter' for messages\n",
    "4. Fractions: Use decimals (0.3333, 0.6666) NOT symbols (1/3, 2/3)\n",
    "5. Completeness: Translate ALL including witness statements, dates, 'witnessed', 'sealed'\n",
    "6. Tense: Past tense for completed actions (sent, received, delivered, paid)\n",
    "7. Structure: Keep legal format, periods, commas between items\n",
    "8. Do NOT use '...'. If text is broken, use <big_gap>\n",
    "Output: ONLY the complete English translation.\n",
    "\"\"\"\n",
    "\n",
    "def get_model_id(base_url: str) -> str:\n",
    "    r = requests.get(f\"{base_url}/v1/models\", timeout=10)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    data = j.get(\"data\", [])\n",
    "    if not data:\n",
    "        raise RuntimeError(f\"No models returned from {base_url}/v1/models: {j}\")\n",
    "    return data[0].get(\"id\") or data[0].get(\"model\") or \"default\"\n",
    "\n",
    "def chat_completions(\n",
    "    base_url: str,\n",
    "    messages: list[dict],\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    max_tokens: int,\n",
    "    top_k: int,\n",
    "    min_p: float,\n",
    "    top_p: float,\n",
    "    presence_penalty: float,\n",
    "    timeout_s: float,\n",
    "    retry_loading: bool,\n",
    ") -> dict:\n",
    "    url = f\"{base_url}/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_k\": top_k,\n",
    "        \"min_p\": min_p,\n",
    "        \"top_p\": top_p,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "    deadline = time.time() + timeout_s\n",
    "    last_err = None\n",
    "    while time.time() < deadline:\n",
    "        r = requests.post(url, json=payload, timeout=500)\n",
    "        if r.status_code in (200, 201):\n",
    "            return r.json()\n",
    "        if retry_loading and r.status_code == 503 and (\"model is loading\" in r.text.lower()):\n",
    "            last_err = \"loading\"\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        if r.status_code in (404, 405):\n",
    "            raise RuntimeError(\n",
    "                f\"{url} returned {r.status_code}. This server may not expose /v1/chat/completions. \"\n",
    "                f\"Body head: {r.text[:200]!r}\"\n",
    "            )\n",
    "        last_err = (r.status_code, r.text[:500])\n",
    "        r.raise_for_status()\n",
    "    raise TimeoutError(f\"chat_completions timed out within {timeout_s}s. last_err={last_err}\")\n",
    "\n",
    "\n",
    "def few_shot_to_messages(few_shot: list[dict]) -> list[dict]:\n",
    "    messages: list[dict] = []\n",
    "    for shot in few_shot:\n",
    "        src = shot[\"transliteration\"]\n",
    "        tgt = shot[\"translation\"]\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Just translate Akkadian(transliteration) to English: {src}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": tgt})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def build_messages(system_prompt: str, few_shot: list[dict], text: str) -> list[dict]:\n",
    "    few_shot_messages = few_shot_to_messages(few_shot)\n",
    "    if few_shot_messages is None or len(few_shot_messages) == 0:\n",
    "        return [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        + [{\"role\": \"user\",\"content\": f\"Don't output superfluous comments. Just translate Akkadian(transliteration) to English: {text}\"}]\n",
    "    return (\n",
    "        [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        + few_shot_messages\n",
    "        + [{\"role\": \"user\",\"content\": f\"Just translate Akkadian(transliteration) to English: {text}\"}]\n",
    "    )\n",
    "\n",
    "def predict_max_token(transliteration_len: int, ratio: float, upper_bound: int) -> int:\n",
    "    token_len = (transliteration_len / 4)\n",
    "    pred = min(token_len * ratio, upper_bound)\n",
    "    return int(pred)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"/kaggle/input/deep-past-initiative-machine-translation\")\n",
    "    parser.add_argument(\"--test_with_few_shot_path\", type=str, default=\"/kaggle/working/test_with_few_shot.parquet\")\n",
    "    parser.add_argument(\"--max_tokens_ub\", type=int, default=2048)\n",
    "    parser.add_argument(\"--base_url\", type=str, default=\"http://127.0.0.1:8080\")\n",
    "    parser.add_argument(\"--out\", type=str, default=\"submission.csv\")\n",
    "    args = parser.parse_args()\n",
    "    data_dir = Path(args.data_dir)\n",
    "    #test_path = data_dir / \"test.csv\"\n",
    "    sample_sub_path = data_dir / \"sample_submission.csv\"\n",
    "    #test_df = pd.read_csv(test_path)\n",
    "    test_df = pd.read_parquet(args.test_with_few_shot_path)\n",
    "    sub_df = pd.read_csv(sample_sub_path)\n",
    "    tgt_col = \"translation\"\n",
    "    source_col = \"transliteration\"\n",
    "    few_shot_col = \"few_shot\"\n",
    "    \n",
    "    model_id = get_model_id(args.base_url)\n",
    "    # Translate\n",
    "    outputs = []\n",
    "    for i, row in tqdm(test_df.iterrows()):\n",
    "        text = row[source_col]\n",
    "        few_shot = row[few_shot_col]\n",
    "        messages = build_messages(SYSTEM_PROMPT, few_shot, text)\n",
    "        max_tokens = predict_max_token(len(text), 1.5, args.max_tokens_ub)\n",
    "\n",
    "        response = chat_completions(\n",
    "            args.base_url,\n",
    "            messages=messages,\n",
    "            model=model_id,\n",
    "            temperature=0.7,\n",
    "            max_tokens=max_tokens,\n",
    "            top_k=20,\n",
    "            min_p=0.00,\n",
    "            top_p=0.80,\n",
    "            presence_penalty=1.0,\n",
    "            timeout_s=600.0,\n",
    "            retry_loading=True,\n",
    "        )\n",
    "        outputs.append(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "    # Fill submission in the exact column layout of sample_submission\n",
    "    sub_df[tgt_col] = outputs\n",
    "    out_path = Path(args.out)\n",
    "    sub_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved: {out_path.resolve()}\")\n",
    "    print(sub_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65003627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:15.915427Z",
     "iopub.status.busy": "2026-02-11T11:03:15.915038Z",
     "iopub.status.idle": "2026-02-11T11:03:19.664689Z",
     "shell.execute_reply": "2026-02-11T11:03:19.663765Z"
    },
    "papermill": {
     "duration": 3.755063,
     "end_time": "2026-02-11T11:03:19.666537",
     "exception": false,
     "start_time": "2026-02-11T11:03:15.911474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 63.14it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python retriever.py --data_dir /kaggle/input/deep-past-initiative-machine-translation --out test_with_few_shot.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a39c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:03:19.673812Z",
     "iopub.status.busy": "2026-02-11T11:03:19.673554Z",
     "iopub.status.idle": "2026-02-11T11:06:24.723869Z",
     "shell.execute_reply": "2026-02-11T11:06:24.723121Z"
    },
    "papermill": {
     "duration": 185.056106,
     "end_time": "2026-02-11T11:06:24.725721",
     "exception": false,
     "start_time": "2026-02-11T11:03:19.669615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_server file: /kaggle/working/llama_server.py\r\n",
      "LlamaServer.__init__: (self, server_bin: str, model_path: str, host: str = '127.0.0.1', port: int = 8080, n_gpu_layers: int = 30, n_cpu_moe: int = 19, ctx: int = 7168, parallel: int = 1, threads: int = 4, cram: int = 5120, extra_args=None, workdir: str | None = None, ld_library_path: str | None = None, log_path: str | None = None)\r\n",
      "server up: http://127.0.0.1:8080\r\n",
      "{'models': [{'name': 'Qwen3-Next-80B-A3B-Instruct-Q4_K_M.gguf', 'model': 'Qwen3-Next-80B-A3B-Instruct-Q4_K_M.gguf', 'modified_at': '', 'size': '', 'digest': '', 'type': 'model', 'description': '', 'tags': [''], 'capabilities': ['completion'], 'parameters': '', 'details': {'parent_model': '', 'format': 'gguf', 'family': '', 'families': [''], 'parameter_size': '', 'quantization_level': ''}}], 'object': 'list', 'data': [{'id': 'Qwen3-Next-80B-A3B-Instruct-Q4_K_M.gguf', 'object': 'model', 'created': 1770807984, 'owned_by': 'llamacpp', 'meta': {'vocab_type': 2, 'n_vocab': 151936, 'n_ctx_train': 262144, 'n_embd': 2048, 'n_params': 79674391296, 'size': 48500114432}}]}\r\n"
     ]
    }
   ],
   "source": [
    "!python run_server.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aecc5fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:06:24.733980Z",
     "iopub.status.busy": "2026-02-11T11:06:24.733231Z",
     "iopub.status.idle": "2026-02-11T11:08:04.340533Z",
     "shell.execute_reply": "2026-02-11T11:08:04.339807Z"
    },
    "papermill": {
     "duration": 99.613658,
     "end_time": "2026-02-11T11:08:04.342421",
     "exception": false,
     "start_time": "2026-02-11T11:06:24.728763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4it [01:38, 24.69s/it]\r\n",
      "Saved: /kaggle/working/submission.csv\r\n",
      "   id                                        translation\r\n",
      "0   0  Thus, Kanesh, say to Aqil...: To the payers, o...\r\n",
      "1   1  From this day on, whoever buys meteoric iron, ...\r\n",
      "2   2  As soon as you have heard our letter, whoever ...\r\n",
      "3   3  Send a copy of this letter to every colony and...\r\n"
     ]
    }
   ],
   "source": [
    "!python submit.py --data_dir /kaggle/input/deep-past-initiative-machine-translation --out submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02a728ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T11:08:04.350488Z",
     "iopub.status.busy": "2026-02-11T11:08:04.350021Z",
     "iopub.status.idle": "2026-02-11T11:08:04.588266Z",
     "shell.execute_reply": "2026-02-11T11:08:04.587388Z"
    },
    "papermill": {
     "duration": 0.244938,
     "end_time": "2026-02-11T11:08:04.590640",
     "exception": false,
     "start_time": "2026-02-11T11:08:04.345702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pgrep -f \"llama-server\" | xargs kill -9\n",
    "!pgrep -f \"run_server.py\" | xargs kill -9"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9380868,
     "sourceId": 14684353,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9381348,
     "sourceId": 14685090,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 299.089264,
   "end_time": "2026-02-11T11:08:04.915945",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-11T11:03:05.826681",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
