{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dalloliogm/tidymodels-calories?scriptVersionId=237392380\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Prediction using Tidymodels in R\n\nI like tidymodel's syntax in R. I think it is very elegant. Nothing against Python - I just like the tidyverse approach, it seems very readable.","metadata":{}},{"cell_type":"code","source":"\nlibrary(tidyverse) # metapackage of all tidyverse packages\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(workflowsets)\noptions(repr.plot.width = 22, repr.plot.height = 12)\n\n\nn_trees = 50\n","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = read_csv(\"../input/playground-series-s5e5/train.csv\") #%>% \n#    head (2500) # THis dataset is so big that for debugging, I just read some of the rows.\ntrain %>% glimpse\ntest = read_csv(\"../input/playground-series-s5e5/test.csv\")\ntrain_test = bind_rows(\n    train %>% mutate(dataset=\"train\"),\n    test %>% mutate(dataset = \"test\")\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission = read_csv(\"../input/playground-series-s5e5/submission.csv\")\n#submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# My own version of DataExplorer\ndevtools::install_github(\"dalloliogm/DataExplorer\", dependencies=F)\nlibrary(DataExplorer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generate an automatic report\n\nThe DataExplorer library generates a report showing data distribution and other EDA. It is saved to the output folder.","metadata":{}},{"cell_type":"code","source":"#create_report(train_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"introduce(train_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Missing data\n\nLuckily, there is no missing data in the this dataset.","metadata":{}},{"cell_type":"code","source":"plot_missing(train_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"plot_str(train_test)","metadata":{}},{"cell_type":"markdown","source":"### Distribution of numerical columns\n\nAlmost all the features in the dataset are numerical, except for Gender. Here we plot their distribution, keeping an eye for different shapes between train and test, which could bias our prediction. It seems there are no biases between train and test.","metadata":{}},{"cell_type":"code","source":"plot_histogram(train_test, fill=dataset, position=\"identity\", alpha=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Distribution of Categorical variables\n\nWe have exactly the same distribution of M/F between train and test","metadata":{}},{"cell_type":"code","source":"plot_bar(train_test, fill=dataset, alpha=0.2, by=\"dataset\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Distribution of the predicted variable\n\nLet's zoom in into Calories, the predicted variable. It is not a normal distribution - it is very right-skewed, with a tail to the right.\n\nWhat are the implications of this?\n\n- for tree based model, like Random Forest and XGBOost, there it should be no impact\n- for regression models, it may be beneficial to transform the data to make Calories more normal.","metadata":{}},{"cell_type":"code","source":"train %>% ggplot(aes(x=Calories)) + geom_density()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Correlation between variables\n\n\nLet's look at the correlation between variables, without any data transformation.\n\nIt is interesitng to see that Body_Temp, Heart_Rate, Duration already have good correlation with Calories.\n\nOther variables have correlation to each other - like Weight and Height. Sex_male and Sex_female are anticorrelated, of course.","metadata":{}},{"cell_type":"code","source":"plot_correlation(train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining Recipes\n\nWe define different recipes for Regression and Tree-based models.\n\nFor regressions:\n- we include an interaction between Age and Duration, and other interactions based on the correlation plot.\n- We remove predictors that are too correlated with each other, as shown in the correlation plot\n- We also transform Calories into log1p to make them more gaussian - we'll need to remember to transform it back in test.\n\nFor Trees, we one-hot encode the Sex category, although it is not strictly required in R.","metadata":{}},{"cell_type":"code","source":"# For linear models: log-transform + normalize\ntrain <- train %>% mutate(log_calories = log1p(Calories))\n\nrecipe_lm <- recipe(log_calories ~ ., data = train) %>%\n  step_rm(id, Calories) %>%\n  step_dummy(Sex) %>%\n  step_interact(terms = ~ Age:Duration) %>%\n  step_interact(terms = ~ Duration:Heart_Rate + Duration:Body_Temp + Heart_Rate:Body_Temp) %>%\n  step_corr(all_numeric_predictors(), threshold = 0.9) %>%\n  step_normalize(all_numeric_predictors()) \n\n# For tree models: raw target + one-hot encoding\nrecipe_tree <- recipe(Calories ~ ., data = train) %>%\n  step_rm(id) %>%\n  step_dummy(Sex, one_hot = TRUE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"recipe_lm %>% prep %>% juice %>% head","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Did the log-transformation on calories work, transforming it into a normal variable?\ntrain %>% ggplot(aes(x=log_calories)) + geom_density()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"recipe_tree %>% prep %>% juice %>% head","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Specifications\n\n","metadata":{}},{"cell_type":"code","source":"# Linear Regression\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"code","source":"# Random Forest (tunable)\nrf_spec <- rand_forest(\n  mtry = tune(),\n  min_n = tune(),\n  trees = n_trees\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# XGBoost (tunable)\nxgb_spec <- boost_tree(\n  trees = n_trees,\n  learn_rate = tune(),\n  tree_depth = tune()\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_set <- workflow_set(\n  preproc = list(\n    linear = recipe_lm,\n    tree = recipe_tree\n  ),\n  models = list(\n    lm = lm_spec,\n    rf = rf_spec,\n    xgb = xgb_spec\n  ),\n  cross = TRUE  # try all recipe/model combinations\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"# XGBoost model\nxgb_model <- boost_tree(\n  trees = 300,\n  tree_depth = tune(), \n  learn_rate = tune(),\n  loss_reduction = tune()\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\n# Random Forest model\nrf_model <- rand_forest(\n  mtry = tune(),\n  trees = 50,\n  min_n = tune()\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Workflows","metadata":{}},{"cell_type":"code","source":"model_set <- workflow_set(\n  preproc = list(\n    linear = recipe_lm,\n    tree = recipe_tree\n  ),\n  models = list(\n    lm = lm_spec,\n    rf = rf_spec,\n    xgb = xgb_spec\n  ),\n  cross = TRUE  # try all recipe/model combinations\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tune all workflows with Cross Validation\n\nK-Fold Cross-Validation should be fine for this dataset, as there is no bias in the distribution of any of the variables.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get number of predictors after recipe\nprep_tree <- prep(recipe_tree, training = train)\njuice_tree <- juice(prep_tree)\nmtry_range <- finalize(mtry(), juice_tree)\n\n# Custom grid using the finalized range\nparam_grid <- grid_latin_hypercube(\n  mtry(range = mtry_range),\n  min_n(),\n  tree_depth(),\n  learn_rate(),\n  size = 10\n)\n\n# Tune with custom grid\nresults <- model_set %>%\n  workflow_map(\n    \"tune_grid\",\n    resamples = folds,\n    grid = param_grid,\n    metrics = metric_set(rmse),\n    control = control_grid(save_workflow = TRUE),\n    verbose = TRUE\n  )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Tune Random Forest\n#rf_res <- tune_grid(\n#  rf_workflow,\n#  resamples = folds,\n#  grid = 10,\n#  metrics = metric_set(rmse)\n#)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### COmpare performances","metadata":{}},{"cell_type":"code","source":"autoplot(results)\ncollect_metrics(results) %>%\n  filter(.metric == \"rmse\") %>%\n  arrange(mean)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Select best workflow\n\nWe select the best workflow for each approach. We then ensemble them together.","metadata":{}},{"cell_type":"code","source":"# Linear model\nbest_lm <- extract_workflow_set_result(results, \"linear_lm\") %>% select_best(\"rmse\")\nwf_lm <- extract_workflow(results, \"linear_lm\") %>% finalize_workflow(best_lm)\nfit_lm <- fit(wf_lm, data = train)\n\n# XGBoost\nbest_xgb <- extract_workflow_set_result(results, \"tree_xgb\") %>% select_best(\"rmse\")\nwf_xgb <- extract_workflow(results, \"tree_xgb\") %>% finalize_workflow(best_xgb)\nfit_xgb <- fit(wf_xgb, data = train)\n\n# Random Forest\nbest_rf <- extract_workflow_set_result(results, \"tree_rf\") %>% select_best(\"rmse\")\nwf_rf <- extract_workflow(results, \"tree_rf\") %>% finalize_workflow(best_rf)\nfit_rf <- fit(wf_rf, data = train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Make Predictions\n\nRemember to transform Calories back, after log transforming it.","metadata":{}},{"cell_type":"code","source":"pred_lm <- predict(fit_lm, new_data = test)  # linear model (log-scale)\npred_xgb <- predict(fit_xgb, new_data = test)\npred_rf <- predict(fit_rf, new_data = test)\npred_lm <- mutate(pred_lm, .pred = expm1(.pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Average Predictions\n\nWe can use an average weight, if one model performs better than the others.","metadata":{}},{"cell_type":"code","source":"ensemble_pred <- (pred_lm$.pred + pred_xgb$.pred + pred_rf$.pred) / 3\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create Submission file","metadata":{}},{"cell_type":"code","source":"submission <- tibble(\n  id = test$id,\n  Calories = ensemble_pred\n)\n\nwrite_csv(submission, \"submission.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = results %>% select(id, predictions)\nsubmission %>% write_csv(\"submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}