{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2608ad5b",
   "metadata": {
    "papermill": {
     "duration": 0.004131,
     "end_time": "2025-10-20T09:54:35.564989",
     "exception": false,
     "start_time": "2025-10-20T09:54:35.560858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "forked from: \n",
    "- https://www.kaggle.com/code/subarnasaikia/cafa-6-base-model-lb-0-209/\n",
    "- https://www.kaggle.com/code/nihilisticneuralnet/protbert-ensemble/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0311dfb",
   "metadata": {
    "papermill": {
     "duration": 0.003048,
     "end_time": "2025-10-20T09:54:35.571550",
     "exception": false,
     "start_time": "2025-10-20T09:54:35.568502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58f7fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:54:35.579072Z",
     "iopub.status.busy": "2025-10-20T09:54:35.578852Z",
     "iopub.status.idle": "2025-10-20T09:54:35.587159Z",
     "shell.execute_reply": "2025-10-20T09:54:35.586540Z"
    },
    "papermill": {
     "duration": 0.013291,
     "end_time": "2025-10-20T09:54:35.588285",
     "exception": false,
     "start_time": "2025-10-20T09:54:35.574994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are listed!!!\n"
     ]
    }
   ],
   "source": [
    "# TSV FILES\n",
    "SAMPLE_SUBMISSION_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/sample_submission.tsv\"\n",
    "IA_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\"\n",
    "TESTSUPERSET_TAXON_LIST_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\"\n",
    "TRAIN_TERMS_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\n",
    "TRAIN_TAXONOMY_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\"\n",
    "\n",
    "# FASTA FILES\n",
    "TESTSUPERSET_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"\n",
    "TRAIN_SEQUENCES_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\n",
    "\n",
    "# OBO FILE\n",
    "GO_BASIC_OBO = \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n",
    "\n",
    "# OUTPUT FILE\n",
    "OUTPUT_TSV = \"/kaggle/working/submission1.tsv\"\n",
    "\n",
    "print(\"Files are listed!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941c373",
   "metadata": {
    "papermill": {
     "duration": 0.003181,
     "end_time": "2025-10-20T09:54:35.594857",
     "exception": false,
     "start_time": "2025-10-20T09:54:35.591676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee7d703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:54:35.602609Z",
     "iopub.status.busy": "2025-10-20T09:54:35.601893Z",
     "iopub.status.idle": "2025-10-20T09:54:35.607344Z",
     "shell.execute_reply": "2025-10-20T09:54:35.606721Z"
    },
    "papermill": {
     "duration": 0.010237,
     "end_time": "2025-10-20T09:54:35.608397",
     "exception": false,
     "start_time": "2025-10-20T09:54:35.598160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config...\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    \"TRAIN_FASTA\": TRAIN_SEQUENCES_FASTA,\n",
    "    \"TRAIN_TERMS\": TRAIN_TERMS_TSV,\n",
    "    \"TRAIN_TAXONOMY\": TRAIN_TAXONOMY_TSV,\n",
    "    \"GO_OBO\": GO_BASIC_OBO,\n",
    "    \"IA_FILE\": IA_TSV,\n",
    "    \"TEST_FASTA\": TESTSUPERSET_FASTA,\n",
    "    \"TEST_TAXON_LIST\": TESTSUPERSET_TAXON_LIST_TSV,\n",
    "    \"SAMPLE_SUBMISSION\": SAMPLE_SUBMISSION_TSV,\n",
    "    \"OUTPUT_SUBMISSION\": OUTPUT_TSV,\n",
    "    # Embedding settings:\n",
    "    \"USE_PLM_MODEL\": False,  # set False to force TF-IDF fallback\n",
    "    # If using TF/HF transformer model, either place checkpoint in dataset and point here,\n",
    "    # or use model name if internet enabled. On Kaggle usually you must provide local model.\n",
    "    \"PLM_MODEL_NAME_OR_PATH\": \"/kaggle/input/esm-2/keras/esm2_t6_8m/1\",  \n",
    "    \"PLM_BATCH_SIZE\": 8,\n",
    "    # Memory & batch sizes for streaming\n",
    "    \"EMBED_BATCH_SIZE\": 8,          # batch size used when embedding (train & test)\n",
    "    \"PREDICT_BATCH_SIZE\": 64,       # how many examples to predict at once (fit to memory)\n",
    "    # Label limitation to keep model small\n",
    "    \"TOP_K_LABELS\": 3000,\n",
    "    # Model training hyperparams\n",
    "    \"RANDOM_SEED\": 42,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"EPOCHS\": 10,\n",
    "    \"LEARNING_RATE\": 1e-3,\n",
    "    \"HIDDEN_UNITS\": 512,\n",
    "    \"DROPOUT\": 0.2,\n",
    "    # Submission postprocessing\n",
    "    \"TOP_K_PER_PROTEIN\": 200,\n",
    "    \"GLOBAL_THRESHOLD_SEARCH\": True,\n",
    "    \"THRESHOLD_GRID\": [i/100 for i in range(1, 51)],\n",
    "    # Propagation\n",
    "    \"PROPAGATE_TRAIN_LABELS\": True,\n",
    "    \"PROPAGATE_PREDICTIONS\": True,\n",
    "\n",
    "    # On-disk paths for memmaps/embeddings\n",
    "    \"TRAIN_EMB_MEMMAP\": \"/kaggle/working/train_embs.memmap\",\n",
    "    \"TRAIN_EMB_SHAPE_FILE\": \"/kaggle/working/train_embs_shape.npy\",\n",
    "}\n",
    "\n",
    "\n",
    "print(\"config...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecd0313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:54:35.615872Z",
     "iopub.status.busy": "2025-10-20T09:54:35.615500Z",
     "iopub.status.idle": "2025-10-20T09:55:02.669988Z",
     "shell.execute_reply": "2025-10-20T09:55:02.669049Z"
    },
    "papermill": {
     "duration": 27.059377,
     "end_time": "2025-10-20T09:55:02.671102",
     "exception": false,
     "start_time": "2025-10-20T09:54:35.611725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 09:54:39.911557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760954080.134930      34 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760954080.188146      34 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done!!!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# imports\n",
    "# ------------------------------------------------------------\n",
    "import os, gc, math, random, sys, time\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, losses, metrics\n",
    "import esm   # after installing from GitHub\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# optional PLM imports\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "# deterministic seeds\n",
    "random.seed(CONFIG[\"RANDOM_SEED\"])\n",
    "np.random.seed(CONFIG[\"RANDOM_SEED\"])\n",
    "tf.random.set_seed(CONFIG[\"RANDOM_SEED\"])\n",
    "\n",
    "print(\"import done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6561f080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:55:02.679749Z",
     "iopub.status.busy": "2025-10-20T09:55:02.679324Z",
     "iopub.status.idle": "2025-10-20T09:55:02.689299Z",
     "shell.execute_reply": "2025-10-20T09:55:02.688687Z"
    },
    "papermill": {
     "duration": 0.015665,
     "end_time": "2025-10-20T09:55:02.690452",
     "exception": false,
     "start_time": "2025-10-20T09:55:02.674787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# tiny utils for FASTA/TSV/OBO parsing \n",
    "# ------------------------------------------------------------\n",
    "def read_fasta(path: str) -> Dict[str, str]:\n",
    "    seqs = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        pid = None; seq_parts = []\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if pid: seqs[pid] = \"\".join(seq_parts)\n",
    "                header=line[1:].split()[0]\n",
    "                if \"|\" in header:\n",
    "                    parts=header.split(\"|\"); pid = parts[1] if len(parts)>=2 else header\n",
    "                else:\n",
    "                    pid = header\n",
    "                seq_parts=[]\n",
    "            else:\n",
    "                seq_parts.append(line.strip())\n",
    "        if pid: seqs[pid] = \"\".join(seq_parts)\n",
    "    print(f\"[io] Read {len(seqs)} sequences from {path}\")\n",
    "    return seqs\n",
    "\n",
    "def read_train_terms(path: str) -> Dict[str, List[str]]:\n",
    "    mapping = defaultdict(list)\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"protein\",\"go\",\"ont\"], dtype=str)\n",
    "    for _, r in df.iterrows(): mapping[r.protein].append(r.go)\n",
    "    print(f\"[io] Read training annotations for {len(mapping)} proteins from {path}\")\n",
    "    return mapping\n",
    "\n",
    "def parse_obo(go_obo_path: str) -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]:\n",
    "    parents = defaultdict(set); children = defaultdict(set)\n",
    "    if not os.path.exists(go_obo_path): return parents, children\n",
    "    with open(go_obo_path,\"r\") as f:\n",
    "        cur_id=None\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line==\"[Term]\": cur_id=None\n",
    "            elif line.startswith(\"id: \"): cur_id=line.split(\"id: \")[1].strip()\n",
    "            elif line.startswith(\"is_a: \"):\n",
    "                pid=line.split()[1].strip()\n",
    "                if cur_id: parents[cur_id].add(pid); children[pid].add(cur_id)\n",
    "            elif line.startswith(\"relationship: part_of \"):\n",
    "                parts=line.split(); \n",
    "                if len(parts)>=3:\n",
    "                    pid=parts[2].strip()\n",
    "                    if cur_id: parents[cur_id].add(pid); children[pid].add(cur_id)\n",
    "    print(f\"[io] Parsed OBO: {len(parents)} nodes with parents\")\n",
    "    return parents, children\n",
    "\n",
    "def get_ancestors(go_id: str, parents: Dict[str, Set[str]]) -> Set[str]:\n",
    "    ans=set(); stack=[go_id]\n",
    "    while stack:\n",
    "        cur=stack.pop()\n",
    "        for p in parents.get(cur,[]): \n",
    "            if p not in ans:\n",
    "                ans.add(p); stack.append(p)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e2471e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:55:02.698477Z",
     "iopub.status.busy": "2025-10-20T09:55:02.698250Z",
     "iopub.status.idle": "2025-10-20T09:55:35.034767Z",
     "shell.execute_reply": "2025-10-20T09:55:35.033885Z"
    },
    "papermill": {
     "duration": 32.345253,
     "end_time": "2025-10-20T09:55:35.039335",
     "exception": false,
     "start_time": "2025-10-20T09:55:02.694082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[io] Read 82404 sequences from /kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\n",
      "[io] Read training annotations for 82405 proteins from /kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\n",
      "[io] Parsed OBO: 40121 nodes with parents\n",
      "[io] Read 224309 sequences from /kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\n",
      "[io] 82404 train proteins with sequences available\n",
      "[prep] Propagating train labels up GO graph\n",
      "[prep] Restricting to top-3000 GO terms\n",
      "[prep] Using 3000 target GO terms\n",
      "[prep] Label matrix shape: (82404, 3000)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load the data\n",
    "# ------------------------------------------------------------\n",
    "train_seqs = read_fasta(CONFIG[\"TRAIN_FASTA\"])\n",
    "train_terms = read_train_terms(CONFIG[\"TRAIN_TERMS\"])\n",
    "parents_map, children_map = parse_obo(CONFIG[\"GO_OBO\"])\n",
    "test_seqs = read_fasta(CONFIG[\"TEST_FASTA\"])\n",
    "\n",
    "# Keep proteins present in both seq & terms\n",
    "train_proteins = [p for p in train_terms.keys() if p in train_seqs]\n",
    "print(f\"[io] {len(train_proteins)} train proteins with sequences available\")\n",
    "\n",
    "# propagate train labels (optional)\n",
    "if CONFIG[\"PROPAGATE_TRAIN_LABELS\"] and parents_map:\n",
    "    print(\"[prep] Propagating train labels up GO graph\")\n",
    "    propagated={}\n",
    "    for p in train_proteins:\n",
    "        terms=set(train_terms[p])\n",
    "        extra=set()\n",
    "        for t in list(terms): extra |= get_ancestors(t, parents_map)\n",
    "        propagated[p]=sorted(terms|extra)\n",
    "    train_terms = propagated\n",
    "\n",
    "# choose top-k labels (to control model size)\n",
    "all_term_counts = Counter()\n",
    "for p in train_proteins: all_term_counts.update(train_terms[p])\n",
    "all_terms_sorted = [t for t,_ in all_term_counts.most_common()]\n",
    "if CONFIG[\"TOP_K_LABELS\"] is not None:\n",
    "    chosen_terms = set(all_terms_sorted[:CONFIG[\"TOP_K_LABELS\"]])\n",
    "    print(f\"[prep] Restricting to top-{CONFIG['TOP_K_LABELS']} GO terms\")\n",
    "else:\n",
    "    chosen_terms = set(all_terms_sorted)\n",
    "print(f\"[prep] Using {len(chosen_terms)} target GO terms\")\n",
    "\n",
    "for p in train_proteins:\n",
    "    train_terms[p] = [t for t in train_terms[p] if t in chosen_terms]\n",
    "\n",
    "X_proteins = train_proteins\n",
    "y_labels = [train_terms[p] for p in X_proteins]\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=sorted(chosen_terms))\n",
    "Y = mlb.fit_transform(y_labels).astype(np.float32)\n",
    "print(\"[prep] Label matrix shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17279d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:55:35.047906Z",
     "iopub.status.busy": "2025-10-20T09:55:35.047677Z",
     "iopub.status.idle": "2025-10-20T09:55:35.101296Z",
     "shell.execute_reply": "2025-10-20T09:55:35.100543Z"
    },
    "papermill": {
     "duration": 0.059368,
     "end_time": "2025-10-20T09:55:35.102458",
     "exception": false,
     "start_time": "2025-10-20T09:55:35.043090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# PLM embedding helpers (ESM2); memory-conscious: produce numpy arrays per-batch\n",
    "# ------------------------------------------------------------\n",
    "def seqs_for_plm_input_esm(seqs: List[str]) -> List[str]:\n",
    "    # ESM expects raw sequences (no spaces); we simply uppercase and replace unknowns\n",
    "    out=[]\n",
    "    for s in seqs:\n",
    "        s2 = s.upper().replace(\"U\",\"X\").replace(\"O\",\"X\").replace(\"B\",\"X\").replace(\"Z\",\"X\")\n",
    "        out.append(s2)\n",
    "    return out\n",
    "\n",
    "def embed_with_plm_to_memmap(all_seq_ids: List[str],\n",
    "                             seqs_dict: Dict[str,str],\n",
    "                             memmap_path: str,\n",
    "                             batch_size:int=8,\n",
    "                             model_name_or_path: str = CONFIG[\"PLM_MODEL_NAME_OR_PATH\"]):\n",
    "    \"\"\"\n",
    "    Compute embeddings using the ESM loader and write to disk-backed memmap.\n",
    "    Returns memmap object and embedding dimension.\n",
    "    This function expects the local directory model_name_or_path to contain the ESM checkpoint\n",
    "    produced by the ESM tooling (e.g., esm2_t33_650M_UR50D.pt or a model dir).\n",
    "    \"\"\"\n",
    "    if not TORCH_AVAILABLE:\n",
    "        raise RuntimeError(\"Torch not available; cannot load ESM model.\")\n",
    "\n",
    "    model_dir = str(model_name_or_path)\n",
    "    if not Path(model_dir).exists():\n",
    "        raise FileNotFoundError(f\"ESM model path not found: {model_dir}\")\n",
    "\n",
    "    # Try to load via esm loader that understands local formats\n",
    "    try:\n",
    "        # If model_name_or_path is a directory that contains a model checkpoint,\n",
    "        # this loader will try to read it. If it points directly to a .pt file it also works.\n",
    "        print(f\"[esm] Loading local ESM model from: {model_dir}\")\n",
    "        model, alphabet = esm.pretrained.load_model_and_alphabet_local(model_dir)\n",
    "    except Exception as e:\n",
    "        # some ESM checkpoints use slightly different utilities - try the convenience function names:\n",
    "        try:\n",
    "            # If the user packaged a directory like \"esm2_t33_650M_UR50D\" that contains model.pt\n",
    "            print(f\"[esm] load_model_and_alphabet_local failed, attempting esm2 convenience loader...\")\n",
    "            model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()  # fallback: only works if model files are accessible\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(\"Failed to load ESM model via esm.pretrained. Ensure the directory contains a valid ESM checkpoint.\") from e\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    N = len(all_seq_ids)\n",
    "\n",
    "    # Determine embedding dimension using a tiny sample (1 sequence)\n",
    "    sample_seq = seqs_dict[all_seq_ids[0]]\n",
    "    _, _, sample_tokens = batch_converter([(all_seq_ids[0], sample_seq)])\n",
    "    sample_tokens = sample_tokens.to(device)\n",
    "    with torch.no_grad():\n",
    "        results = model(sample_tokens, repr_layers=[model.num_layers], return_contacts=False)\n",
    "        # pick the highest repr layer in results\n",
    "        repr_keys = sorted(results[\"representations\"].keys())\n",
    "        last_layer_key = repr_keys[-1]\n",
    "        emb_dim = results[\"representations\"][last_layer_key].shape[-1]\n",
    "    # create memmap file on disk (float32)\n",
    "    mem = np.memmap(memmap_path, dtype=np.float32, mode=\"w+\", shape=(N, int(emb_dim)))\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch_ids = all_seq_ids[i:i+batch_size]\n",
    "        # Prepare list of (id, seq) tuples\n",
    "        batch_pairs = [(pid, seqs_dict[pid]) for pid in batch_ids]\n",
    "        labels, sequences, tokens = batch_converter(batch_pairs)  # tokens: (B, L)\n",
    "        tokens = tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            results = model(tokens, repr_layers=[model.num_layers], return_contacts=False)\n",
    "            repr_keys = sorted(results[\"representations\"].keys())\n",
    "            last_layer_key = repr_keys[-1]\n",
    "            repr_tensor = results[\"representations\"][last_layer_key].cpu()   # (B, L, C)\n",
    "        # For each sequence, slice out residues and mean-pool (drop BOS/EOS token at positions 0 and -1)\n",
    "        for j, seq in enumerate(sequences):\n",
    "            seq_len = len(seq)\n",
    "            # ESM token layout: tokens include BOS at pos 0 and EOS at pos seq_len+1 so we slice 1:seq_len+1\n",
    "            seq_repr = repr_tensor[j, 1:seq_len+1, :]   # (seq_len, C)\n",
    "            seq_embed = seq_repr.mean(axis=0).numpy().astype(np.float32)\n",
    "            mem[idx + j, :] = seq_embed\n",
    "        idx += len(batch_ids)\n",
    "\n",
    "        # free intermediate tensors and empty CUDA cache\n",
    "        del tokens, results, repr_tensor, seq_repr, seq_embed\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(f\"[esm] Wrote embeddings {i}..{i+len(batch_ids)} / {N}\")\n",
    "\n",
    "    mem.flush()\n",
    "    print(f\"[esm] Finished writing memmap to {memmap_path} with dim {emb_dim}\")\n",
    "    return mem, int(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b394f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:55:35.110995Z",
     "iopub.status.busy": "2025-10-20T09:55:35.110774Z",
     "iopub.status.idle": "2025-10-20T09:56:10.551613Z",
     "shell.execute_reply": "2025-10-20T09:56:10.550816Z"
    },
    "papermill": {
     "duration": 35.449522,
     "end_time": "2025-10-20T09:56:10.555782",
     "exception": false,
     "start_time": "2025-10-20T09:55:35.106260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fallback] Using TF-IDF k-mer embeddings for train (memory-friendly for moderate sizes)\n",
      "[embed] Train embeddings: method=tfidf, shape=(82404, 8558), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Embedding training set (memmap) OR fallback TF-IDF (also memory-friendly)\n",
    "# ------------------------------------------------------------\n",
    "USE_PLM = CONFIG[\"USE_PLM_MODEL\"] and TORCH_AVAILABLE\n",
    "if USE_PLM:\n",
    "    # compute training embeddings to disk memmap to avoid storing huge array in RAM\n",
    "    train_ids = X_proteins\n",
    "    train_memmap_path = CONFIG[\"TRAIN_EMB_MEMMAP\"]\n",
    "    if not os.path.exists(train_memmap_path):\n",
    "        print(\"[plm] Computing train embeddings to memmap. This may take time but keeps RAM low.\")\n",
    "        train_mem, D = embed_with_plm_to_memmap(train_ids, train_seqs, train_memmap_path,\n",
    "                                                batch_size=CONFIG[\"EMBED_BATCH_SIZE\"],\n",
    "                                                model_name_or_path=CONFIG[\"PLM_MODEL_NAME_OR_PATH\"])\n",
    "        # Save shape info for later reopening\n",
    "        np.save(CONFIG[\"TRAIN_EMB_SHAPE_FILE\"], np.array([len(train_ids), D], dtype=np.int64))\n",
    "        # Keep mem as memmap object reference\n",
    "        emb_train = np.array(train_mem)  # small temporary conversion for training. If too large, we will reopen memmap later.\n",
    "        # To be safe, copy to np.float32 if necessary\n",
    "        if emb_train.dtype != np.float32: emb_train = emb_train.astype(np.float32)\n",
    "        del train_mem; gc.collect()\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    else:\n",
    "        # memmap already exists: load shape and open\n",
    "        shape = np.load(CONFIG[\"TRAIN_EMB_SHAPE_FILE\"])\n",
    "        Nshape, D = int(shape[0]), int(shape[1])\n",
    "        emb_train = np.memmap(train_memmap_path, dtype=np.float32, mode=\"r\", shape=(Nshape, D))\n",
    "    embedding_method = \"plm\"\n",
    "else:\n",
    "    # TF-IDF fallback (fits in memory usually; if not, you can also memmap)\n",
    "    print(\"[fallback] Using TF-IDF k-mer embeddings for train (memory-friendly for moderate sizes)\")\n",
    "    def kmers(seq, k=3):\n",
    "        return \" \".join([seq[i:i+k] for i in range(len(seq)-k+1)])\n",
    "    train_texts = [kmers(train_seqs[p], k=3) for p in X_proteins]\n",
    "    tfidf = TfidfVectorizer(analyzer=\"word\", token_pattern=r\"(?u)\\b\\w+\\b\", max_features=20000)\n",
    "    emb_train = tfidf.fit_transform(train_texts).astype(np.float32).toarray()\n",
    "    embedding_method = \"tfidf\"\n",
    "\n",
    "print(f\"[embed] Train embeddings: method={embedding_method}, shape={emb_train.shape}, dtype={emb_train.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35e1c10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:56:10.564454Z",
     "iopub.status.busy": "2025-10-20T09:56:10.564007Z",
     "iopub.status.idle": "2025-10-20T09:56:11.726262Z",
     "shell.execute_reply": "2025-10-20T09:56:11.725342Z"
    },
    "papermill": {
     "duration": 1.167848,
     "end_time": "2025-10-20T09:56:11.727607",
     "exception": false,
     "start_time": "2025-10-20T09:56:10.559759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] shapes: (70043, 8558) (12361, 8558) (70043, 3000) (12361, 3000)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Train / validation split (we load train embeddings into memory here - if it's too big, we can train with memmap directly)\n",
    "# ------------------------------------------------------------\n",
    "# If embeddings are memmap and too big, we can load a subset or use a generator; here we assume emb_train fits for training.\n",
    "X_emb = emb_train\n",
    "y = Y\n",
    "X_tr, X_val, y_tr, y_val, prot_tr, prot_val = train_test_split(\n",
    "    X_emb, y, X_proteins, test_size=0.15, random_state=CONFIG[\"RANDOM_SEED\"]\n",
    ")\n",
    "print(\"[train] shapes:\", X_tr.shape, X_val.shape, y_tr.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa0447d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:56:11.737150Z",
     "iopub.status.busy": "2025-10-20T09:56:11.736618Z",
     "iopub.status.idle": "2025-10-20T09:56:13.479539Z",
     "shell.execute_reply": "2025-10-20T09:56:13.478947Z"
    },
    "papermill": {
     "duration": 1.749,
     "end_time": "2025-10-20T09:56:13.480621",
     "exception": false,
     "start_time": "2025-10-20T09:56:11.731621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760954172.160252      34 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1760954172.160951      34 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8558</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,382,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">771,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8558\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m4,382,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3000\u001b[0m)           │       \u001b[38;5;34m771,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,287,608</span> (20.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,287,608\u001b[0m (20.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,286,072</span> (20.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,286,072\u001b[0m (20.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Keras model\n",
    "# ------------------------------------------------------------\n",
    "D = X_tr.shape[1]; M = y_tr.shape[1]\n",
    "def build_model(input_dim, output_dim, hidden_units=512, dropout=0.4, lr=1e-3):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(hidden_units, activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(hidden_units//2, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    out = layers.Dense(output_dim, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
    "                  loss=losses.BinaryCrossentropy(),\n",
    "                  metrics=[metrics.Precision(), metrics.Recall()])\n",
    "    return model\n",
    "\n",
    "model = build_model(D, M, hidden_units=CONFIG[\"HIDDEN_UNITS\"], dropout=CONFIG[\"DROPOUT\"], lr=CONFIG[\"LEARNING_RATE\"])\n",
    "model.summary()\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n",
    "ckpt_path = \"/kaggle/working/best_model.h5\"\n",
    "mc = callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "648def66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:56:13.490710Z",
     "iopub.status.busy": "2025-10-20T09:56:13.490496Z",
     "iopub.status.idle": "2025-10-20T09:57:21.632524Z",
     "shell.execute_reply": "2025-10-20T09:57:21.631862Z"
    },
    "papermill": {
     "duration": 68.148417,
     "end_time": "2025-10-20T09:57:21.633858",
     "exception": false,
     "start_time": "2025-10-20T09:56:13.485441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760954184.797539      77 service.cc:148] XLA service 0x7b5b4c00cd80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1760954184.798212      77 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1760954184.798235      77 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1760954185.167156      77 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1760954187.168371      77 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.04525, saving model to /kaggle/working/best_model.h5\n",
      "2189/2189 - 19s - 8ms/step - loss: 0.0768 - precision: 0.1386 - recall: 0.1651 - val_loss: 0.0452 - val_precision: 0.6441 - val_recall: 0.1723\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_loss improved from 0.04525 to 0.04447, saving model to /kaggle/working/best_model.h5\n",
      "2189/2189 - 8s - 4ms/step - loss: 0.0444 - precision: 0.6414 - recall: 0.1826 - val_loss: 0.0445 - val_precision: 0.6306 - val_recall: 0.1884\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_loss improved from 0.04447 to 0.04363, saving model to /kaggle/working/best_model.h5\n",
      "2189/2189 - 8s - 4ms/step - loss: 0.0414 - precision: 0.6570 - recall: 0.2063 - val_loss: 0.0436 - val_precision: 0.6162 - val_recall: 0.2075\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.04363\n",
      "2189/2189 - 8s - 4ms/step - loss: 0.0389 - precision: 0.6712 - recall: 0.2346 - val_loss: 0.0439 - val_precision: 0.6012 - val_recall: 0.2218\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.04363\n",
      "2189/2189 - 8s - 4ms/step - loss: 0.0367 - precision: 0.6830 - recall: 0.2660 - val_loss: 0.0445 - val_precision: 0.5814 - val_recall: 0.2295\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.04363\n",
      "2189/2189 - 8s - 4ms/step - loss: 0.0347 - precision: 0.6946 - recall: 0.2994 - val_loss: 0.0459 - val_precision: 0.5870 - val_recall: 0.2234\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Train\n",
    "# ------------------------------------------------------------\n",
    "history = model.fit(X_tr, y_tr, validation_data=(X_val, y_val),\n",
    "                    epochs=CONFIG[\"EPOCHS\"], batch_size=CONFIG[\"BATCH_SIZE\"],\n",
    "                    callbacks=[es, mc], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d8856b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:57:21.645775Z",
     "iopub.status.busy": "2025-10-20T09:57:21.645555Z",
     "iopub.status.idle": "2025-10-20T09:57:49.604987Z",
     "shell.execute_reply": "2025-10-20T09:57:49.603948Z"
    },
    "papermill": {
     "duration": 27.96696,
     "end_time": "2025-10-20T09:57:49.606470",
     "exception": false,
     "start_time": "2025-10-20T09:57:21.639510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval] best_thresh 0.06 best IA-weighted F1 0.09064167271397598\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Evaluate & select global threshold\n",
    "# ------------------------------------------------------------\n",
    "def weighted_precision_recall_f1(y_true, y_pred_bin, ia_map, mlb_obj):\n",
    "    tp = ((y_true==1)&(y_pred_bin==1)).sum(axis=0).astype(float)\n",
    "    fp = ((y_true==0)&(y_pred_bin==1)).sum(axis=0).astype(float)\n",
    "    fn = ((y_true==1)&(y_pred_bin==0)).sum(axis=0).astype(float)\n",
    "    eps=1e-12\n",
    "    prec = tp/(tp+fp+eps); rec = tp/(tp+fn+eps)\n",
    "    f1 = 2*prec*rec/(prec+rec+eps)\n",
    "    cls = mlb_obj.classes_\n",
    "    weights = np.array([ia_weights.get(c,1.0) for c in cls], dtype=float) if 'ia_weights' in globals() else np.ones(len(cls))\n",
    "    weighted_f1 = (f1*weights).sum()/(weights.sum()+eps)\n",
    "    weighted_prec = (prec*weights).sum()/(weights.sum()+eps)\n",
    "    weighted_rec = (rec*weights).sum()/(weights.sum()+eps)\n",
    "    return weighted_prec, weighted_rec, weighted_f1\n",
    "\n",
    "# load IA weights if available (safe)\n",
    "def read_IA_safe(path):\n",
    "    if not os.path.exists(path): return {}\n",
    "    df=pd.read_csv(path, sep=\"\\t\", header=None, names=[\"go\",\"ia\"], dtype=str)\n",
    "    d={}\n",
    "    for _,r in df.iterrows():\n",
    "        try: d[r.go]=float(r.ia)\n",
    "        except: \n",
    "            try: d[r.go]=float(r.ia.replace(\",\",\".\")) \n",
    "            except: d[r.go]=0.0\n",
    "    return d\n",
    "\n",
    "ia_weights = read_IA_safe(CONFIG[\"IA_FILE\"])\n",
    "\n",
    "y_val_prob = model.predict(X_val, batch_size=CONFIG[\"BATCH_SIZE\"], verbose=0)\n",
    "best_thresh = 0.5; best_score = -1.0\n",
    "if CONFIG[\"GLOBAL_THRESHOLD_SEARCH\"]:\n",
    "    for t in CONFIG[\"THRESHOLD_GRID\"]:\n",
    "        y_pred_bin = (y_val_prob >= t).astype(int)\n",
    "        _,_,f1 = weighted_precision_recall_f1(y_val, y_pred_bin, ia_weights, mlb)\n",
    "        if f1 > best_score: best_score=f1; best_thresh=t\n",
    "print(f\"[eval] best_thresh {best_thresh} best IA-weighted F1 {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a31d4d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:57:49.619488Z",
     "iopub.status.busy": "2025-10-20T09:57:49.619223Z",
     "iopub.status.idle": "2025-10-20T10:34:36.164244Z",
     "shell.execute_reply": "2025-10-20T10:34:36.163396Z"
    },
    "papermill": {
     "duration": 2206.553084,
     "end_time": "2025-10-20T10:34:36.165620",
     "exception": false,
     "start_time": "2025-10-20T09:57:49.612536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Streaming 224309 test sequences in batches of 8 (embed) / predict 64\n",
      "[stream] processed 0 / 224309\n",
      "[stream] processed 400 / 224309\n",
      "[stream] processed 800 / 224309\n",
      "[stream] processed 1200 / 224309\n",
      "[stream] processed 1600 / 224309\n",
      "[stream] processed 2000 / 224309\n",
      "[stream] processed 2400 / 224309\n",
      "[stream] processed 2800 / 224309\n",
      "[stream] processed 3200 / 224309\n",
      "[stream] processed 3600 / 224309\n",
      "[stream] processed 4000 / 224309\n",
      "[stream] processed 4400 / 224309\n",
      "[stream] processed 4800 / 224309\n",
      "[stream] processed 5200 / 224309\n",
      "[stream] processed 5600 / 224309\n",
      "[stream] processed 6000 / 224309\n",
      "[stream] processed 6400 / 224309\n",
      "[stream] processed 6800 / 224309\n",
      "[stream] processed 7200 / 224309\n",
      "[stream] processed 7600 / 224309\n",
      "[stream] processed 8000 / 224309\n",
      "[stream] processed 8400 / 224309\n",
      "[stream] processed 8800 / 224309\n",
      "[stream] processed 9200 / 224309\n",
      "[stream] processed 9600 / 224309\n",
      "[stream] processed 10000 / 224309\n",
      "[stream] processed 10400 / 224309\n",
      "[stream] processed 10800 / 224309\n",
      "[stream] processed 11200 / 224309\n",
      "[stream] processed 11600 / 224309\n",
      "[stream] processed 12000 / 224309\n",
      "[stream] processed 12400 / 224309\n",
      "[stream] processed 12800 / 224309\n",
      "[stream] processed 13200 / 224309\n",
      "[stream] processed 13600 / 224309\n",
      "[stream] processed 14000 / 224309\n",
      "[stream] processed 14400 / 224309\n",
      "[stream] processed 14800 / 224309\n",
      "[stream] processed 15200 / 224309\n",
      "[stream] processed 15600 / 224309\n",
      "[stream] processed 16000 / 224309\n",
      "[stream] processed 16400 / 224309\n",
      "[stream] processed 16800 / 224309\n",
      "[stream] processed 17200 / 224309\n",
      "[stream] processed 17600 / 224309\n",
      "[stream] processed 18000 / 224309\n",
      "[stream] processed 18400 / 224309\n",
      "[stream] processed 18800 / 224309\n",
      "[stream] processed 19200 / 224309\n",
      "[stream] processed 19600 / 224309\n",
      "[stream] processed 20000 / 224309\n",
      "[stream] processed 20400 / 224309\n",
      "[stream] processed 20800 / 224309\n",
      "[stream] processed 21200 / 224309\n",
      "[stream] processed 21600 / 224309\n",
      "[stream] processed 22000 / 224309\n",
      "[stream] processed 22400 / 224309\n",
      "[stream] processed 22800 / 224309\n",
      "[stream] processed 23200 / 224309\n",
      "[stream] processed 23600 / 224309\n",
      "[stream] processed 24000 / 224309\n",
      "[stream] processed 24400 / 224309\n",
      "[stream] processed 24800 / 224309\n",
      "[stream] processed 25200 / 224309\n",
      "[stream] processed 25600 / 224309\n",
      "[stream] processed 26000 / 224309\n",
      "[stream] processed 26400 / 224309\n",
      "[stream] processed 26800 / 224309\n",
      "[stream] processed 27200 / 224309\n",
      "[stream] processed 27600 / 224309\n",
      "[stream] processed 28000 / 224309\n",
      "[stream] processed 28400 / 224309\n",
      "[stream] processed 28800 / 224309\n",
      "[stream] processed 29200 / 224309\n",
      "[stream] processed 29600 / 224309\n",
      "[stream] processed 30000 / 224309\n",
      "[stream] processed 30400 / 224309\n",
      "[stream] processed 30800 / 224309\n",
      "[stream] processed 31200 / 224309\n",
      "[stream] processed 31600 / 224309\n",
      "[stream] processed 32000 / 224309\n",
      "[stream] processed 32400 / 224309\n",
      "[stream] processed 32800 / 224309\n",
      "[stream] processed 33200 / 224309\n",
      "[stream] processed 33600 / 224309\n",
      "[stream] processed 34000 / 224309\n",
      "[stream] processed 34400 / 224309\n",
      "[stream] processed 34800 / 224309\n",
      "[stream] processed 35200 / 224309\n",
      "[stream] processed 35600 / 224309\n",
      "[stream] processed 36000 / 224309\n",
      "[stream] processed 36400 / 224309\n",
      "[stream] processed 36800 / 224309\n",
      "[stream] processed 37200 / 224309\n",
      "[stream] processed 37600 / 224309\n",
      "[stream] processed 38000 / 224309\n",
      "[stream] processed 38400 / 224309\n",
      "[stream] processed 38800 / 224309\n",
      "[stream] processed 39200 / 224309\n",
      "[stream] processed 39600 / 224309\n",
      "[stream] processed 40000 / 224309\n",
      "[stream] processed 40400 / 224309\n",
      "[stream] processed 40800 / 224309\n",
      "[stream] processed 41200 / 224309\n",
      "[stream] processed 41600 / 224309\n",
      "[stream] processed 42000 / 224309\n",
      "[stream] processed 42400 / 224309\n",
      "[stream] processed 42800 / 224309\n",
      "[stream] processed 43200 / 224309\n",
      "[stream] processed 43600 / 224309\n",
      "[stream] processed 44000 / 224309\n",
      "[stream] processed 44400 / 224309\n",
      "[stream] processed 44800 / 224309\n",
      "[stream] processed 45200 / 224309\n",
      "[stream] processed 45600 / 224309\n",
      "[stream] processed 46000 / 224309\n",
      "[stream] processed 46400 / 224309\n",
      "[stream] processed 46800 / 224309\n",
      "[stream] processed 47200 / 224309\n",
      "[stream] processed 47600 / 224309\n",
      "[stream] processed 48000 / 224309\n",
      "[stream] processed 48400 / 224309\n",
      "[stream] processed 48800 / 224309\n",
      "[stream] processed 49200 / 224309\n",
      "[stream] processed 49600 / 224309\n",
      "[stream] processed 50000 / 224309\n",
      "[stream] processed 50400 / 224309\n",
      "[stream] processed 50800 / 224309\n",
      "[stream] processed 51200 / 224309\n",
      "[stream] processed 51600 / 224309\n",
      "[stream] processed 52000 / 224309\n",
      "[stream] processed 52400 / 224309\n",
      "[stream] processed 52800 / 224309\n",
      "[stream] processed 53200 / 224309\n",
      "[stream] processed 53600 / 224309\n",
      "[stream] processed 54000 / 224309\n",
      "[stream] processed 54400 / 224309\n",
      "[stream] processed 54800 / 224309\n",
      "[stream] processed 55200 / 224309\n",
      "[stream] processed 55600 / 224309\n",
      "[stream] processed 56000 / 224309\n",
      "[stream] processed 56400 / 224309\n",
      "[stream] processed 56800 / 224309\n",
      "[stream] processed 57200 / 224309\n",
      "[stream] processed 57600 / 224309\n",
      "[stream] processed 58000 / 224309\n",
      "[stream] processed 58400 / 224309\n",
      "[stream] processed 58800 / 224309\n",
      "[stream] processed 59200 / 224309\n",
      "[stream] processed 59600 / 224309\n",
      "[stream] processed 60000 / 224309\n",
      "[stream] processed 60400 / 224309\n",
      "[stream] processed 60800 / 224309\n",
      "[stream] processed 61200 / 224309\n",
      "[stream] processed 61600 / 224309\n",
      "[stream] processed 62000 / 224309\n",
      "[stream] processed 62400 / 224309\n",
      "[stream] processed 62800 / 224309\n",
      "[stream] processed 63200 / 224309\n",
      "[stream] processed 63600 / 224309\n",
      "[stream] processed 64000 / 224309\n",
      "[stream] processed 64400 / 224309\n",
      "[stream] processed 64800 / 224309\n",
      "[stream] processed 65200 / 224309\n",
      "[stream] processed 65600 / 224309\n",
      "[stream] processed 66000 / 224309\n",
      "[stream] processed 66400 / 224309\n",
      "[stream] processed 66800 / 224309\n",
      "[stream] processed 67200 / 224309\n",
      "[stream] processed 67600 / 224309\n",
      "[stream] processed 68000 / 224309\n",
      "[stream] processed 68400 / 224309\n",
      "[stream] processed 68800 / 224309\n",
      "[stream] processed 69200 / 224309\n",
      "[stream] processed 69600 / 224309\n",
      "[stream] processed 70000 / 224309\n",
      "[stream] processed 70400 / 224309\n",
      "[stream] processed 70800 / 224309\n",
      "[stream] processed 71200 / 224309\n",
      "[stream] processed 71600 / 224309\n",
      "[stream] processed 72000 / 224309\n",
      "[stream] processed 72400 / 224309\n",
      "[stream] processed 72800 / 224309\n",
      "[stream] processed 73200 / 224309\n",
      "[stream] processed 73600 / 224309\n",
      "[stream] processed 74000 / 224309\n",
      "[stream] processed 74400 / 224309\n",
      "[stream] processed 74800 / 224309\n",
      "[stream] processed 75200 / 224309\n",
      "[stream] processed 75600 / 224309\n",
      "[stream] processed 76000 / 224309\n",
      "[stream] processed 76400 / 224309\n",
      "[stream] processed 76800 / 224309\n",
      "[stream] processed 77200 / 224309\n",
      "[stream] processed 77600 / 224309\n",
      "[stream] processed 78000 / 224309\n",
      "[stream] processed 78400 / 224309\n",
      "[stream] processed 78800 / 224309\n",
      "[stream] processed 79200 / 224309\n",
      "[stream] processed 79600 / 224309\n",
      "[stream] processed 80000 / 224309\n",
      "[stream] processed 80400 / 224309\n",
      "[stream] processed 80800 / 224309\n",
      "[stream] processed 81200 / 224309\n",
      "[stream] processed 81600 / 224309\n",
      "[stream] processed 82000 / 224309\n",
      "[stream] processed 82400 / 224309\n",
      "[stream] processed 82800 / 224309\n",
      "[stream] processed 83200 / 224309\n",
      "[stream] processed 83600 / 224309\n",
      "[stream] processed 84000 / 224309\n",
      "[stream] processed 84400 / 224309\n",
      "[stream] processed 84800 / 224309\n",
      "[stream] processed 85200 / 224309\n",
      "[stream] processed 85600 / 224309\n",
      "[stream] processed 86000 / 224309\n",
      "[stream] processed 86400 / 224309\n",
      "[stream] processed 86800 / 224309\n",
      "[stream] processed 87200 / 224309\n",
      "[stream] processed 87600 / 224309\n",
      "[stream] processed 88000 / 224309\n",
      "[stream] processed 88400 / 224309\n",
      "[stream] processed 88800 / 224309\n",
      "[stream] processed 89200 / 224309\n",
      "[stream] processed 89600 / 224309\n",
      "[stream] processed 90000 / 224309\n",
      "[stream] processed 90400 / 224309\n",
      "[stream] processed 90800 / 224309\n",
      "[stream] processed 91200 / 224309\n",
      "[stream] processed 91600 / 224309\n",
      "[stream] processed 92000 / 224309\n",
      "[stream] processed 92400 / 224309\n",
      "[stream] processed 92800 / 224309\n",
      "[stream] processed 93200 / 224309\n",
      "[stream] processed 93600 / 224309\n",
      "[stream] processed 94000 / 224309\n",
      "[stream] processed 94400 / 224309\n",
      "[stream] processed 94800 / 224309\n",
      "[stream] processed 95200 / 224309\n",
      "[stream] processed 95600 / 224309\n",
      "[stream] processed 96000 / 224309\n",
      "[stream] processed 96400 / 224309\n",
      "[stream] processed 96800 / 224309\n",
      "[stream] processed 97200 / 224309\n",
      "[stream] processed 97600 / 224309\n",
      "[stream] processed 98000 / 224309\n",
      "[stream] processed 98400 / 224309\n",
      "[stream] processed 98800 / 224309\n",
      "[stream] processed 99200 / 224309\n",
      "[stream] processed 99600 / 224309\n",
      "[stream] processed 100000 / 224309\n",
      "[stream] processed 100400 / 224309\n",
      "[stream] processed 100800 / 224309\n",
      "[stream] processed 101200 / 224309\n",
      "[stream] processed 101600 / 224309\n",
      "[stream] processed 102000 / 224309\n",
      "[stream] processed 102400 / 224309\n",
      "[stream] processed 102800 / 224309\n",
      "[stream] processed 103200 / 224309\n",
      "[stream] processed 103600 / 224309\n",
      "[stream] processed 104000 / 224309\n",
      "[stream] processed 104400 / 224309\n",
      "[stream] processed 104800 / 224309\n",
      "[stream] processed 105200 / 224309\n",
      "[stream] processed 105600 / 224309\n",
      "[stream] processed 106000 / 224309\n",
      "[stream] processed 106400 / 224309\n",
      "[stream] processed 106800 / 224309\n",
      "[stream] processed 107200 / 224309\n",
      "[stream] processed 107600 / 224309\n",
      "[stream] processed 108000 / 224309\n",
      "[stream] processed 108400 / 224309\n",
      "[stream] processed 108800 / 224309\n",
      "[stream] processed 109200 / 224309\n",
      "[stream] processed 109600 / 224309\n",
      "[stream] processed 110000 / 224309\n",
      "[stream] processed 110400 / 224309\n",
      "[stream] processed 110800 / 224309\n",
      "[stream] processed 111200 / 224309\n",
      "[stream] processed 111600 / 224309\n",
      "[stream] processed 112000 / 224309\n",
      "[stream] processed 112400 / 224309\n",
      "[stream] processed 112800 / 224309\n",
      "[stream] processed 113200 / 224309\n",
      "[stream] processed 113600 / 224309\n",
      "[stream] processed 114000 / 224309\n",
      "[stream] processed 114400 / 224309\n",
      "[stream] processed 114800 / 224309\n",
      "[stream] processed 115200 / 224309\n",
      "[stream] processed 115600 / 224309\n",
      "[stream] processed 116000 / 224309\n",
      "[stream] processed 116400 / 224309\n",
      "[stream] processed 116800 / 224309\n",
      "[stream] processed 117200 / 224309\n",
      "[stream] processed 117600 / 224309\n",
      "[stream] processed 118000 / 224309\n",
      "[stream] processed 118400 / 224309\n",
      "[stream] processed 118800 / 224309\n",
      "[stream] processed 119200 / 224309\n",
      "[stream] processed 119600 / 224309\n",
      "[stream] processed 120000 / 224309\n",
      "[stream] processed 120400 / 224309\n",
      "[stream] processed 120800 / 224309\n",
      "[stream] processed 121200 / 224309\n",
      "[stream] processed 121600 / 224309\n",
      "[stream] processed 122000 / 224309\n",
      "[stream] processed 122400 / 224309\n",
      "[stream] processed 122800 / 224309\n",
      "[stream] processed 123200 / 224309\n",
      "[stream] processed 123600 / 224309\n",
      "[stream] processed 124000 / 224309\n",
      "[stream] processed 124400 / 224309\n",
      "[stream] processed 124800 / 224309\n",
      "[stream] processed 125200 / 224309\n",
      "[stream] processed 125600 / 224309\n",
      "[stream] processed 126000 / 224309\n",
      "[stream] processed 126400 / 224309\n",
      "[stream] processed 126800 / 224309\n",
      "[stream] processed 127200 / 224309\n",
      "[stream] processed 127600 / 224309\n",
      "[stream] processed 128000 / 224309\n",
      "[stream] processed 128400 / 224309\n",
      "[stream] processed 128800 / 224309\n",
      "[stream] processed 129200 / 224309\n",
      "[stream] processed 129600 / 224309\n",
      "[stream] processed 130000 / 224309\n",
      "[stream] processed 130400 / 224309\n",
      "[stream] processed 130800 / 224309\n",
      "[stream] processed 131200 / 224309\n",
      "[stream] processed 131600 / 224309\n",
      "[stream] processed 132000 / 224309\n",
      "[stream] processed 132400 / 224309\n",
      "[stream] processed 132800 / 224309\n",
      "[stream] processed 133200 / 224309\n",
      "[stream] processed 133600 / 224309\n",
      "[stream] processed 134000 / 224309\n",
      "[stream] processed 134400 / 224309\n",
      "[stream] processed 134800 / 224309\n",
      "[stream] processed 135200 / 224309\n",
      "[stream] processed 135600 / 224309\n",
      "[stream] processed 136000 / 224309\n",
      "[stream] processed 136400 / 224309\n",
      "[stream] processed 136800 / 224309\n",
      "[stream] processed 137200 / 224309\n",
      "[stream] processed 137600 / 224309\n",
      "[stream] processed 138000 / 224309\n",
      "[stream] processed 138400 / 224309\n",
      "[stream] processed 138800 / 224309\n",
      "[stream] processed 139200 / 224309\n",
      "[stream] processed 139600 / 224309\n",
      "[stream] processed 140000 / 224309\n",
      "[stream] processed 140400 / 224309\n",
      "[stream] processed 140800 / 224309\n",
      "[stream] processed 141200 / 224309\n",
      "[stream] processed 141600 / 224309\n",
      "[stream] processed 142000 / 224309\n",
      "[stream] processed 142400 / 224309\n",
      "[stream] processed 142800 / 224309\n",
      "[stream] processed 143200 / 224309\n",
      "[stream] processed 143600 / 224309\n",
      "[stream] processed 144000 / 224309\n",
      "[stream] processed 144400 / 224309\n",
      "[stream] processed 144800 / 224309\n",
      "[stream] processed 145200 / 224309\n",
      "[stream] processed 145600 / 224309\n",
      "[stream] processed 146000 / 224309\n",
      "[stream] processed 146400 / 224309\n",
      "[stream] processed 146800 / 224309\n",
      "[stream] processed 147200 / 224309\n",
      "[stream] processed 147600 / 224309\n",
      "[stream] processed 148000 / 224309\n",
      "[stream] processed 148400 / 224309\n",
      "[stream] processed 148800 / 224309\n",
      "[stream] processed 149200 / 224309\n",
      "[stream] processed 149600 / 224309\n",
      "[stream] processed 150000 / 224309\n",
      "[stream] processed 150400 / 224309\n",
      "[stream] processed 150800 / 224309\n",
      "[stream] processed 151200 / 224309\n",
      "[stream] processed 151600 / 224309\n",
      "[stream] processed 152000 / 224309\n",
      "[stream] processed 152400 / 224309\n",
      "[stream] processed 152800 / 224309\n",
      "[stream] processed 153200 / 224309\n",
      "[stream] processed 153600 / 224309\n",
      "[stream] processed 154000 / 224309\n",
      "[stream] processed 154400 / 224309\n",
      "[stream] processed 154800 / 224309\n",
      "[stream] processed 155200 / 224309\n",
      "[stream] processed 155600 / 224309\n",
      "[stream] processed 156000 / 224309\n",
      "[stream] processed 156400 / 224309\n",
      "[stream] processed 156800 / 224309\n",
      "[stream] processed 157200 / 224309\n",
      "[stream] processed 157600 / 224309\n",
      "[stream] processed 158000 / 224309\n",
      "[stream] processed 158400 / 224309\n",
      "[stream] processed 158800 / 224309\n",
      "[stream] processed 159200 / 224309\n",
      "[stream] processed 159600 / 224309\n",
      "[stream] processed 160000 / 224309\n",
      "[stream] processed 160400 / 224309\n",
      "[stream] processed 160800 / 224309\n",
      "[stream] processed 161200 / 224309\n",
      "[stream] processed 161600 / 224309\n",
      "[stream] processed 162000 / 224309\n",
      "[stream] processed 162400 / 224309\n",
      "[stream] processed 162800 / 224309\n",
      "[stream] processed 163200 / 224309\n",
      "[stream] processed 163600 / 224309\n",
      "[stream] processed 164000 / 224309\n",
      "[stream] processed 164400 / 224309\n",
      "[stream] processed 164800 / 224309\n",
      "[stream] processed 165200 / 224309\n",
      "[stream] processed 165600 / 224309\n",
      "[stream] processed 166000 / 224309\n",
      "[stream] processed 166400 / 224309\n",
      "[stream] processed 166800 / 224309\n",
      "[stream] processed 167200 / 224309\n",
      "[stream] processed 167600 / 224309\n",
      "[stream] processed 168000 / 224309\n",
      "[stream] processed 168400 / 224309\n",
      "[stream] processed 168800 / 224309\n",
      "[stream] processed 169200 / 224309\n",
      "[stream] processed 169600 / 224309\n",
      "[stream] processed 170000 / 224309\n",
      "[stream] processed 170400 / 224309\n",
      "[stream] processed 170800 / 224309\n",
      "[stream] processed 171200 / 224309\n",
      "[stream] processed 171600 / 224309\n",
      "[stream] processed 172000 / 224309\n",
      "[stream] processed 172400 / 224309\n",
      "[stream] processed 172800 / 224309\n",
      "[stream] processed 173200 / 224309\n",
      "[stream] processed 173600 / 224309\n",
      "[stream] processed 174000 / 224309\n",
      "[stream] processed 174400 / 224309\n",
      "[stream] processed 174800 / 224309\n",
      "[stream] processed 175200 / 224309\n",
      "[stream] processed 175600 / 224309\n",
      "[stream] processed 176000 / 224309\n",
      "[stream] processed 176400 / 224309\n",
      "[stream] processed 176800 / 224309\n",
      "[stream] processed 177200 / 224309\n",
      "[stream] processed 177600 / 224309\n",
      "[stream] processed 178000 / 224309\n",
      "[stream] processed 178400 / 224309\n",
      "[stream] processed 178800 / 224309\n",
      "[stream] processed 179200 / 224309\n",
      "[stream] processed 179600 / 224309\n",
      "[stream] processed 180000 / 224309\n",
      "[stream] processed 180400 / 224309\n",
      "[stream] processed 180800 / 224309\n",
      "[stream] processed 181200 / 224309\n",
      "[stream] processed 181600 / 224309\n",
      "[stream] processed 182000 / 224309\n",
      "[stream] processed 182400 / 224309\n",
      "[stream] processed 182800 / 224309\n",
      "[stream] processed 183200 / 224309\n",
      "[stream] processed 183600 / 224309\n",
      "[stream] processed 184000 / 224309\n",
      "[stream] processed 184400 / 224309\n",
      "[stream] processed 184800 / 224309\n",
      "[stream] processed 185200 / 224309\n",
      "[stream] processed 185600 / 224309\n",
      "[stream] processed 186000 / 224309\n",
      "[stream] processed 186400 / 224309\n",
      "[stream] processed 186800 / 224309\n",
      "[stream] processed 187200 / 224309\n",
      "[stream] processed 187600 / 224309\n",
      "[stream] processed 188000 / 224309\n",
      "[stream] processed 188400 / 224309\n",
      "[stream] processed 188800 / 224309\n",
      "[stream] processed 189200 / 224309\n",
      "[stream] processed 189600 / 224309\n",
      "[stream] processed 190000 / 224309\n",
      "[stream] processed 190400 / 224309\n",
      "[stream] processed 190800 / 224309\n",
      "[stream] processed 191200 / 224309\n",
      "[stream] processed 191600 / 224309\n",
      "[stream] processed 192000 / 224309\n",
      "[stream] processed 192400 / 224309\n",
      "[stream] processed 192800 / 224309\n",
      "[stream] processed 193200 / 224309\n",
      "[stream] processed 193600 / 224309\n",
      "[stream] processed 194000 / 224309\n",
      "[stream] processed 194400 / 224309\n",
      "[stream] processed 194800 / 224309\n",
      "[stream] processed 195200 / 224309\n",
      "[stream] processed 195600 / 224309\n",
      "[stream] processed 196000 / 224309\n",
      "[stream] processed 196400 / 224309\n",
      "[stream] processed 196800 / 224309\n",
      "[stream] processed 197200 / 224309\n",
      "[stream] processed 197600 / 224309\n",
      "[stream] processed 198000 / 224309\n",
      "[stream] processed 198400 / 224309\n",
      "[stream] processed 198800 / 224309\n",
      "[stream] processed 199200 / 224309\n",
      "[stream] processed 199600 / 224309\n",
      "[stream] processed 200000 / 224309\n",
      "[stream] processed 200400 / 224309\n",
      "[stream] processed 200800 / 224309\n",
      "[stream] processed 201200 / 224309\n",
      "[stream] processed 201600 / 224309\n",
      "[stream] processed 202000 / 224309\n",
      "[stream] processed 202400 / 224309\n",
      "[stream] processed 202800 / 224309\n",
      "[stream] processed 203200 / 224309\n",
      "[stream] processed 203600 / 224309\n",
      "[stream] processed 204000 / 224309\n",
      "[stream] processed 204400 / 224309\n",
      "[stream] processed 204800 / 224309\n",
      "[stream] processed 205200 / 224309\n",
      "[stream] processed 205600 / 224309\n",
      "[stream] processed 206000 / 224309\n",
      "[stream] processed 206400 / 224309\n",
      "[stream] processed 206800 / 224309\n",
      "[stream] processed 207200 / 224309\n",
      "[stream] processed 207600 / 224309\n",
      "[stream] processed 208000 / 224309\n",
      "[stream] processed 208400 / 224309\n",
      "[stream] processed 208800 / 224309\n",
      "[stream] processed 209200 / 224309\n",
      "[stream] processed 209600 / 224309\n",
      "[stream] processed 210000 / 224309\n",
      "[stream] processed 210400 / 224309\n",
      "[stream] processed 210800 / 224309\n",
      "[stream] processed 211200 / 224309\n",
      "[stream] processed 211600 / 224309\n",
      "[stream] processed 212000 / 224309\n",
      "[stream] processed 212400 / 224309\n",
      "[stream] processed 212800 / 224309\n",
      "[stream] processed 213200 / 224309\n",
      "[stream] processed 213600 / 224309\n",
      "[stream] processed 214000 / 224309\n",
      "[stream] processed 214400 / 224309\n",
      "[stream] processed 214800 / 224309\n",
      "[stream] processed 215200 / 224309\n",
      "[stream] processed 215600 / 224309\n",
      "[stream] processed 216000 / 224309\n",
      "[stream] processed 216400 / 224309\n",
      "[stream] processed 216800 / 224309\n",
      "[stream] processed 217200 / 224309\n",
      "[stream] processed 217600 / 224309\n",
      "[stream] processed 218000 / 224309\n",
      "[stream] processed 218400 / 224309\n",
      "[stream] processed 218800 / 224309\n",
      "[stream] processed 219200 / 224309\n",
      "[stream] processed 219600 / 224309\n",
      "[stream] processed 220000 / 224309\n",
      "[stream] processed 220400 / 224309\n",
      "[stream] processed 220800 / 224309\n",
      "[stream] processed 221200 / 224309\n",
      "[stream] processed 221600 / 224309\n",
      "[stream] processed 222000 / 224309\n",
      "[stream] processed 222400 / 224309\n",
      "[stream] processed 222800 / 224309\n",
      "[stream] processed 223200 / 224309\n",
      "[stream] processed 223600 / 224309\n",
      "[stream] processed 224000 / 224309\n",
      "[done] Submission written to /kaggle/working/submission1.tsv\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Streaming test-time: embed test sequences batchwise, predict, propagate, write submission lines immediately\n",
    "# ------------------------------------------------------------\n",
    "# Precompute helpful mappings for propagation\n",
    "term_to_idx = {t:i for i,t in enumerate(mlb.classes_)}\n",
    "idx_to_term = {i:t for t,i in term_to_idx.items()}\n",
    "\n",
    "# Build parents_map restricted to chosen_terms to speed propagation\n",
    "restricted_parents = {}\n",
    "for t in mlb.classes_:\n",
    "    restricted_parents[t] = set([p for p in parents_map.get(t, set()) if p in term_to_idx])\n",
    "\n",
    "# small propagation routine operating on a batch_of_probs (N_batch, M)\n",
    "def propagate_batch(pred_batch: np.ndarray, parents_map_local: Dict[str, Set[str]], classes_list: List[str], iterations=3):\n",
    "    # pred_batch: float32 shape (B, M)\n",
    "    B, Mloc = pred_batch.shape\n",
    "    idx_map = {i:classes_list[i] for i in range(Mloc)}\n",
    "    term_to_idx_local = {classes_list[i]: i for i in range(Mloc)}\n",
    "    for _ in range(iterations):\n",
    "        changed = False\n",
    "        # vectorized-ish: for each child index, update parent index with max\n",
    "        # loop over terms (M might be a few thousands => ok per small batch)\n",
    "        for child_idx in range(Mloc):\n",
    "            child_term = idx_map[child_idx]\n",
    "            child_scores = pred_batch[:, child_idx]\n",
    "            for pterm in parents_map_local.get(child_term, []):\n",
    "                pidx = term_to_idx_local[pterm]\n",
    "                # update parent where child's score exceeds parent\n",
    "                mask = child_scores > pred_batch[:, pidx]\n",
    "                if mask.any():\n",
    "                    pred_batch[mask, pidx] = child_scores[mask]\n",
    "                    changed = True\n",
    "        if not changed: break\n",
    "    return pred_batch\n",
    "\n",
    "# Open submission file for streaming write\n",
    "out_fpath = CONFIG[\"OUTPUT_SUBMISSION\"]\n",
    "open(out_fpath, \"w\").close()  # truncate\n",
    "out_f = open(out_fpath, \"a\")\n",
    "\n",
    "# Create chunked iterator of test sequence IDs\n",
    "test_ids = list(test_seqs.keys())\n",
    "N_test = len(test_ids)\n",
    "print(f\"[test] Streaming {N_test} test sequences in batches of {CONFIG['EMBED_BATCH_SIZE']} (embed) / predict {CONFIG['PREDICT_BATCH_SIZE']}\")\n",
    "\n",
    "# If using PLM, prepare tokenizer & model once (on CPU/GPU)\n",
    "if USE_PLM:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"PLM_MODEL_NAME_OR_PATH\"], do_lower_case=False)\n",
    "    plm_model = AutoModel.from_pretrained(CONFIG[\"PLM_MODEL_NAME_OR_PATH\"])\n",
    "    plm_model.eval()\n",
    "    if torch.cuda.is_available(): plm_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Helper to embed a list of sequences and return numpy array float32 of shape (len(seq_list), D)\n",
    "def embed_batch_return_np(seq_list: List[str]):\n",
    "    if USE_PLM:\n",
    "        proc = seqs_for_plm_input_esm(seq_list)\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(proc, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "            out = plm_model(**inputs)\n",
    "            last_hidden = out.last_hidden_state  # (B, L, dim)\n",
    "            mask = inputs.get(\"attention_mask\", None)\n",
    "            if mask is not None:\n",
    "                mask = mask.unsqueeze(-1)\n",
    "                summed = (last_hidden * mask).sum(1)\n",
    "                counts = mask.sum(1).clamp(min=1)\n",
    "                mean_pooled = (summed / counts).cpu().numpy().astype(np.float32)\n",
    "            else:\n",
    "                mean_pooled = last_hidden.mean(dim=1).cpu().numpy().astype(np.float32)\n",
    "        # free GPU memory\n",
    "        del inputs, out, last_hidden\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return mean_pooled\n",
    "    else:\n",
    "        # TF-IDF fallback: transform using vectorizer in small chunk\n",
    "        texts = [\" \".join([seq[i:i+3] for i in range(len(seq)-3+1)]) for seq in seq_list]\n",
    "        arr = tfidf.transform(texts).astype(np.float32).toarray()\n",
    "        return arr\n",
    "\n",
    "# We'll process test samples in small \"embed\" batches and then accumulate a list of embedded rows\n",
    "# until we have PREDICT_BATCH_SIZE embeddings ready for model.predict(...) then predict and write submission lines.\n",
    "embed_batch = []\n",
    "embed_ids = []\n",
    "\n",
    "for i in range(0, N_test, CONFIG[\"EMBED_BATCH_SIZE\"]):\n",
    "    batch_ids = test_ids[i:i+CONFIG[\"EMBED_BATCH_SIZE\"]]\n",
    "    seqs_batch = [test_seqs[pid] for pid in batch_ids]\n",
    "    # compute embeddings for this mini-batch\n",
    "    emb_mini = embed_batch_return_np(seqs_batch)  # shape (Bmini, D)\n",
    "    # append to buffer\n",
    "    embed_batch.append(emb_mini)\n",
    "    embed_ids.extend(batch_ids)\n",
    "    # if enough buffered to predict, or we're at the end, flush to prediction\n",
    "    buffered_examples = sum(arr.shape[0] for arr in embed_batch)\n",
    "    if buffered_examples >= CONFIG[\"PREDICT_BATCH_SIZE\"] or (i+CONFIG[\"EMBED_BATCH_SIZE\"] >= N_test):\n",
    "        # stack buffered embeddings (should be moderate size)\n",
    "        X_buffer = np.vstack(embed_batch).astype(np.float32)  # shape (Bbuf, D)\n",
    "        # predict in one shot for this buffer\n",
    "        y_buffer_prob = model.predict(X_buffer, batch_size=min(128, X_buffer.shape[0]), verbose=0)\n",
    "        # propagate per-batch (if desired)\n",
    "        if CONFIG[\"PROPAGATE_PREDICTIONS\"] and parents_map:\n",
    "            y_buffer_prob = propagate_batch(y_buffer_prob, restricted_parents, list(mlb.classes_), iterations=3)\n",
    "        # for each row, write top-K lines\n",
    "        for ridx, pid in enumerate(embed_ids):\n",
    "            probs = y_buffer_prob[ridx]\n",
    "            # pick top-K_PER_PROTEIN indices (and filter near-zero)\n",
    "            top_k = CONFIG[\"TOP_K_PER_PROTEIN\"]\n",
    "            if top_k is None:\n",
    "                idxs = np.where(probs >= best_thresh)[0]\n",
    "            else:\n",
    "                idxs = np.argsort(probs)[-top_k:]\n",
    "                idxs = [int(x) for x in idxs if probs[x] > 1e-6]\n",
    "            idxs = sorted(idxs, key=lambda x: probs[x], reverse=True)\n",
    "            for idx in idxs:\n",
    "                score = float(probs[idx])\n",
    "                if score <= 0.0: continue\n",
    "                go_id = mlb.classes_[idx]\n",
    "                out_f.write(f\"{pid}\\t{go_id}\\t{score:.3f}\\n\")\n",
    "        out_f.flush()\n",
    "        # free buffer\n",
    "        del X_buffer, y_buffer_prob, embed_batch\n",
    "        embed_batch = []\n",
    "        embed_ids = []\n",
    "        gc.collect()\n",
    "        if TORCH_AVAILABLE and torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    # small progress print\n",
    "    if (i // CONFIG[\"EMBED_BATCH_SIZE\"]) % 50 == 0:\n",
    "        print(f\"[stream] processed {i} / {N_test}\")\n",
    "\n",
    "out_f.close()\n",
    "print(f\"[done] Submission written to {CONFIG['OUTPUT_SUBMISSION']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bba55a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:34:36.218836Z",
     "iopub.status.busy": "2025-10-20T10:34:36.218609Z",
     "iopub.status.idle": "2025-10-20T10:34:36.222153Z",
     "shell.execute_reply": "2025-10-20T10:34:36.221545Z"
    },
    "papermill": {
     "duration": 0.031176,
     "end_time": "2025-10-20T10:34:36.223191",
     "exception": false,
     "start_time": "2025-10-20T10:34:36.192015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------------\n",
    "# # Save model / artifacts if desired (lightweight)\n",
    "# # ------------------------------------------------------------\n",
    "# model.save(\"/kaggle/working/cafa6_baseline_model\")\n",
    "# np.save(\"/kaggle/working/mlb_classes.npy\", np.array(mlb.classes_, dtype=object))\n",
    "# print(\"[done] saved model and classes; notebook finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bb3bc88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:34:36.275638Z",
     "iopub.status.busy": "2025-10-20T10:34:36.275418Z",
     "iopub.status.idle": "2025-10-20T10:34:49.631957Z",
     "shell.execute_reply": "2025-10-20T10:34:49.631357Z"
    },
    "papermill": {
     "duration": 13.384805,
     "end_time": "2025-10-20T10:34:49.633405",
     "exception": false,
     "start_time": "2025-10-20T10:34:36.248600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission1 = pd.read_csv(\"/kaggle/working/submission1.tsv\",\n",
    "                          sep='\\t', header=None, names=['Id', 'GO term', 'Confidence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c2d9a",
   "metadata": {
    "papermill": {
     "duration": 0.025191,
     "end_time": "2025-10-20T10:34:49.684685",
     "exception": false,
     "start_time": "2025-10-20T10:34:49.659494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b3ebfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:34:49.736701Z",
     "iopub.status.busy": "2025-10-20T10:34:49.736460Z",
     "iopub.status.idle": "2025-10-20T10:34:55.969648Z",
     "shell.execute_reply": "2025-10-20T10:34:55.968979Z"
    },
    "papermill": {
     "duration": 6.261082,
     "end_time": "2025-10-20T10:34:55.971012",
     "exception": false,
     "start_time": "2025-10-20T10:34:49.709930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission2 = pd.read_csv('/kaggle/input/cafa5-055923-pred/submission.tsv',\n",
    "                          sep='\\t', header=None, names=['Id', 'GO term', 'Confidence']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b629d",
   "metadata": {
    "papermill": {
     "duration": 0.025595,
     "end_time": "2025-10-20T10:34:56.022685",
     "exception": false,
     "start_time": "2025-10-20T10:34:55.997090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78d6f5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:34:56.075105Z",
     "iopub.status.busy": "2025-10-20T10:34:56.074858Z",
     "iopub.status.idle": "2025-10-20T10:37:02.288247Z",
     "shell.execute_reply": "2025-10-20T10:37:02.287556Z"
    },
    "papermill": {
     "duration": 126.241378,
     "end_time": "2025-10-20T10:37:02.289659",
     "exception": false,
     "start_time": "2025-10-20T10:34:56.048281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission1['Confidence'] = pd.to_numeric(submission1['Confidence'], errors='coerce')\n",
    "submission2['Confidence'] = pd.to_numeric(submission2['Confidence'], errors='coerce')\n",
    "\n",
    "merged = submission1.merge(submission2,\n",
    "                           on=['Id', 'GO term'],\n",
    "                           how='outer',\n",
    "                           suffixes=('1', '2'))\n",
    "\n",
    "merged['Confidence'] = merged['Confidence2'].combine_first(merged['Confidence1'])\n",
    "final_submission = merged[['Id', 'GO term', 'Confidence']]\n",
    "final_submission.to_csv('submission.tsv', sep='\\t', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 3546496,
     "sourceId": 6180004,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 425816,
     "modelInstanceId": 407954,
     "sourceId": 517313,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2553.291976,
   "end_time": "2025-10-20T10:37:05.134027",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-20T09:54:31.842051",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
