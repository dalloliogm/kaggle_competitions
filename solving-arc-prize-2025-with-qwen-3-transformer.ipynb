{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f29ac54",
   "metadata": {
    "papermill": {
     "duration": 0.003643,
     "end_time": "2025-05-07T19:02:15.255741",
     "exception": false,
     "start_time": "2025-05-07T19:02:15.252098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: flex-start;\">\n",
    "    <div style=\"text-align: left;\">\n",
    "        <p style=\"color:#FFD700; font-size: 15px; font-weight: bold; margin-bottom: 1px; text-align: left;\">Published on  May 1, 2025</p>\n",
    "        <h4 style=\"color:#4B0082; font-weight: bold; text-align: left; margin-top: 6px;\">Author: Jocelyn C. Dumlao</h4>\n",
    "        <p style=\"font-size: 17px; line-height: 1.7; color: #333; text-align: center; margin-top: 20px;\"></p>\n",
    "        <a href=\"https://www.linkedin.com/in/jocelyn-dumlao-168921a8/\" target=\"_blank\" style=\"display: inline-block; background-color: #003f88; color: #fff; text-decoration: none; padding: 5px 10px; border-radius: 10px; margin: 15px;\">LinkedIn</a>\n",
    "        <a href=\"https://github.com/jcdumlao14\" target=\"_blank\" style=\"display: inline-block; background-color: transparent; color: #059c99; text-decoration: none; padding: 5px 10px; border-radius: 10px; margin: 15px; border: 2px solid #007bff;\">GitHub</a>\n",
    "        <a href=\"https://www.youtube.com/@CogniCraftedMinds\" target=\"_blank\" style=\"display: inline-block; background-color: #ff0054; color: #fff; text-decoration: none; padding: 5px 10px; border-radius: 10px; margin: 15px;\">YouTube</a>\n",
    "        <a href=\"https://www.kaggle.com/jocelyndumlao\" target=\"_blank\" style=\"display: inline-block; background-color: #3a86ff; color: #fff; text-decoration: none; padding: 5px 10px; border-radius: 10px; margin: 15px;\">Kaggle</a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab396f",
   "metadata": {
    "papermill": {
     "duration": 0.002737,
     "end_time": "2025-05-07T19:02:15.261505",
     "exception": false,
     "start_time": "2025-05-07T19:02:15.258768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Import Libraries</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43521439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:02:15.268747Z",
     "iopub.status.busy": "2025-05-07T19:02:15.268032Z",
     "iopub.status.idle": "2025-05-07T19:02:23.196275Z",
     "shell.execute_reply": "2025-05-07T19:02:23.195467Z"
    },
    "papermill": {
     "duration": 7.933433,
     "end_time": "2025-05-07T19:02:23.197787",
     "exception": false,
     "start_time": "2025-05-07T19:02:15.264354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8072ead",
   "metadata": {
    "papermill": {
     "duration": 0.00272,
     "end_time": "2025-05-07T19:02:23.203738",
     "exception": false,
     "start_time": "2025-05-07T19:02:23.201018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reference: [Qwen 3 qwen-lm/qwen-3](https://www.kaggle.com/models/qwen-lm/qwen-3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5e1f5",
   "metadata": {
    "papermill": {
     "duration": 0.002573,
     "end_time": "2025-05-07T19:02:23.209057",
     "exception": false,
     "start_time": "2025-05-07T19:02:23.206484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Define paths to the datasets</p>\n",
    "\n",
    "- The code first locates the JSON files containing the ARC (Abstraction and Reasoning Corpus) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39db34fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:02:23.215703Z",
     "iopub.status.busy": "2025-05-07T19:02:23.215374Z",
     "iopub.status.idle": "2025-05-07T19:02:23.228206Z",
     "shell.execute_reply": "2025-05-07T19:02:23.227691Z"
    },
    "papermill": {
     "duration": 0.017243,
     "end_time": "2025-05-07T19:02:23.229225",
     "exception": false,
     "start_time": "2025-05-07T19:02:23.211982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paths to the datasets\n",
    "training_solutions_path = '/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json' if os.path.exists('/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json') else 'arc-agi_training_solutions.json'  \n",
    "evaluation_solutions_path = '/kaggle/input/arc-prize-2025/arc-agi_evaluation_solutions.json' if os.path.exists('/kaggle/input/arc-prize-2025/arc-agi_evaluation_solutions.json') else 'arc-agi_evaluation_solutions.json' \n",
    "evaluation_challenges_path = '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json' if os.path.exists('/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json') else 'arc-agi_evaluation_challenges.json'\n",
    "sample_submission_path = '/kaggle/input/arc-prize-2025/sample_submission.json' if os.path.exists('/kaggle/input/arc-prize-2025/sample_submission.json') else 'sample_submission.json' \n",
    "training_challenges_path = '/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json' if os.path.exists('/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json') else 'arc-agi_training_challenges.json'\n",
    "test_challenges_path = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json' if os.path.exists('/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json') else 'arc-agi_test_challenges.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ffff9",
   "metadata": {
    "papermill": {
     "duration": 0.002729,
     "end_time": "2025-05-07T19:02:23.234779",
     "exception": false,
     "start_time": "2025-05-07T19:02:23.232050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Load the Data</p>\n",
    "\n",
    "- It then loads the contents of these JSON files into Python dictionaries. This includes training puzzles, evaluation puzzles, solutions, and a sample submission file.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c645f4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:02:23.241209Z",
     "iopub.status.busy": "2025-05-07T19:02:23.240990Z",
     "iopub.status.idle": "2025-05-07T19:02:23.882598Z",
     "shell.execute_reply": "2025-05-07T19:02:23.881838Z"
    },
    "papermill": {
     "duration": 0.646457,
     "end_time": "2025-05-07T19:02:23.884002",
     "exception": false,
     "start_time": "2025-05-07T19:02:23.237545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "training_solutions = load_json(training_solutions_path)\n",
    "evaluation_solutions = load_json(evaluation_solutions_path)\n",
    "evaluation_challenges = load_json(evaluation_challenges_path)\n",
    "sample_submission = load_json(sample_submission_path)\n",
    "training_challenges = load_json(training_challenges_path)\n",
    "test_challenges = load_json(test_challenges_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a06321",
   "metadata": {
    "papermill": {
     "duration": 0.005292,
     "end_time": "2025-05-07T19:02:23.898562",
     "exception": false,
     "start_time": "2025-05-07T19:02:23.893270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Choose a Model</p>\n",
    "\n",
    "- Download a pre-trained Qwen model from Kaggle Hub (a place to share models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f613c1eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:02:23.910790Z",
     "iopub.status.busy": "2025-05-07T19:02:23.910530Z",
     "iopub.status.idle": "2025-05-07T19:02:24.450269Z",
     "shell.execute_reply": "2025-05-07T19:02:24.449457Z"
    },
    "papermill": {
     "duration": 0.547022,
     "end_time": "2025-05-07T19:02:24.451762",
     "exception": false,
     "start_time": "2025-05-07T19:02:23.904740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded from Kaggle Hub: /kaggle/input/qwen-3/transformers/0.6b/1\n"
     ]
    }
   ],
   "source": [
    "# Model loading and setup\n",
    "try:\n",
    "    import kagglehub\n",
    "    model_name = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "    print(f\"Model downloaded from Kaggle Hub: {model_name}\")\n",
    "except ImportError:\n",
    "    print(\"Kaggle Hub not available.  Make sure you have kaggle installed and are in a kaggle environment with internet access.\")\n",
    "    model_name = \"Qwen/Qwen-7B\"  # or any other appropriate Qwen model you have access to\n",
    "except Exception as e:\n",
    "     print(f\"Error downloading from Kaggle Hub: {e}.  Using a local or alternative model.\")\n",
    "     model_name = \"Qwen/Qwen-7B\" # Replace this if needed to a suitable model\n",
    "     print(f\"Trying model {model_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aae850",
   "metadata": {
    "papermill": {
     "duration": 0.005275,
     "end_time": "2025-05-07T19:02:24.462436",
     "exception": false,
     "start_time": "2025-05-07T19:02:24.457161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Set Up the Hardware</p>\n",
    "\n",
    "- It determines if a GPU is available (using CUDA) and sets the processing device to either the GPU or the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87463f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:02:24.471630Z",
     "iopub.status.busy": "2025-05-07T19:02:24.471077Z",
     "iopub.status.idle": "2025-05-07T19:02:24.545444Z",
     "shell.execute_reply": "2025-05-07T19:02:24.544657Z"
    },
    "papermill": {
     "duration": 0.079386,
     "end_time": "2025-05-07T19:02:24.546770",
     "exception": false,
     "start_time": "2025-05-07T19:02:24.467384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability and set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ccbb73",
   "metadata": {
    "papermill": {
     "duration": 0.003026,
     "end_time": "2025-05-07T19:02:24.553077",
     "exception": false,
     "start_time": "2025-05-07T19:02:24.550051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Load the Model and Tokenizer</p>\n",
    "\n",
    "- This is the core step. It loads the Qwen model and its corresponding tokenizer. The tokenizer converts text into numbers that the model can understand, and vice versa. The model is also moved to the chosen device (GPU or CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fee6aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:02:24.560243Z",
     "iopub.status.busy": "2025-05-07T19:02:24.559626Z",
     "iopub.status.idle": "2025-05-07T19:02:56.364797Z",
     "shell.execute_reply": "2025-05-07T19:02:56.364225Z"
    },
    "papermill": {
     "duration": 31.810073,
     "end_time": "2025-05-07T19:02:56.366078",
     "exception": false,
     "start_time": "2025-05-07T19:02:24.556005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 19:02:32.108005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746644552.288761      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746644552.341484      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) #trust_remote_code needed for some models\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    if (device == \"cuda\"):\n",
    "        model = model.to(device) # Move the model to the GPU\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise  # Re-raise the exception so the program stops if the model cannot be loaded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3eaf3",
   "metadata": {
    "papermill": {
     "duration": 0.002889,
     "end_time": "2025-05-07T19:02:56.372336",
     "exception": false,
     "start_time": "2025-05-07T19:02:56.369447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Create a Chatbot Class</p>\n",
    "\n",
    "- The chatbot will remember the conversation, and it can use that knowledge to make a better prediction.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60fb3a9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:02:56.379928Z",
     "iopub.status.busy": "2025-05-07T19:02:56.379061Z",
     "iopub.status.idle": "2025-05-07T19:02:56.386125Z",
     "shell.execute_reply": "2025-05-07T19:02:56.385583Z"
    },
    "papermill": {
     "duration": 0.011715,
     "end_time": "2025-05-07T19:02:56.387108",
     "exception": false,
     "start_time": "2025-05-07T19:02:56.375393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QwenChatbot:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.history = []\n",
    "\n",
    "    def generate_response(self, user_input, enable_thinking=True, max_tokens=512):  # Added max_tokens\n",
    "        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)  # Move input to the same device as the model\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens  # Use max_tokens here\n",
    "            )\n",
    "\n",
    "        output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "\n",
    "        #parsing thinking content\n",
    "        try:\n",
    "            # rindex finding 151668 ()\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "        response = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "        # Update history\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return thinking_content, response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b69273",
   "metadata": {
    "papermill": {
     "duration": 0.002877,
     "end_time": "2025-05-07T19:02:56.393057",
     "exception": false,
     "start_time": "2025-05-07T19:02:56.390180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Demonstration and Task Integration</p>\n",
    "\n",
    "- The script includes an if __name__ == \"__main__\": block, which is executed when the script is run directly (not imported as a module).\n",
    "- First input (without /think or /no_think tags, thinking mode is enabled by default)\n",
    "- Second input with /no_think\n",
    "- Third input with /think\n",
    "- The script loops through a few challenges from the training set. For each challenge, it constructs a prompt by showing the model some examples (input/output pairs) and asking it to predict the output for a new input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2521f",
   "metadata": {
    "papermill": {
     "duration": 0.002686,
     "end_time": "2025-05-07T19:02:56.398695",
     "exception": false,
     "start_time": "2025-05-07T19:02:56.396009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create a Chatbot object and interact with the model. \n",
    "- The model can respond in two ways: The first way is that it will tell the thought about the answer. The second way is the real answer that users want to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd278c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:02:56.405681Z",
     "iopub.status.busy": "2025-05-07T19:02:56.405201Z",
     "iopub.status.idle": "2025-05-07T19:03:18.597005Z",
     "shell.execute_reply": "2025-05-07T19:03:18.596109Z"
    },
    "papermill": {
     "duration": 22.196658,
     "end_time": "2025-05-07T19:03:18.598200",
     "exception": false,
     "start_time": "2025-05-07T19:02:56.401542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How many r's in strawberries?\n",
      "Thinking: <think>\n",
      "Okay, the user is asking, \"How many r's in strawberries?\" Let me break this down. First, I need to count the number of 'r's in the word \"strawberries.\" Let me write it out: S-T-R-A-W-B-E-R-R-I-N-G-S. Now, I'll go through each letter one by one. Starting with the first letter 'S', there are no 'r's. Next, 'T' – no. Then 'R' – that's the first 'r'. Then 'A', 'W', 'B', 'E', 'R', 'R', 'I', 'N', 'G'. So here, there are two 'r's. Wait, let me check again. The word is \"strawberries.\" Let me spell it out: S-T-R-A-W-B-E-R-R-I-N-G-S. Yes, the first 'r' and the second 'r' in the middle. So that's two. I think that's correct. The user might be testing if I can count them properly. Let me make sure there's no other 'r's. No, only two. So the answer should be 2.\n",
      "</think>\n",
      "Bot: How many r's in strawberries?  \n",
      "\n",
      "There are **2** r's in the word \"strawberries.\"  \n",
      "\n",
      "**Answer:** 2\n",
      "----------------------\n",
      "User: Then, how many r's in blueberries? /no_think\n",
      "Thinking: \n",
      "Bot: How many r's in blueberries?  \n",
      "\n",
      "There are **2** r's in the word \"blueberries.\"  \n",
      "\n",
      "**Answer:** 2\n",
      "----------------------\n",
      "User: Really? /think\n",
      "Thinking: <think>\n",
      "Okay, the user just said \"Really?\" after asking about the number of r's in strawberries and blueberries. Let me check the previous answers.\n",
      "\n",
      "First, they asked about strawberries, and I said 2 r's. Then blueberries also had 2 r's. Now, they're confirming that the answer is correct. \n",
      "\n",
      "Wait, maybe they want to verify if the answer is right. So I should acknowledge their confirmation and maybe add a friendly note. Let me make sure there's no mistake in the previous answers. Both strawberries and blueberries have exactly two r's. \n",
      "\n",
      "I should respond with a positive affirmation and perhaps offer further assistance if they need anything else. Keep it simple and concise.\n",
      "</think>\n",
      "Bot: Yes, that's correct! Both strawberries and blueberries contain **2 r's**. Let me know if you have more questions! 😊\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Demonstration and ARC Task Integration ---\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = QwenChatbot(model, tokenizer)\n",
    "\n",
    "# Example Usage with the provided test cases.\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n",
    "    user_input_1 = \"How many r's in strawberries?\"\n",
    "    print(f\"User: {user_input_1}\")\n",
    "    thinking_content, response_1 = chatbot.generate_response(user_input_1)\n",
    "    print(f\"Thinking: {thinking_content}\")\n",
    "    print(f\"Bot: {response_1}\")\n",
    "    print(\"----------------------\")\n",
    "\n",
    "    # Second input with /no_think\n",
    "    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n",
    "    print(f\"User: {user_input_2}\")\n",
    "    thinking_content, response_2 = chatbot.generate_response(user_input_2, enable_thinking=False) # explicitly disable thinking for the demo\n",
    "    print(f\"Thinking: {thinking_content}\") # should be empty\n",
    "    print(f\"Bot: {response_2}\")\n",
    "    print(\"----------------------\")\n",
    "\n",
    "    # Third input with /think\n",
    "    user_input_3 = \"Really? /think\"\n",
    "    print(f\"User: {user_input_3}\")\n",
    "    thinking_content, response_3 = chatbot.generate_response(user_input_3, enable_thinking=True)\n",
    "    print(f\"Thinking: {thinking_content}\")\n",
    "    print(f\"Bot: {response_3}\")\n",
    "\n",
    "    print(\"----------------------\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b0e97",
   "metadata": {
    "papermill": {
     "duration": 0.003214,
     "end_time": "2025-05-07T19:03:18.605484",
     "exception": false,
     "start_time": "2025-05-07T19:03:18.602270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <p style=\"padding:10px;background-color:#09abf2;font-family:newtimeroman;font-size:100%;text-align:center;border-radius:12px;font-weight:200;border: 6px outset #f2102e;\">Solve Challenges - ARC dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "538e63d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:03:18.612770Z",
     "iopub.status.busy": "2025-05-07T19:03:18.612543Z",
     "iopub.status.idle": "2025-05-07T19:03:40.981271Z",
     "shell.execute_reply": "2025-05-07T19:03:40.980473Z"
    },
    "papermill": {
     "duration": 22.373865,
     "end_time": "2025-05-07T19:03:40.982581",
     "exception": false,
     "start_time": "2025-05-07T19:03:18.608716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Solving ARC Challenge:\n",
      "Challenge ID: 00576224\n",
      "Thinking: \n",
      "Model's Prediction:\n",
      "<think>\n",
      "Okay, let's see. The user provided two examples and then a new input. The task is to predict the output for the test input [[3,2],[7,8]] based on the patterns in the previous examples. \n",
      "\n",
      "First, I need to understand the pattern from the first two outputs. Let me look at the first input [[7,9],[4,3]] and its output [[7,9,7,9,7,9],[4,3,4,3,4,3],[9,7,9,7,9,7],[3,4,3,4,3,4],[7,9,7,9,7,9],[4,3,4,3,4,3]]. \n",
      "\n",
      "Looking at the output, each line seems to have the same elements as the input but arranged in a different order. Let me check:\n",
      "\n",
      "Input 1:\n",
      "[7,9] → Output starts with [7,9,7,9,7,9]\n",
      "So first two numbers are same as input, then two more, then repeats. Similarly, the next input [[4,3]] gives [4,3,4,3,4,3]. So it's repeating the input elements in each line, but with a different order. \n",
      "\n",
      "Now, the test input is [[3,2],[7,8]]. Let's apply the same pattern. \n",
      "\n",
      "First line: [3,2] → Output starts with [3,2,3,2,3,2]\n",
      "Second line: [7,8] → Output starts with [7,8,7,8,7,8]\n",
      "\n",
      "So the output should be:\n",
      "\n",
      "[[3,2,3,2,3,2], [7,8,7,8,7,8]]\n",
      "\n",
      "But wait, let me check if there's another pattern. Maybe each line is formed by concatenating the elements of the input in order. For example, input [a, b] becomes [a, b, a, b]. So for the first input, [7,9] becomes [7,9,7,9], which matches the output. Similarly, [4,3] becomes [4,3,4,3]. So yes, the pattern is correct. \n",
      "\n",
      "Therefore, applying the same logic to the test input [[3,2],[7,8]] should produce the same output as the example. The answer should be:\n",
      "\n",
      "[[3,2,3,2\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Example of integrating with the ARC dataset ---\n",
    "# Let's try to answer one of the training challenges.\n",
    "sample_challenge_id = list(training_challenges.keys())[0]  # Get the first challenge ID\n",
    "challenge = training_challenges[sample_challenge_id]\n",
    "\n",
    "# Construct a prompt that describes the task and provides examples\n",
    "prompt = f\"Solve the following abstract reasoning challenge.  Here's the challenge:\\n\\n\"\n",
    "for i, task in enumerate(challenge['train']): # Use examples from the training set to build the prompt\n",
    "     prompt += f\"Input {i+1}:\\n\"\n",
    "     prompt += str(task['input']) + \"\\n\"\n",
    "     prompt += f\"Output {i+1}:\\n\"\n",
    "     prompt += str(task['output']) + \"\\n\\n\"\n",
    "\n",
    "# Add the test input.  Tell the model to predict this output\n",
    "prompt += \"Now, predict the output for the following test input:\\n\"\n",
    "prompt += str(challenge['test'][0]['input']) + \"\\n\"\n",
    "prompt += \"Output:\\n\"  # Ask the model to generate the output\n",
    "\n",
    "# Generate the response\n",
    "print(\"----------------------\")\n",
    "print(\"Solving ARC Challenge:\")\n",
    "print(f\"Challenge ID: {sample_challenge_id}\")\n",
    "thinking_content, arc_response = chatbot.generate_response(prompt)\n",
    "print(f\"Thinking: {thinking_content}\")\n",
    "print(f\"Model's Prediction:\\n{arc_response}\")\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e92f17",
   "metadata": {
    "papermill": {
     "duration": 0.003485,
     "end_time": "2025-05-07T19:03:40.989953",
     "exception": false,
     "start_time": "2025-05-07T19:03:40.986468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301506,
     "sourceId": 363124,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 93.267523,
   "end_time": "2025-05-07T19:03:44.445492",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-07T19:02:11.177969",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
