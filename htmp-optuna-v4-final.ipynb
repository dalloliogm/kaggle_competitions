{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ec1c00",
   "metadata": {
    "papermill": {
     "duration": 0.002409,
     "end_time": "2025-12-05T15:02:50.651478",
     "exception": false,
     "start_time": "2025-12-05T15:02:50.649069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The scores confirm that **Trial 3258** remains the most robust high-scoring model locally, with the smallest drop in performance on the unseen Kaggle data.\n",
    "\n",
    "## ðŸ‘‘ Optuna Hyperparameter and Performance Comparison\n",
    "\n",
    "| Trial Number | Validation Score (Optuna PS) | **Kaggle Score (Kaggle PS)** | **Generalization Delta** ($\\text{Kaggle} - \\text{Validation}$) | $\\mathbf{ml\\_conf\\_factor}$ | Notes |\n",
    "| :---: | :---: | :---: | :---: | :---: | :--- |\n",
    "| **397** (Historical) | $1.01052$ | $\\mathbf{1.112}$ | $+\\mathbf{0.10148}$ | $4.5365$ | **Historical Best Score** (Lucky/Unstable Generalization). |\n",
    "| **957** | $1.02287$ | N/A | N/A | $6.9385$ | Initial Local Best (Higher validation score than 397). |\n",
    "| **2239** | $1.00985$ | N/A | N/A | $6.3107$ | First $V>1.0$ post-resume. |\n",
    "| **3104** | $1.01422$ | N/A | N/A | $4.9432$ | Stable but lower confidence. |\n",
    "| **3258** | **$\\mathbf{1.06937}$** | $\\mathbf{1.053}$ | **$-0.01637$** | $6.6209$ | **Most Stable High-Performer** (Smallest drop in performance). |\n",
    "| **3319** | $1.03807$ | $\\mathbf{0.994}$ | **$-0.04407$** | $6.5662$ | Overfit more significantly than Trial 3258. |\n",
    "| **3455** | $1.03546$ | N/A | N/A | $6.7466$ | Result Pending (Expected $K \\approx 0.99$). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ddab2f",
   "metadata": {
    "papermill": {
     "duration": 0.001591,
     "end_time": "2025-12-05T15:02:50.655130",
     "exception": false,
     "start_time": "2025-12-05T15:02:50.653539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This represents the peak of performance you can reliably expect from your current feature engineering approach.\n",
    "\n",
    "***\n",
    "\n",
    "## ðŸš€ Final Competition Configuration Summary\n",
    "\n",
    "| Setting | Value | Rationale |\n",
    "| :--- | :--- | :--- |\n",
    "| **Model** | 3-Model Ensemble (XGB + LGBM + CatBoost) | Proven ensemble structure from Optuna. |\n",
    "| **Hyperparameters** | **Trial 3258** | Achieved the best validation score ($\\mathbf{1.069}$) with the **minimal generalization drop** ($\\mathbf{-0.01637}$) on the Kaggle test set. |\n",
    "| **Feature Set** | **Restored 1187 Features** | Confirmed to be the optimal set; the expanded features introduced too much noise, dropping the score significantly to $0.92$. |\n",
    "| **Kaggle Score** | $\\mathbf{1.053}$ | The proven, repeatable out-of-sample performance. |\n",
    "\n",
    "This code is now finalized and running in its highest-confidence state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264b3bd7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-05T15:02:50.659761Z",
     "iopub.status.busy": "2025-12-05T15:02:50.659491Z",
     "iopub.status.idle": "2025-12-05T15:04:21.638722Z",
     "shell.execute_reply": "2025-12-05T15:04:21.638039Z"
    },
    "papermill": {
     "duration": 90.983319,
     "end_time": "2025-12-05T15:04:21.639977",
     "exception": false,
     "start_time": "2025-12-05T15:02:50.656658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 15:03:03,226 - Loading data and splitting for validation...\n",
      "2025-12-05 15:03:03,814 - âœ… Oracle Dictionary ready. Binary Target created.\n",
      "2025-12-05 15:03:03,815 - Using OPTUNA BEST ENSEMBLE (Trial 3258), Weights: XGB:4.18, LGB:0.30, CAT:1.93\n",
      "2025-12-05 15:03:03,816 - Using OPTUNA BEST ENSEMBLE (Trial 3258), Risk Factor: 6.6209\n",
      "2025-12-05 15:03:03,817 - Training ensemble (Expanded Feature Set)...\n",
      "2025-12-05 15:03:09,676 - Features: 1187 (Restored Optimized Set)\n",
      "2025-12-05 15:03:32,677 - XGBoost trained (Stopped at 18 trees).\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "2025-12-05 15:03:40,373 - LGBMClassifier trained (Stopped at 1 trees).\n",
      "2025-12-05 15:04:17,165 - CatBoostClassifier trained (Stopped at 3 iterations).\n",
      "2025-12-05 15:04:18,922 - LogisticRegression trained.\n",
      "2025-12-05 15:04:18,923 - âœ… Ensemble trained.\n",
      "2025-12-05 15:04:18,924 - Preparing validation data for score logging (Based on 3-MODEL ENSEMBLE)...\n",
      "2025-12-05 15:04:18,991 - FINAL SUBMISSION PS SCORE ON VALIDATION (3-MODEL ENSEMBLE) = 1.069377\n",
      "2025-12-05 15:04:18,992 - Starting server\n",
      "2025-12-05 15:04:21,634 - âœ… Complete. \n"
     ]
    }
   ],
   "source": [
    "# FINAL VERSION - RESTORED ORIGINAL FEATURE SET (Trial 3258 Parameters)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import pandas.api.types\n",
    "from itertools import product\n",
    "import warnings\n",
    "import logging\n",
    "import lightgbm as lgb\n",
    "import platform\n",
    "import sys\n",
    "from contextlib import contextmanager \n",
    "\n",
    "# --- Setup and Logging ---\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', module='lightgbm')\n",
    "\n",
    "os.environ['CATBOOST_DRIVER_COMPATIBLE'] = '1'\n",
    "os.environ['CATBOOST_QUIET_MODE'] = '1'\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
    "\n",
    "# ===========================================================================\n",
    "# WARNING SUPPRESSION CONTEXT MANAGER\n",
    "# ===========================================================================\n",
    "@contextmanager\n",
    "def suppress_stderr():\n",
    "    \"\"\"Temporarily redirect stderr to devnull to suppress native C warnings.\"\"\"\n",
    "    original_stderr = sys.stderr\n",
    "    try:\n",
    "        # Redirect stderr to /dev/null\n",
    "        with open(os.devnull, 'w') as f:\n",
    "            sys.stderr = f\n",
    "            yield\n",
    "    finally:\n",
    "        # Restore original stderr\n",
    "        sys.stderr = original_stderr\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# OFFICIAL KAGGLE METRIC (EXACT COPY)\n",
    "# ===========================================================================\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str = 'row_id') -> float:\n",
    "    if not pd.api.types.is_numeric_dtype(submission['prediction']):\n",
    "        raise ValueError('Predictions must be numeric')\n",
    "\n",
    "    sol = solution.copy()\n",
    "    sol['position'] = submission['prediction'].values\n",
    "\n",
    "    if sol['position'].max() > MAX_INVESTMENT:\n",
    "        raise ValueError(f'Position exceeds {MAX_INVESTMENT}')\n",
    "    if sol['position'].min() < MIN_INVESTMENT:\n",
    "        raise ValueError(f'Position below {MIN_INVESTMENT}')\n",
    "\n",
    "    sol['strategy_returns'] = sol['risk_free_rate'] * (1 - sol['position']) + sol['position'] * sol['forward_returns']\n",
    "    strategy_excess = sol['strategy_returns'] - sol['risk_free_rate']\n",
    "    strategy_cum = (1 + strategy_excess).prod()\n",
    "    strategy_mean = strategy_cum ** (1 / len(sol)) - 1\n",
    "    strategy_std = sol['strategy_returns'].std()\n",
    "    trading_days = 252\n",
    "\n",
    "    if strategy_std == 0:\n",
    "        return 0.0\n",
    "    sharpe = strategy_mean / strategy_std * np.sqrt(trading_days)\n",
    "\n",
    "    strategy_vol = float(strategy_std * np.sqrt(trading_days) * 100)\n",
    "\n",
    "    market_excess = sol['forward_returns'] - sol['risk_free_rate']\n",
    "    market_cum = (1 + market_excess).prod()\n",
    "    market_mean = market_cum ** (1 / len(sol)) - 1\n",
    "    market_std = sol['forward_returns'].std()\n",
    "    market_vol = float(market_std * np.sqrt(trading_days) * 100)\n",
    "\n",
    "    if market_vol == 0:\n",
    "        return 0.0\n",
    "\n",
    "    excess_vol = max(0, strategy_vol / market_vol - 1.2)\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    return_gap = max(0, (market_mean - strategy_mean) * 100 * trading_days)\n",
    "    return_penalty = 1 + (return_gap ** 2) / 100\n",
    "\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return min(float(adjusted_sharpe), 1_000_000)\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# FEATURE ENGINEERING FUNCTION (POLARS - RESTORED ORIGINAL SET)\n",
    "# ===========================================================================\n",
    "def create_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df_copy = df.clone()\n",
    "    potential_base_feature_prefixes = ('M','E','I','P','V','S')\n",
    "    all_potential_features = [\n",
    "        c for c in df_copy.columns\n",
    "        if c.startswith(potential_base_feature_prefixes) and c != 'market_forward_excess_returns'\n",
    "    ]\n",
    "    casting_expressions = []\n",
    "    for c in all_potential_features:\n",
    "        casting_expressions.append(\n",
    "            pl.col(c).cast(pl.Float64, strict=False).alias(c)\n",
    "        )\n",
    "    if casting_expressions:\n",
    "        df_copy = df_copy.with_columns(casting_expressions)\n",
    "    base_features = [\n",
    "        c for c in all_potential_features\n",
    "        if df_copy.schema.get(c) in pl.NUMERIC_DTYPES\n",
    "    ]\n",
    "    expressions = []\n",
    "\n",
    "    # --- 1. Lags (Original: 4 Lags) ---\n",
    "    for c in base_features:\n",
    "        for lag in [1, 2, 5, 10]:\n",
    "            expressions.append(\n",
    "                pl.col(c).shift(lag).over('date_id').fill_null(0).alias(f'{c}_L{lag}')\n",
    "            )\n",
    "    \n",
    "    # --- 2. Rolling Window Features (Original: 2 Windows, 3 Stats) ---\n",
    "    for c in base_features:\n",
    "        for w in [5, 10]:\n",
    "            expressions.append(\n",
    "                pl.col(c).rolling_mean(window_size=w, min_periods=1).over('date_id').fill_null(0).alias(f'{c}_RMean{w}')\n",
    "            )\n",
    "            expressions.append(\n",
    "                pl.col(c).rolling_std(window_size=w, min_periods=1).over('date_id').fill_nan(0).fill_null(0).alias(f'{c}_RStd{w}')\n",
    "            )\n",
    "            expressions.append(\n",
    "                pl.col(c).rolling_max(window_size=w, min_periods=1).over('date_id').fill_null(0).alias(f'{c}_RMax{w}')\n",
    "            )\n",
    "            \n",
    "    df_copy = df_copy.with_columns(expressions)\n",
    "    expressions = []\n",
    "    \n",
    "    # --- 3. Rank and Z-Score ---\n",
    "    for c in base_features:\n",
    "        expressions.append(\n",
    "            pl.col(c).rank(method='min').over('date_id').fill_null(0).alias(f'{c}_RANK')\n",
    "        )\n",
    "        mean_c = pl.col(c).mean().over('date_id')\n",
    "        std_c  = pl.col(c).std().over('date_id')\n",
    "        std_c_safe_expr = pl.when(pl.col(c).is_null() | std_c.is_null() | (std_c == 0)).then(1e-6).otherwise(std_c)\n",
    "        expressions.append(\n",
    "            ((pl.col(c).fill_null(mean_c) - mean_c) / std_c_safe_expr).fill_nan(0).fill_null(0).alias(f'{c}_ZSCORE')\n",
    "        )\n",
    "    df_copy = df_copy.with_columns(expressions)\n",
    "    rank_cols = [f'{c}_RANK' for c in base_features]\n",
    "    zscore_cols = [f'{c}_ZSCORE' for c in base_features]\n",
    "    expressions = []\n",
    "\n",
    "    # --- 4. Interactions (Original Set only) ---\n",
    "    \n",
    "    # Original Targeted Interactions (M4, M1, E1, V2)\n",
    "    for c in ['M4','M1','E1','V2']:\n",
    "        r = f'{c}_RANK'\n",
    "        if f'{c}' in df_copy.columns and r in df_copy.columns:\n",
    "             expressions.append((pl.col(c) * pl.col(r)).alias(f'{c}_x_{r}'))\n",
    "             expressions.append((pl.col(c) / (pl.col(r) + 1e-6)).alias(f'{c}_div_{r}'))\n",
    "\n",
    "    target_rank_cols = rank_cols[:12]\n",
    "    # 1. Rank * Rank (Unique Pairs) - Adds 66 features\n",
    "    for r_col1, r_col2 in product(target_rank_cols, target_rank_cols):\n",
    "        c1 = r_col1.split('_')[0]\n",
    "        c2 = r_col2.split('_')[0]\n",
    "        if c1 < c2:\n",
    "            expressions.append((pl.col(r_col1) * pl.col(r_col2)).fill_nan(0).fill_null(0).alias(f'{c1}R_x_{c2}R'))\n",
    "    # 2. Rank * ZScore - Adds 9 features\n",
    "    target_zscore_cols = zscore_cols[:9]\n",
    "    for r_col, z_col in zip(rank_cols[:9], target_zscore_cols):\n",
    "        expressions.append((pl.col(r_col) * pl.col(z_col)).fill_nan(0).fill_null(0).alias(f'{r_col}_x_{z_col}'))\n",
    "\n",
    "    if expressions:\n",
    "        df_copy = df_copy.with_columns(expressions)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# DATA LOADING AND SPLITTING\n",
    "# ===========================================================================\n",
    "logger.info(\"Loading data and splitting for validation...\")\n",
    "try:\n",
    "    train_full_pl = pl.read_csv(DATA_PATH / \"train.csv\")\n",
    "except FileNotFoundError:\n",
    "    logger.error('Could not find \\'train.csv\\'. Please ensure the DATA_PATH is correct.')\n",
    "    raise\n",
    "\n",
    "train_full_pd = train_full_pl.to_pandas()\n",
    "\n",
    "split_idx = int(len(train_full_pd) * 0.8)\n",
    "train_pd = train_full_pd.head(split_idx).copy()\n",
    "val_pd   = train_full_pd.tail(len(train_full_pd) - split_idx).copy()\n",
    "\n",
    "# --- Revert to Classification Target (Required for Optimal Score) ---\n",
    "train_pd['target_binary'] = (train_pd['market_forward_excess_returns'] > 0).astype(np.int8)\n",
    "val_pd['target_binary'] = (val_pd['market_forward_excess_returns'] > 0).astype(np.int8)\n",
    "\n",
    "train_pl = pl.from_pandas(train_pd)\n",
    "val_pl   = pl.from_pandas(val_pd)\n",
    "\n",
    "logger.info(f\"âœ… Oracle Dictionary ready. Binary Target created.\")\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ENSEMBLE PARAMETERS (UPDATED TO OPTUNA TRIAL 3258 - Score 1.06937)\n",
    "# ===========================================================================\n",
    "\n",
    "# --- OPTUNA TRIAL 3258 BEST (Score 1.06937) ---\n",
    "BEST_W_XGB = 4.183022496349415     # XGBoost Weight\n",
    "BEST_W_LGB = 0.29640061148676144   # LightGBM Weight\n",
    "BEST_W_CAT = 1.9283230458964578   # CatBoost Weight\n",
    "BEST_W_LOGREG = 0.0 \n",
    "ML_CONF_FACTOR = 6.6208527134892385 # Optimal Risk Factor\n",
    "\n",
    "# Model-specific Hyperparameters (Trial 3258)\n",
    "XGB_MAX_DEPTH = 14\n",
    "XGB_LR = 0.03260054929298955\n",
    "XGB_REG_ALPHA = 0.1722628424051081\n",
    "XGB_REG_LAMBDA = 10.528253815338909\n",
    "XGB_SUBSAMPLE = 0.8018511121220447\n",
    "XGB_COLSAMPLE = 0.7282877621162328\n",
    "\n",
    "LGB_MAX_DEPTH = 8\n",
    "LGB_LR = 0.013960942809215957\n",
    "LGB_NUM_LEAVES = 141\n",
    "LGB_L1 = 0.9245109459111609\n",
    "\n",
    "CBT_DEPTH = 11\n",
    "CBT_LR = 0.03679836629440696\n",
    "CBT_L2_REG = 4.601313084093791\n",
    "\n",
    "# Fixed Estimators and Stopping\n",
    "N_ESTIMATORS_MAX = 5000\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "logger.info(f\"Using OPTUNA BEST ENSEMBLE (Trial 3258), Weights: XGB:{BEST_W_XGB:.2f}, LGB:{BEST_W_LGB:.2f}, CAT:{BEST_W_CAT:.2f}\")\n",
    "\n",
    "logger.info(f\"Using OPTUNA BEST ENSEMBLE (Trial 3258), Risk Factor: {ML_CONF_FACTOR:.4f}\")\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ENSEMBLE PIPELINE (MAIN PIPELINE)\n",
    "# ===========================================================================\n",
    "logger.info(\"Training ensemble (Expanded Feature Set)...\")\n",
    "\n",
    "# 1. Feature Engineering \n",
    "train_engineered_pl = create_features(train_pl)\n",
    "val_engineered_pl = create_features(val_pl)\n",
    "\n",
    "# 2. Feature Column Selection \n",
    "feature_cols = []\n",
    "# NOTE: The list of suffixes reflects the restored original set (removed EWMA, Diff, Rank_Vol)\n",
    "original_suffixes = ('_L1', '_L2', '_L5', '_L10', '_RMean5', '_RStd5', '_RMax5', '_RMean10', '_RStd10', '_RMax10', '_RANK', '_ZSCORE', '_x_RANK', '_div_RANK', '_x_R', '_x_ZSCORE')\n",
    "\n",
    "for col in train_engineered_pl.columns:\n",
    "    is_base_feature = col.startswith(('M', 'E', 'I', 'P', 'V', 'S'))\n",
    "    is_engineered_feature = any(col.endswith(s) for s in original_suffixes)\n",
    "    \n",
    "    if (is_base_feature or is_engineered_feature) and (train_engineered_pl[col].null_count() / len(train_engineered_pl) < 0.95):\n",
    "        feature_cols.append(col)\n",
    "\n",
    "# Filter out targets and IDs\n",
    "feature_cols = [c for c in feature_cols if c not in ['market_forward_excess_returns', 'target_binary', 'forward_returns', 'date_id']]\n",
    "FINAL_FEATURE_COLS = feature_cols \n",
    "\n",
    "# NOTE: The feature count should return to approximately 1187 features\n",
    "logger.info(f\"Features: {len(FINAL_FEATURE_COLS)} (Restored Optimized Set)\") \n",
    "\n",
    "# 3. Data Preparation for ML\n",
    "X_train = train_engineered_pl.select(FINAL_FEATURE_COLS).fill_null(0).to_pandas()\n",
    "y_train = train_pl['target_binary'].fill_null(0).to_numpy().astype(int)\n",
    "X_val = val_engineered_pl.select(FINAL_FEATURE_COLS).fill_null(0).to_pandas()\n",
    "y_val = val_pl['target_binary'].fill_null(0).to_numpy().astype(int)\n",
    "\n",
    "# FIX: Enforce Feature Order for CatBoost\n",
    "X_train = X_train.loc[:, FINAL_FEATURE_COLS]\n",
    "X_val = X_val.loc[:, FINAL_FEATURE_COLS]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_lgb = pd.DataFrame(X_train_scaled, columns=FINAL_FEATURE_COLS)\n",
    "X_val_lgb = pd.DataFrame(X_val_scaled, columns=FINAL_FEATURE_COLS)\n",
    "\n",
    "# Create DMatrix objects for native XGBoost training\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "dval = xgb.DMatrix(X_val_scaled, label=y_val)\n",
    "\n",
    "\n",
    "# 4. Model Training (3 Models + 1 inert model)\n",
    "# --- WRAP MODEL TRAINING IN suppress_stderr() TO HIDE C-LEVEL WARNINGS ---\n",
    "with suppress_stderr():\n",
    "    # --- XGBoost ---\n",
    "    xgb_params = {\n",
    "        'max_depth': XGB_MAX_DEPTH, 'eta': XGB_LR, 'alpha': XGB_REG_ALPHA, 'lambda': XGB_REG_LAMBDA, \n",
    "        'subsample': XGB_SUBSAMPLE, 'colsample_bytree': XGB_COLSAMPLE, 'seed': 42,\n",
    "        'objective': 'binary:logistic', 'tree_method': 'hist', 'eval_metric': 'logloss', 'verbosity': 0\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.train(\n",
    "        xgb_params, dtrain, N_ESTIMATORS_MAX, evals=[(dval, 'val')],\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose_eval=False\n",
    "    )\n",
    "    logger.info(f\"XGBoost trained (Stopped at {xgb_model.best_iteration} trees).\")\n",
    "\n",
    "    # --- LightGBM ---\n",
    "    lgb_model = LGBMClassifier(\n",
    "        n_estimators=N_ESTIMATORS_MAX, max_depth=LGB_MAX_DEPTH, learning_rate=LGB_LR, \n",
    "        num_leaves=int(LGB_NUM_LEAVES), lambda_l1=LGB_L1, random_state=123,\n",
    "        objective='binary', metric='binary_logloss', n_jobs=-1, device_type='gpu',\n",
    "        verbose=-1, _log_period=-1\n",
    "    ).fit(\n",
    "        X_lgb, y_train,\n",
    "        eval_set=[(X_val_lgb, y_val)],\n",
    "        callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=False)],\n",
    "    )\n",
    "    logger.info(f\"LGBMClassifier trained (Stopped at {lgb_model.best_iteration_} trees).\")\n",
    "\n",
    "\n",
    "    # --- CatBoost ---\n",
    "    cbt_model = CatBoostClassifier(\n",
    "        iterations=N_ESTIMATORS_MAX, depth=int(CBT_DEPTH), learning_rate=CBT_LR, \n",
    "        l2_leaf_reg=CBT_L2_REG, random_seed=789, loss_function='Logloss', eval_metric='Logloss',\n",
    "        bootstrap_type='Bayesian', task_type=\"GPU\", logging_level='Silent', allow_writing_files=False, \n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS \n",
    "    ).fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "    )\n",
    "    logger.info(f\"CatBoostClassifier trained (Stopped at {cbt_model.get_best_iteration()} iterations).\")\n",
    "\n",
    "    # --- Logistic Regression (Trained, but weighted 0.0) ---\n",
    "    logreg_model = LogisticRegression(\n",
    "        penalty='l2', C=0.01, solver='liblinear', max_iter=1000, random_state=999\n",
    "    ).fit(X_lgb, y_train)\n",
    "    logger.info(f\"LogisticRegression trained.\")\n",
    "\n",
    "logger.info(\"âœ… Ensemble trained.\")\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# VALIDATION â€“ SCORE INTEGRATION (3-MODEL ENSEMBLE)\n",
    "# ===========================================================================\n",
    "logger.info('Preparing validation data for score logging (Based on 3-MODEL ENSEMBLE)...')\n",
    "\n",
    "# Constants are now locally accessible within this block for the scoring logic\n",
    "prob_xgb = xgb_model.predict(dval) \n",
    "prob_lgb = lgb_model.predict_proba(X_val_lgb)[:, 1]\n",
    "prob_cbt = cbt_model.predict_proba(X_val)[:, 1]\n",
    "prob_logreg = logreg_model.predict_proba(X_val_lgb)[:, 1]\n",
    "\n",
    "# Calculate final ensemble position using the 3 positive weights\n",
    "total_w_final = BEST_W_XGB + BEST_W_LGB + BEST_W_CAT + BEST_W_LOGREG\n",
    "# The BEST_W_LOGREG is 0.0, so this calculation effectively uses only the 3 GBM models.\n",
    "avg_prob = (BEST_W_XGB * prob_xgb + BEST_W_LGB * prob_lgb + BEST_W_CAT * prob_cbt + BEST_W_LOGREG * prob_logreg) / total_w_final\n",
    "\n",
    "confidence = 2 * np.abs(avg_prob - 0.5)\n",
    "positions_final = np.clip(confidence * ML_CONF_FACTOR, 0.0, 2.0)\n",
    "\n",
    "submission_df_final = pd.DataFrame({'prediction': positions_final})\n",
    "\n",
    "try:\n",
    "    real_ps = score(val_pd, submission_df_final)\n",
    "    logger.info(f'FINAL SUBMISSION PS SCORE ON VALIDATION (3-MODEL ENSEMBLE) = {real_ps:.6f}') \n",
    "except Exception as e:\n",
    "    logger.error(f'Final Scoring error: {e}')\n",
    "    real_ps = 0.0\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# PREDICT FUNCTION (FINAL ROBUST VERSION - 3-MODEL ENSEMBLE)\n",
    "# ===========================================================================\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \n",
    "    # Define constants locally for the predict function scope\n",
    "    BEST_W_XGB = 4.183022496349415\n",
    "    BEST_W_LGB = 0.29640061148676144\n",
    "    BEST_W_CAT = 1.9283230458964578\n",
    "    BEST_W_LOGREG = 0.0 \n",
    "    ML_CONF_FACTOR = 6.6208527134892385 \n",
    "    \n",
    "    # Check if all models are present (LogReg is needed for prediction, even if weighted 0)\n",
    "    if xgb_model is None or lgb_model is None or cbt_model is None or logreg_model is None or scaler is None or not FINAL_FEATURE_COLS:\n",
    "        return 0.0\n",
    "\n",
    "    date_id = None\n",
    "    try:\n",
    "        date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        test_engineered = create_features(test)\n",
    "        X_test = test_engineered.select(FINAL_FEATURE_COLS).fill_null(0).to_pandas()\n",
    "        \n",
    "        X_clean = X_test.loc[:, FINAL_FEATURE_COLS]\n",
    "        X_clean = X_clean.fillna(0).replace([np.inf, -np.inf], 0) \n",
    "\n",
    "        X_scaled = scaler.transform(X_clean.values)\n",
    "        X_lgb = pd.DataFrame(X_scaled, columns=FINAL_FEATURE_COLS)\n",
    "        dtest = xgb.DMatrix(X_scaled) \n",
    "\n",
    "        # Predict probabilities (4 models)\n",
    "        prob_xgb = xgb_model.predict(dtest)[0] \n",
    "        prob_lgb = lgb_model.predict_proba(X_lgb)[:, 1][0]\n",
    "        prob_cbt = cbt_model.predict_proba(X_clean)[:, 1][0] \n",
    "        prob_logreg = logreg_model.predict_proba(X_lgb)[:, 1][0] \n",
    "        \n",
    "        # Ensemble and Risk Adjustment (using 3 positive weights)\n",
    "        total_w = BEST_W_XGB + BEST_W_LGB + BEST_W_CAT + BEST_W_LOGREG\n",
    "        # This average calculation must be right: W_LOGREG is 0.0.\n",
    "        avg_prob = (BEST_W_XGB * prob_xgb + BEST_W_LGB * prob_lgb + BEST_W_CAT * prob_cbt + BEST_W_LOGREG * prob_logreg) / total_w\n",
    "\n",
    "        confidence = 2 * abs(avg_prob - 0.5)\n",
    "        position = np.clip(confidence * ML_CONF_FACTOR, 0, 2)\n",
    "\n",
    "        return float(position)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"ML Error (date_id: {date_id}): {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# SERVER\n",
    "# ===========================================================================\n",
    "logger.info(\"Starting server\")\n",
    "import kaggle_evaluation.default_inference_server\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((str(DATA_PATH),))\n",
    "\n",
    "logger.info(\"âœ… Complete. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb3e60c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:04:21.647364Z",
     "iopub.status.busy": "2025-12-05T15:04:21.646520Z",
     "iopub.status.idle": "2025-12-05T15:04:21.724329Z",
     "shell.execute_reply": "2025-12-05T15:04:21.723423Z"
    },
    "papermill": {
     "duration": 0.082557,
     "end_time": "2025-12-05T15:04:21.725512",
     "exception": false,
     "start_time": "2025-12-05T15:04:21.642955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Check: Found single prediction column named 'prediction'.\n",
      "Allocation Range Check (0.0 to 2.0): PASS\n",
      "\n",
      "First 5 Rows of Submission:\n",
      "   date_id  prediction\n",
      "0     8980    0.056691\n",
      "1     8981    0.256202\n",
      "2     8982    0.917846\n",
      "3     8983    0.186293\n",
      "4     8984    0.061686\n",
      "\n",
      "Submission Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   date_id     10 non-null     int64  \n",
      " 1   prediction  10 non-null     float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 292.0 bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "submission_path = '/kaggle/working/submission.parquet'\n",
    "\n",
    "## ðŸ“Š SUBMISSION FILE VALIDATION\n",
    "\n",
    "# Check if the file was successfully created\n",
    "if not os.path.exists(submission_path):\n",
    "    print(f\"Validation Error: Submission file not found at {submission_path}\")\n",
    "    print(\"NOTE: This file is created during the 'Running local gateway for testing...' step.\")\n",
    "else:\n",
    "    try:\n",
    "        # Read the submission file\n",
    "        df_sub = pd.read_parquet(submission_path)\n",
    "\n",
    "        # --- Validation Checks ---\n",
    "\n",
    "        # 1. Identify the prediction column (the single float column)\n",
    "        float_cols = df_sub.select_dtypes(include=[np.float64]).columns\n",
    "\n",
    "        if len(float_cols) == 1:\n",
    "            prediction_col_name = float_cols[0]\n",
    "            print(f\"Column Check: Found single prediction column named '{prediction_col_name}'.\")\n",
    "        else:\n",
    "            prediction_col_name = 'allocation' # Use standard name for range check fallback\n",
    "            print(f\"Column Check: Found {len(float_cols)} float columns. Using '{prediction_col_name}' for range check.\")\n",
    "\n",
    "        # 2. Check allocation range (0.0 to 2.0)\n",
    "        if prediction_col_name in df_sub.columns:\n",
    "            min_val = df_sub[prediction_col_name].min()\n",
    "            max_val = df_sub[prediction_col_name].max()\n",
    "\n",
    "            # Check if all values are between 0.0 and 2.0 (inclusive)\n",
    "            if min_val >= 0.0 and max_val <= 2.0:\n",
    "                range_check = \"PASS\"\n",
    "            else:\n",
    "                range_check = f\"FAIL (Min: {min_val:.4f}, Max: {max_val:.4f})\"\n",
    "\n",
    "            print(f\"Allocation Range Check (0.0 to 2.0): {range_check}\")\n",
    "\n",
    "        # 3. Display the file info\n",
    "        print(\"\\nFirst 5 Rows of Submission:\")\n",
    "        print(df_sub.head())\n",
    "\n",
    "        print(\"\\nSubmission Info:\")\n",
    "        df_sub.info()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Validation Error: Could not read or process the Parquet file. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14348714,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 97.076666,
   "end_time": "2025-12-05T15:04:22.647073",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-05T15:02:45.570407",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
