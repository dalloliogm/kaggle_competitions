{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080dc252",
   "metadata": {
    "_cell_guid": "8e861b35-20fd-448d-b0f0-2d104793d624",
    "_uuid": "27a97ee3-1674-427b-b615-da99989fef3a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003664,
     "end_time": "2025-10-26T20:36:59.351891",
     "exception": false,
     "start_time": "2025-10-26T20:36:59.348227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ§© Inspired by [S5E10 | NN Stacking â€“ Baseline](https://www.kaggle.com/code/masayakawamata/s5e10-nn-stacking-baseline)\n",
    "> *Huge thanks to Masaya Kawamata for the inspiration behind this work.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9561c1bc",
   "metadata": {
    "_cell_guid": "43caa8e8-7fb7-4b07-a95d-71f64f650af5",
    "_uuid": "728b14db-b80a-4028-95b4-4ee2e8aefddc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:59.359015Z",
     "iopub.status.busy": "2025-10-26T20:36:59.358712Z",
     "iopub.status.idle": "2025-10-26T20:37:07.810293Z",
     "shell.execute_reply": "2025-10-26T20:37:07.809174Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8.457241,
     "end_time": "2025-10-26T20:37:07.812146",
     "exception": false,
     "start_time": "2025-10-26T20:36:59.354905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Import standard libraries\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Import third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define constants\n",
    "TARGET = 'accident_risk'\n",
    "BATCH_SIZE = 768*2\n",
    "MAX_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-3\n",
    "LR_DECAY = 0.950\n",
    "SEED_LIST = [222, 9375, 1418, 2783, 8364, 5464]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31acde6f",
   "metadata": {
    "_cell_guid": "2494328f-8d45-4b65-87b8-e909b3c2a599",
    "_uuid": "f5ff6a05-a201-4431-9ee6-4e876e2ad77d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:37:07.820014Z",
     "iopub.status.busy": "2025-10-26T20:37:07.819557Z",
     "iopub.status.idle": "2025-10-26T20:37:07.824548Z",
     "shell.execute_reply": "2025-10-26T20:37:07.823761Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011103,
     "end_time": "2025-10-26T20:37:07.826330",
     "exception": false,
     "start_time": "2025-10-26T20:37:07.815227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define device selection\n",
    "def get_device():\n",
    "    # Choose CUDA if available, else CPU\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b8baf2",
   "metadata": {
    "_cell_guid": "35a27c90-5119-4c8d-ac8f-b8acc18ffa67",
    "_uuid": "9171dd3b-23a0-4322-a984-76c50fc6a2b3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:37:07.834104Z",
     "iopub.status.busy": "2025-10-26T20:37:07.833258Z",
     "iopub.status.idle": "2025-10-26T20:37:07.838779Z",
     "shell.execute_reply": "2025-10-26T20:37:07.837895Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010999,
     "end_time": "2025-10-26T20:37:07.840504",
     "exception": false,
     "start_time": "2025-10-26T20:37:07.829505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define reproducibility setup\n",
    "def set_seed(seed):\n",
    "    # Set Python seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set NumPy seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set PyTorch CPU seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Set PyTorch CUDA seed if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Enable deterministic operations for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61639273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T20:37:07.848859Z",
     "iopub.status.busy": "2025-10-26T20:37:07.848376Z",
     "iopub.status.idle": "2025-10-26T20:37:07.854555Z",
     "shell.execute_reply": "2025-10-26T20:37:07.853636Z"
    },
    "papermill": {
     "duration": 0.011814,
     "end_time": "2025-10-26T20:37:07.856103",
     "exception": false,
     "start_time": "2025-10-26T20:37:07.844289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define feature engineering\n",
    "def add_engineered_features(df):\n",
    "    # Compute engineered feature\n",
    "    df['engineered_feature'] = (\n",
    "        0.3 * df[\"curvature\"] +\n",
    "        0.2 * (df[\"lighting\"] == \"night\").astype(int) +\n",
    "        0.1 * (df[\"weather\"] != \"clear\").astype(int) +\n",
    "        0.2 * (df[\"speed_limit\"] >= 60).astype(int) +\n",
    "        0.1 * (df[\"num_reported_accidents\"] > 2).astype(int)\n",
    "    )\n",
    "\n",
    "    # Return dataframe with new feature\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9463dd77",
   "metadata": {
    "_cell_guid": "35743f5c-4a40-4b57-ae09-4e33247f9f0e",
    "_uuid": "4e061267-611b-49af-ad5e-dd61b5dc2be7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:37:07.863852Z",
     "iopub.status.busy": "2025-10-26T20:37:07.863482Z",
     "iopub.status.idle": "2025-10-26T20:37:07.869673Z",
     "shell.execute_reply": "2025-10-26T20:37:07.868559Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012145,
     "end_time": "2025-10-26T20:37:07.871372",
     "exception": false,
     "start_time": "2025-10-26T20:37:07.859227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define MLP meta-model\n",
    "class MetaMLP(nn.Module):\n",
    "    # Initialize layers\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1228a987",
   "metadata": {
    "_cell_guid": "64c4b008-4448-4745-adef-ad8926093ee6",
    "_uuid": "564413dd-f949-4fa1-acc4-b7cf4d333408",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:37:07.878726Z",
     "iopub.status.busy": "2025-10-26T20:37:07.878378Z",
     "iopub.status.idle": "2025-10-26T20:37:07.888906Z",
     "shell.execute_reply": "2025-10-26T20:37:07.887905Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015887,
     "end_time": "2025-10-26T20:37:07.890385",
     "exception": false,
     "start_time": "2025-10-26T20:37:07.874498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training routine for one seed using full data\n",
    "def run_one_seed(X_train, y_train, X_test, device):\n",
    "    # Convert full training dataset to tensors\n",
    "    train_ds = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "\n",
    "    # Convert test dataset to tensor\n",
    "    test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    # Build training data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MetaMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Initialize exponential learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_DECAY)\n",
    "\n",
    "    # Initialize loss criterion\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize cumulative training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Iterate over batches in training data\n",
    "        for xb, yb in train_loader:\n",
    "            # Move batch to the selected device\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through model\n",
    "            preds = model(xb)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            # Backpropagate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate total training loss\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # Step the scheduler to decay learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute average training loss\n",
    "        train_loss /= len(train_ds)\n",
    "\n",
    "        # Compute training RMSE\n",
    "        train_rmse = np.sqrt(train_loss)\n",
    "\n",
    "        # Print epoch-level performance\n",
    "        print(f\"Epoch {epoch + 1:03d} | LR: {scheduler.get_last_lr()[0]:.6f} | Train RMSE: {train_rmse:.5f}\")\n",
    "\n",
    "    # Set model to evaluation mode for predictions\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation for prediction\n",
    "    with torch.no_grad():\n",
    "        # Generate training predictions\n",
    "        train_preds = model(\n",
    "            torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "        ).cpu().view(-1).numpy()\n",
    "\n",
    "        # Generate test predictions\n",
    "        test_preds = model(\n",
    "            test_tensor.to(device)\n",
    "        ).cpu().view(-1).numpy()\n",
    "\n",
    "    # Return training and test predictions\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cfca75a",
   "metadata": {
    "_cell_guid": "31efa8f5-214a-46f8-aefe-850cb89a10ef",
    "_uuid": "a596cf45-83ef-4db3-b83d-53e5e3f670af",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:37:07.897659Z",
     "iopub.status.busy": "2025-10-26T20:37:07.897328Z",
     "iopub.status.idle": "2025-10-26T20:37:07.903542Z",
     "shell.execute_reply": "2025-10-26T20:37:07.902624Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01202,
     "end_time": "2025-10-26T20:37:07.905400",
     "exception": false,
     "start_time": "2025-10-26T20:37:07.893380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dataframe merge by ID\n",
    "def merge_dataframes_by_id(data_list, id_col='id', feature_col=TARGET):\n",
    "    # Select the first dataframe in the list\n",
    "    first = data_list[0]\n",
    "\n",
    "    # Rename the feature column of the first dataframe using its model name\n",
    "    merged = first['df'].rename(columns={feature_col: f\"{feature_col}_{first['name']}\"})\n",
    "\n",
    "    # Iterate over the remaining dataframes in the list\n",
    "    for data in data_list[1:]:\n",
    "        # Rename the feature column in the current dataframe\n",
    "        renamed = data['df'].rename(columns={feature_col: f\"{feature_col}_{data['name']}\"})\n",
    "\n",
    "        # Merge the renamed dataframe with the accumulated merged dataframe\n",
    "        merged = pd.merge(merged, renamed, on=id_col, how='outer')\n",
    "\n",
    "    # Return the final merged dataframe\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c3d750",
   "metadata": {
    "_cell_guid": "768fbac8-b245-48e3-a88d-ae9d3cde0fad",
    "_uuid": "3e5a4d38-92d3-4c7c-b046-851bf307bd56",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:37:07.913636Z",
     "iopub.status.busy": "2025-10-26T20:37:07.913284Z",
     "iopub.status.idle": "2025-10-26T20:37:07.927287Z",
     "shell.execute_reply": "2025-10-26T20:37:07.926242Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020049,
     "end_time": "2025-10-26T20:37:07.928903",
     "exception": false,
     "start_time": "2025-10-26T20:37:07.908854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the main execution\n",
    "def main():\n",
    "    # Load training data\n",
    "    train = pd.read_csv('/kaggle/input/playground-series-s5e10/train.csv')\n",
    "\n",
    "    # Load test data\n",
    "    test = pd.read_csv('/kaggle/input/playground-series-s5e10/test.csv')\n",
    "\n",
    "    # Discover all OOF (out-of-fold) prediction files\n",
    "    oof_files = glob.glob('/kaggle/input/**/oof_*.csv', recursive=True)\n",
    "\n",
    "    # Print the number of OOF files found\n",
    "    print(f\"Found {len(oof_files)} OOF files.\")\n",
    "\n",
    "    # Initialize containers for OOF and test data\n",
    "    all_oof_data = []\n",
    "    all_test_data = []\n",
    "\n",
    "    # Iterate through all OOF file paths\n",
    "    for oof_path in oof_files:\n",
    "        # Construct corresponding test file path\n",
    "        test_path = oof_path.replace('oof_', 'test_')\n",
    "\n",
    "        # Extract model name from file path\n",
    "        model_name = os.path.basename(oof_path).replace('oof_', '').replace('.csv', '')\n",
    "\n",
    "        # Load OOF data and store with model name\n",
    "        all_oof_data.append({'df': pd.read_csv(oof_path), 'name': model_name})\n",
    "\n",
    "        # Load test data and store with model name\n",
    "        all_test_data.append({'df': pd.read_csv(test_path), 'name': model_name})\n",
    "\n",
    "    # Merge OOF dataframes by ID\n",
    "    oof_df = merge_dataframes_by_id(all_oof_data)\n",
    "\n",
    "    # Merge test dataframes by ID\n",
    "    test_df = merge_dataframes_by_id(all_test_data)\n",
    "\n",
    "    # Attach ground truth target to OOF dataframe\n",
    "    oof_df[TARGET] = train[TARGET].values\n",
    "\n",
    "    # Merge in base train/test features to apply feature engineering\n",
    "    oof_df = pd.merge(oof_df, train[['id', 'curvature', 'lighting', 'weather', 'speed_limit', 'num_reported_accidents']], on='id', how='left')\n",
    "    test_df = pd.merge(test_df, test[['id', 'curvature', 'lighting', 'weather', 'speed_limit', 'num_reported_accidents']], on='id', how='left')\n",
    "\n",
    "    # Apply feature engineering\n",
    "    oof_df = add_engineered_features(oof_df)\n",
    "    test_df = add_engineered_features(test_df)\n",
    "\n",
    "    # Identify numerical feature columns\n",
    "    num_features = oof_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Exclude ID and target columns from features\n",
    "    FEATURES = [f for f in num_features if f not in ['id', TARGET]]\n",
    "\n",
    "    # Prepare feature matrix and target vector\n",
    "    X = oof_df[FEATURES]\n",
    "    y = oof_df[TARGET]\n",
    "\n",
    "    # Copy test feature matrix\n",
    "    X_test_full = test_df[FEATURES].copy()\n",
    "\n",
    "    # Initialize standard scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit scaler on full training data and transform all splits\n",
    "    X_train_scaled = scaler.fit_transform(X)\n",
    "    X_test_scaled = scaler.transform(X_test_full)\n",
    "\n",
    "    # Select computation device (CPU or GPU)\n",
    "    device = get_device()\n",
    "\n",
    "    # Initialize arrays for storing averaged predictions\n",
    "    train_pred_accum = np.zeros(len(X))\n",
    "    test_pred_accum = np.zeros(len(X_test_full))\n",
    "\n",
    "    # Loop through each random seed for model averaging\n",
    "    for seed in SEED_LIST:\n",
    "        # Set reproducible random seed\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Print current seed\n",
    "        print(f\"\\n--- Training with seed {seed} ---\")\n",
    "\n",
    "        # Train and predict with one seed\n",
    "        train_preds_seed, test_preds_seed = run_one_seed(\n",
    "            X_train_scaled,\n",
    "            y,\n",
    "            X_test_scaled,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # Accumulate averaged training predictions\n",
    "        train_pred_accum += train_preds_seed / len(SEED_LIST)\n",
    "\n",
    "        # Accumulate averaged test predictions\n",
    "        test_pred_accum += test_preds_seed / len(SEED_LIST)\n",
    "\n",
    "    # Compute RMSE on full training set\n",
    "    train_rmse = mean_squared_error(y, train_pred_accum, squared=False)\n",
    "\n",
    "    # Print overall training performance\n",
    "    print(f\"\\nFull Training RMSE: {train_rmse:.5f}\")\n",
    "\n",
    "    # Create dataframe for current model predictions\n",
    "    new_submission = pd.DataFrame({'id': test.id, TARGET: test_pred_accum})\n",
    "\n",
    "    # ======================================================\n",
    "    # Blend with existing saved submissions using weights\n",
    "    # ======================================================\n",
    "\n",
    "    # Define paths to other saved submissions\n",
    "    blend_files = [\n",
    "        '/kaggle/input/predicting-road-accident-risk-vault/autogluon15.csv',\n",
    "        '/kaggle/input/predicting-road-accident-risk-vault/submission.csv'\n",
    "    ]\n",
    "\n",
    "    # Define blending weights (must sum to 1 with new model)\n",
    "    blend_weights = [0.6, 0.375, 0.025]  # [modelA, modelB, new_model]\n",
    "\n",
    "    # Initialize blended submission with zeros\n",
    "    blended = pd.DataFrame({'id': test.id, TARGET: np.zeros(len(test))})\n",
    "\n",
    "    # Loop through saved submissions and blend\n",
    "    for path, weight in zip(blend_files, blend_weights[:-1]):\n",
    "        # Load existing submission\n",
    "        sub = pd.read_csv(path)\n",
    "\n",
    "        # Add weighted predictions\n",
    "        blended[TARGET] += sub[TARGET] * weight\n",
    "\n",
    "    # Add current model predictions with its weight\n",
    "    blended[TARGET] += new_submission[TARGET] * blend_weights[-1]\n",
    "\n",
    "    # Save final blended submission\n",
    "    blended.to_csv('submission.csv', index=False)\n",
    "\n",
    "    # Print confirmation\n",
    "    print(\"\\nBlended submission saved as 'submission.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3410d941",
   "metadata": {
    "_cell_guid": "10ab92fe-bb22-4ade-b20c-641131df3cff",
    "_uuid": "fc0a7dcf-b243-4bfc-a33e-5fdec90e690b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:37:07.936144Z",
     "iopub.status.busy": "2025-10-26T20:37:07.935841Z",
     "iopub.status.idle": "2025-10-26T21:09:51.673233Z",
     "shell.execute_reply": "2025-10-26T21:09:51.672064Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1963.742687,
     "end_time": "2025-10-26T21:09:51.674850",
     "exception": false,
     "start_time": "2025-10-26T20:37:07.932163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 OOF files.\n",
      "\n",
      "--- Training with seed 222 ---\n",
      "Epoch 001 | LR: 0.004750 | Train RMSE: 0.07735\n",
      "Epoch 002 | LR: 0.004513 | Train RMSE: 0.05622\n",
      "Epoch 003 | LR: 0.004287 | Train RMSE: 0.05617\n",
      "Epoch 004 | LR: 0.004073 | Train RMSE: 0.05617\n",
      "Epoch 005 | LR: 0.003869 | Train RMSE: 0.05616\n",
      "Epoch 006 | LR: 0.003675 | Train RMSE: 0.05617\n",
      "Epoch 007 | LR: 0.003492 | Train RMSE: 0.05612\n",
      "Epoch 008 | LR: 0.003317 | Train RMSE: 0.05610\n",
      "Epoch 009 | LR: 0.003151 | Train RMSE: 0.05609\n",
      "Epoch 010 | LR: 0.002994 | Train RMSE: 0.05608\n",
      "Epoch 011 | LR: 0.002844 | Train RMSE: 0.05612\n",
      "Epoch 012 | LR: 0.002702 | Train RMSE: 0.05613\n",
      "Epoch 013 | LR: 0.002567 | Train RMSE: 0.05606\n",
      "Epoch 014 | LR: 0.002438 | Train RMSE: 0.05607\n",
      "Epoch 015 | LR: 0.002316 | Train RMSE: 0.05604\n",
      "Epoch 016 | LR: 0.002201 | Train RMSE: 0.05603\n",
      "Epoch 017 | LR: 0.002091 | Train RMSE: 0.05604\n",
      "Epoch 018 | LR: 0.001986 | Train RMSE: 0.05604\n",
      "Epoch 019 | LR: 0.001887 | Train RMSE: 0.05602\n",
      "Epoch 020 | LR: 0.001792 | Train RMSE: 0.05604\n",
      "Epoch 021 | LR: 0.001703 | Train RMSE: 0.05603\n",
      "Epoch 022 | LR: 0.001618 | Train RMSE: 0.05602\n",
      "Epoch 023 | LR: 0.001537 | Train RMSE: 0.05603\n",
      "Epoch 024 | LR: 0.001460 | Train RMSE: 0.05600\n",
      "Epoch 025 | LR: 0.001387 | Train RMSE: 0.05599\n",
      "Epoch 026 | LR: 0.001318 | Train RMSE: 0.05600\n",
      "Epoch 027 | LR: 0.001252 | Train RMSE: 0.05597\n",
      "Epoch 028 | LR: 0.001189 | Train RMSE: 0.05600\n",
      "Epoch 029 | LR: 0.001130 | Train RMSE: 0.05599\n",
      "Epoch 030 | LR: 0.001073 | Train RMSE: 0.05596\n",
      "Epoch 031 | LR: 0.001020 | Train RMSE: 0.05597\n",
      "Epoch 032 | LR: 0.000969 | Train RMSE: 0.05598\n",
      "Epoch 033 | LR: 0.000920 | Train RMSE: 0.05598\n",
      "Epoch 034 | LR: 0.000874 | Train RMSE: 0.05598\n",
      "Epoch 035 | LR: 0.000830 | Train RMSE: 0.05594\n",
      "Epoch 036 | LR: 0.000789 | Train RMSE: 0.05596\n",
      "Epoch 037 | LR: 0.000749 | Train RMSE: 0.05595\n",
      "Epoch 038 | LR: 0.000712 | Train RMSE: 0.05594\n",
      "Epoch 039 | LR: 0.000676 | Train RMSE: 0.05594\n",
      "Epoch 040 | LR: 0.000643 | Train RMSE: 0.05594\n",
      "Epoch 041 | LR: 0.000610 | Train RMSE: 0.05594\n",
      "Epoch 042 | LR: 0.000580 | Train RMSE: 0.05594\n",
      "Epoch 043 | LR: 0.000551 | Train RMSE: 0.05593\n",
      "Epoch 044 | LR: 0.000523 | Train RMSE: 0.05593\n",
      "Epoch 045 | LR: 0.000497 | Train RMSE: 0.05592\n",
      "Epoch 046 | LR: 0.000472 | Train RMSE: 0.05593\n",
      "Epoch 047 | LR: 0.000449 | Train RMSE: 0.05592\n",
      "Epoch 048 | LR: 0.000426 | Train RMSE: 0.05591\n",
      "Epoch 049 | LR: 0.000405 | Train RMSE: 0.05592\n",
      "Epoch 050 | LR: 0.000385 | Train RMSE: 0.05591\n",
      "\n",
      "--- Training with seed 9375 ---\n",
      "Epoch 001 | LR: 0.004750 | Train RMSE: 0.06363\n",
      "Epoch 002 | LR: 0.004513 | Train RMSE: 0.05621\n",
      "Epoch 003 | LR: 0.004287 | Train RMSE: 0.05616\n",
      "Epoch 004 | LR: 0.004073 | Train RMSE: 0.05618\n",
      "Epoch 005 | LR: 0.003869 | Train RMSE: 0.05616\n",
      "Epoch 006 | LR: 0.003675 | Train RMSE: 0.05612\n",
      "Epoch 007 | LR: 0.003492 | Train RMSE: 0.05613\n",
      "Epoch 008 | LR: 0.003317 | Train RMSE: 0.05612\n",
      "Epoch 009 | LR: 0.003151 | Train RMSE: 0.05610\n",
      "Epoch 010 | LR: 0.002994 | Train RMSE: 0.05610\n",
      "Epoch 011 | LR: 0.002844 | Train RMSE: 0.05611\n",
      "Epoch 012 | LR: 0.002702 | Train RMSE: 0.05603\n",
      "Epoch 013 | LR: 0.002567 | Train RMSE: 0.05606\n",
      "Epoch 014 | LR: 0.002438 | Train RMSE: 0.05607\n",
      "Epoch 015 | LR: 0.002316 | Train RMSE: 0.05606\n",
      "Epoch 016 | LR: 0.002201 | Train RMSE: 0.05604\n",
      "Epoch 017 | LR: 0.002091 | Train RMSE: 0.05603\n",
      "Epoch 018 | LR: 0.001986 | Train RMSE: 0.05602\n",
      "Epoch 019 | LR: 0.001887 | Train RMSE: 0.05600\n",
      "Epoch 020 | LR: 0.001792 | Train RMSE: 0.05600\n",
      "Epoch 021 | LR: 0.001703 | Train RMSE: 0.05600\n",
      "Epoch 022 | LR: 0.001618 | Train RMSE: 0.05599\n",
      "Epoch 023 | LR: 0.001537 | Train RMSE: 0.05599\n",
      "Epoch 024 | LR: 0.001460 | Train RMSE: 0.05600\n",
      "Epoch 025 | LR: 0.001387 | Train RMSE: 0.05597\n",
      "Epoch 026 | LR: 0.001318 | Train RMSE: 0.05597\n",
      "Epoch 027 | LR: 0.001252 | Train RMSE: 0.05598\n",
      "Epoch 028 | LR: 0.001189 | Train RMSE: 0.05598\n",
      "Epoch 029 | LR: 0.001130 | Train RMSE: 0.05596\n",
      "Epoch 030 | LR: 0.001073 | Train RMSE: 0.05597\n",
      "Epoch 031 | LR: 0.001020 | Train RMSE: 0.05596\n",
      "Epoch 032 | LR: 0.000969 | Train RMSE: 0.05594\n",
      "Epoch 033 | LR: 0.000920 | Train RMSE: 0.05595\n",
      "Epoch 034 | LR: 0.000874 | Train RMSE: 0.05594\n",
      "Epoch 035 | LR: 0.000830 | Train RMSE: 0.05593\n",
      "Epoch 036 | LR: 0.000789 | Train RMSE: 0.05593\n",
      "Epoch 037 | LR: 0.000749 | Train RMSE: 0.05595\n",
      "Epoch 038 | LR: 0.000712 | Train RMSE: 0.05593\n",
      "Epoch 039 | LR: 0.000676 | Train RMSE: 0.05593\n",
      "Epoch 040 | LR: 0.000643 | Train RMSE: 0.05592\n",
      "Epoch 041 | LR: 0.000610 | Train RMSE: 0.05594\n",
      "Epoch 042 | LR: 0.000580 | Train RMSE: 0.05592\n",
      "Epoch 043 | LR: 0.000551 | Train RMSE: 0.05593\n",
      "Epoch 044 | LR: 0.000523 | Train RMSE: 0.05592\n",
      "Epoch 045 | LR: 0.000497 | Train RMSE: 0.05591\n",
      "Epoch 046 | LR: 0.000472 | Train RMSE: 0.05591\n",
      "Epoch 047 | LR: 0.000449 | Train RMSE: 0.05591\n",
      "Epoch 048 | LR: 0.000426 | Train RMSE: 0.05591\n",
      "Epoch 049 | LR: 0.000405 | Train RMSE: 0.05590\n",
      "Epoch 050 | LR: 0.000385 | Train RMSE: 0.05591\n",
      "\n",
      "--- Training with seed 1418 ---\n",
      "Epoch 001 | LR: 0.004750 | Train RMSE: 0.08930\n",
      "Epoch 002 | LR: 0.004513 | Train RMSE: 0.05628\n",
      "Epoch 003 | LR: 0.004287 | Train RMSE: 0.05617\n",
      "Epoch 004 | LR: 0.004073 | Train RMSE: 0.05616\n",
      "Epoch 005 | LR: 0.003869 | Train RMSE: 0.05615\n",
      "Epoch 006 | LR: 0.003675 | Train RMSE: 0.05613\n",
      "Epoch 007 | LR: 0.003492 | Train RMSE: 0.05612\n",
      "Epoch 008 | LR: 0.003317 | Train RMSE: 0.05617\n",
      "Epoch 009 | LR: 0.003151 | Train RMSE: 0.05610\n",
      "Epoch 010 | LR: 0.002994 | Train RMSE: 0.05609\n",
      "Epoch 011 | LR: 0.002844 | Train RMSE: 0.05610\n",
      "Epoch 012 | LR: 0.002702 | Train RMSE: 0.05616\n",
      "Epoch 013 | LR: 0.002567 | Train RMSE: 0.05608\n",
      "Epoch 014 | LR: 0.002438 | Train RMSE: 0.05607\n",
      "Epoch 015 | LR: 0.002316 | Train RMSE: 0.05610\n",
      "Epoch 016 | LR: 0.002201 | Train RMSE: 0.05605\n",
      "Epoch 017 | LR: 0.002091 | Train RMSE: 0.05604\n",
      "Epoch 018 | LR: 0.001986 | Train RMSE: 0.05605\n",
      "Epoch 019 | LR: 0.001887 | Train RMSE: 0.05605\n",
      "Epoch 020 | LR: 0.001792 | Train RMSE: 0.05602\n",
      "Epoch 021 | LR: 0.001703 | Train RMSE: 0.05605\n",
      "Epoch 022 | LR: 0.001618 | Train RMSE: 0.05608\n",
      "Epoch 023 | LR: 0.001537 | Train RMSE: 0.05601\n",
      "Epoch 024 | LR: 0.001460 | Train RMSE: 0.05600\n",
      "Epoch 025 | LR: 0.001387 | Train RMSE: 0.05601\n",
      "Epoch 026 | LR: 0.001318 | Train RMSE: 0.05600\n",
      "Epoch 027 | LR: 0.001252 | Train RMSE: 0.05599\n",
      "Epoch 028 | LR: 0.001189 | Train RMSE: 0.05600\n",
      "Epoch 029 | LR: 0.001130 | Train RMSE: 0.05598\n",
      "Epoch 030 | LR: 0.001073 | Train RMSE: 0.05599\n",
      "Epoch 031 | LR: 0.001020 | Train RMSE: 0.05598\n",
      "Epoch 032 | LR: 0.000969 | Train RMSE: 0.05598\n",
      "Epoch 033 | LR: 0.000920 | Train RMSE: 0.05596\n",
      "Epoch 034 | LR: 0.000874 | Train RMSE: 0.05596\n",
      "Epoch 035 | LR: 0.000830 | Train RMSE: 0.05595\n",
      "Epoch 036 | LR: 0.000789 | Train RMSE: 0.05596\n",
      "Epoch 037 | LR: 0.000749 | Train RMSE: 0.05595\n",
      "Epoch 038 | LR: 0.000712 | Train RMSE: 0.05595\n",
      "Epoch 039 | LR: 0.000676 | Train RMSE: 0.05595\n",
      "Epoch 040 | LR: 0.000643 | Train RMSE: 0.05595\n",
      "Epoch 041 | LR: 0.000610 | Train RMSE: 0.05596\n",
      "Epoch 042 | LR: 0.000580 | Train RMSE: 0.05594\n",
      "Epoch 043 | LR: 0.000551 | Train RMSE: 0.05593\n",
      "Epoch 044 | LR: 0.000523 | Train RMSE: 0.05593\n",
      "Epoch 045 | LR: 0.000497 | Train RMSE: 0.05593\n",
      "Epoch 046 | LR: 0.000472 | Train RMSE: 0.05594\n",
      "Epoch 047 | LR: 0.000449 | Train RMSE: 0.05593\n",
      "Epoch 048 | LR: 0.000426 | Train RMSE: 0.05592\n",
      "Epoch 049 | LR: 0.000405 | Train RMSE: 0.05593\n",
      "Epoch 050 | LR: 0.000385 | Train RMSE: 0.05593\n",
      "\n",
      "--- Training with seed 2783 ---\n",
      "Epoch 001 | LR: 0.004750 | Train RMSE: 0.06277\n",
      "Epoch 002 | LR: 0.004513 | Train RMSE: 0.05616\n",
      "Epoch 003 | LR: 0.004287 | Train RMSE: 0.05612\n",
      "Epoch 004 | LR: 0.004073 | Train RMSE: 0.05611\n",
      "Epoch 005 | LR: 0.003869 | Train RMSE: 0.05609\n",
      "Epoch 006 | LR: 0.003675 | Train RMSE: 0.05611\n",
      "Epoch 007 | LR: 0.003492 | Train RMSE: 0.05607\n",
      "Epoch 008 | LR: 0.003317 | Train RMSE: 0.05606\n",
      "Epoch 009 | LR: 0.003151 | Train RMSE: 0.05603\n",
      "Epoch 010 | LR: 0.002994 | Train RMSE: 0.05611\n",
      "Epoch 011 | LR: 0.002844 | Train RMSE: 0.05606\n",
      "Epoch 012 | LR: 0.002702 | Train RMSE: 0.05604\n",
      "Epoch 013 | LR: 0.002567 | Train RMSE: 0.05605\n",
      "Epoch 014 | LR: 0.002438 | Train RMSE: 0.05605\n",
      "Epoch 015 | LR: 0.002316 | Train RMSE: 0.05602\n",
      "Epoch 016 | LR: 0.002201 | Train RMSE: 0.05602\n",
      "Epoch 017 | LR: 0.002091 | Train RMSE: 0.05599\n",
      "Epoch 018 | LR: 0.001986 | Train RMSE: 0.05600\n",
      "Epoch 019 | LR: 0.001887 | Train RMSE: 0.05601\n",
      "Epoch 020 | LR: 0.001792 | Train RMSE: 0.05599\n",
      "Epoch 021 | LR: 0.001703 | Train RMSE: 0.05597\n",
      "Epoch 022 | LR: 0.001618 | Train RMSE: 0.05598\n",
      "Epoch 023 | LR: 0.001537 | Train RMSE: 0.05599\n",
      "Epoch 024 | LR: 0.001460 | Train RMSE: 0.05597\n",
      "Epoch 025 | LR: 0.001387 | Train RMSE: 0.05597\n",
      "Epoch 026 | LR: 0.001318 | Train RMSE: 0.05598\n",
      "Epoch 027 | LR: 0.001252 | Train RMSE: 0.05596\n",
      "Epoch 028 | LR: 0.001189 | Train RMSE: 0.05596\n",
      "Epoch 029 | LR: 0.001130 | Train RMSE: 0.05595\n",
      "Epoch 030 | LR: 0.001073 | Train RMSE: 0.05595\n",
      "Epoch 031 | LR: 0.001020 | Train RMSE: 0.05595\n",
      "Epoch 032 | LR: 0.000969 | Train RMSE: 0.05594\n",
      "Epoch 033 | LR: 0.000920 | Train RMSE: 0.05595\n",
      "Epoch 034 | LR: 0.000874 | Train RMSE: 0.05594\n",
      "Epoch 035 | LR: 0.000830 | Train RMSE: 0.05593\n",
      "Epoch 036 | LR: 0.000789 | Train RMSE: 0.05593\n",
      "Epoch 037 | LR: 0.000749 | Train RMSE: 0.05593\n",
      "Epoch 038 | LR: 0.000712 | Train RMSE: 0.05593\n",
      "Epoch 039 | LR: 0.000676 | Train RMSE: 0.05592\n",
      "Epoch 040 | LR: 0.000643 | Train RMSE: 0.05592\n",
      "Epoch 041 | LR: 0.000610 | Train RMSE: 0.05592\n",
      "Epoch 042 | LR: 0.000580 | Train RMSE: 0.05591\n",
      "Epoch 043 | LR: 0.000551 | Train RMSE: 0.05591\n",
      "Epoch 044 | LR: 0.000523 | Train RMSE: 0.05591\n",
      "Epoch 045 | LR: 0.000497 | Train RMSE: 0.05591\n",
      "Epoch 046 | LR: 0.000472 | Train RMSE: 0.05591\n",
      "Epoch 047 | LR: 0.000449 | Train RMSE: 0.05590\n",
      "Epoch 048 | LR: 0.000426 | Train RMSE: 0.05590\n",
      "Epoch 049 | LR: 0.000405 | Train RMSE: 0.05590\n",
      "Epoch 050 | LR: 0.000385 | Train RMSE: 0.05589\n",
      "\n",
      "--- Training with seed 8364 ---\n",
      "Epoch 001 | LR: 0.004750 | Train RMSE: 0.07050\n",
      "Epoch 002 | LR: 0.004513 | Train RMSE: 0.05623\n",
      "Epoch 003 | LR: 0.004287 | Train RMSE: 0.05616\n",
      "Epoch 004 | LR: 0.004073 | Train RMSE: 0.05615\n",
      "Epoch 005 | LR: 0.003869 | Train RMSE: 0.05612\n",
      "Epoch 006 | LR: 0.003675 | Train RMSE: 0.05621\n",
      "Epoch 007 | LR: 0.003492 | Train RMSE: 0.05612\n",
      "Epoch 008 | LR: 0.003317 | Train RMSE: 0.05617\n",
      "Epoch 009 | LR: 0.003151 | Train RMSE: 0.05614\n",
      "Epoch 010 | LR: 0.002994 | Train RMSE: 0.05607\n",
      "Epoch 011 | LR: 0.002844 | Train RMSE: 0.05609\n",
      "Epoch 012 | LR: 0.002702 | Train RMSE: 0.05608\n",
      "Epoch 013 | LR: 0.002567 | Train RMSE: 0.05608\n",
      "Epoch 014 | LR: 0.002438 | Train RMSE: 0.05611\n",
      "Epoch 015 | LR: 0.002316 | Train RMSE: 0.05604\n",
      "Epoch 016 | LR: 0.002201 | Train RMSE: 0.05607\n",
      "Epoch 017 | LR: 0.002091 | Train RMSE: 0.05602\n",
      "Epoch 018 | LR: 0.001986 | Train RMSE: 0.05602\n",
      "Epoch 019 | LR: 0.001887 | Train RMSE: 0.05600\n",
      "Epoch 020 | LR: 0.001792 | Train RMSE: 0.05606\n",
      "Epoch 021 | LR: 0.001703 | Train RMSE: 0.05598\n",
      "Epoch 022 | LR: 0.001618 | Train RMSE: 0.05603\n",
      "Epoch 023 | LR: 0.001537 | Train RMSE: 0.05600\n",
      "Epoch 024 | LR: 0.001460 | Train RMSE: 0.05599\n",
      "Epoch 025 | LR: 0.001387 | Train RMSE: 0.05598\n",
      "Epoch 026 | LR: 0.001318 | Train RMSE: 0.05598\n",
      "Epoch 027 | LR: 0.001252 | Train RMSE: 0.05599\n",
      "Epoch 028 | LR: 0.001189 | Train RMSE: 0.05597\n",
      "Epoch 029 | LR: 0.001130 | Train RMSE: 0.05597\n",
      "Epoch 030 | LR: 0.001073 | Train RMSE: 0.05596\n",
      "Epoch 031 | LR: 0.001020 | Train RMSE: 0.05597\n",
      "Epoch 032 | LR: 0.000969 | Train RMSE: 0.05596\n",
      "Epoch 033 | LR: 0.000920 | Train RMSE: 0.05596\n",
      "Epoch 034 | LR: 0.000874 | Train RMSE: 0.05595\n",
      "Epoch 035 | LR: 0.000830 | Train RMSE: 0.05595\n",
      "Epoch 036 | LR: 0.000789 | Train RMSE: 0.05596\n",
      "Epoch 037 | LR: 0.000749 | Train RMSE: 0.05594\n",
      "Epoch 038 | LR: 0.000712 | Train RMSE: 0.05593\n",
      "Epoch 039 | LR: 0.000676 | Train RMSE: 0.05594\n",
      "Epoch 040 | LR: 0.000643 | Train RMSE: 0.05593\n",
      "Epoch 041 | LR: 0.000610 | Train RMSE: 0.05592\n",
      "Epoch 042 | LR: 0.000580 | Train RMSE: 0.05593\n",
      "Epoch 043 | LR: 0.000551 | Train RMSE: 0.05592\n",
      "Epoch 044 | LR: 0.000523 | Train RMSE: 0.05592\n",
      "Epoch 045 | LR: 0.000497 | Train RMSE: 0.05592\n",
      "Epoch 046 | LR: 0.000472 | Train RMSE: 0.05592\n",
      "Epoch 047 | LR: 0.000449 | Train RMSE: 0.05592\n",
      "Epoch 048 | LR: 0.000426 | Train RMSE: 0.05591\n",
      "Epoch 049 | LR: 0.000405 | Train RMSE: 0.05591\n",
      "Epoch 050 | LR: 0.000385 | Train RMSE: 0.05591\n",
      "\n",
      "--- Training with seed 5464 ---\n",
      "Epoch 001 | LR: 0.004750 | Train RMSE: 0.08169\n",
      "Epoch 002 | LR: 0.004513 | Train RMSE: 0.05625\n",
      "Epoch 003 | LR: 0.004287 | Train RMSE: 0.05618\n",
      "Epoch 004 | LR: 0.004073 | Train RMSE: 0.05619\n",
      "Epoch 005 | LR: 0.003869 | Train RMSE: 0.05614\n",
      "Epoch 006 | LR: 0.003675 | Train RMSE: 0.05611\n",
      "Epoch 007 | LR: 0.003492 | Train RMSE: 0.05617\n",
      "Epoch 008 | LR: 0.003317 | Train RMSE: 0.05614\n",
      "Epoch 009 | LR: 0.003151 | Train RMSE: 0.05614\n",
      "Epoch 010 | LR: 0.002994 | Train RMSE: 0.05614\n",
      "Epoch 011 | LR: 0.002844 | Train RMSE: 0.05609\n",
      "Epoch 012 | LR: 0.002702 | Train RMSE: 0.05611\n",
      "Epoch 013 | LR: 0.002567 | Train RMSE: 0.05605\n",
      "Epoch 014 | LR: 0.002438 | Train RMSE: 0.05607\n",
      "Epoch 015 | LR: 0.002316 | Train RMSE: 0.05607\n",
      "Epoch 016 | LR: 0.002201 | Train RMSE: 0.05610\n",
      "Epoch 017 | LR: 0.002091 | Train RMSE: 0.05605\n",
      "Epoch 018 | LR: 0.001986 | Train RMSE: 0.05604\n",
      "Epoch 019 | LR: 0.001887 | Train RMSE: 0.05605\n",
      "Epoch 020 | LR: 0.001792 | Train RMSE: 0.05601\n",
      "Epoch 021 | LR: 0.001703 | Train RMSE: 0.05603\n",
      "Epoch 022 | LR: 0.001618 | Train RMSE: 0.05604\n",
      "Epoch 023 | LR: 0.001537 | Train RMSE: 0.05600\n",
      "Epoch 024 | LR: 0.001460 | Train RMSE: 0.05599\n",
      "Epoch 025 | LR: 0.001387 | Train RMSE: 0.05600\n",
      "Epoch 026 | LR: 0.001318 | Train RMSE: 0.05599\n",
      "Epoch 027 | LR: 0.001252 | Train RMSE: 0.05603\n",
      "Epoch 028 | LR: 0.001189 | Train RMSE: 0.05600\n",
      "Epoch 029 | LR: 0.001130 | Train RMSE: 0.05600\n",
      "Epoch 030 | LR: 0.001073 | Train RMSE: 0.05597\n",
      "Epoch 031 | LR: 0.001020 | Train RMSE: 0.05597\n",
      "Epoch 032 | LR: 0.000969 | Train RMSE: 0.05596\n",
      "Epoch 033 | LR: 0.000920 | Train RMSE: 0.05597\n",
      "Epoch 034 | LR: 0.000874 | Train RMSE: 0.05595\n",
      "Epoch 035 | LR: 0.000830 | Train RMSE: 0.05595\n",
      "Epoch 036 | LR: 0.000789 | Train RMSE: 0.05596\n",
      "Epoch 037 | LR: 0.000749 | Train RMSE: 0.05596\n",
      "Epoch 038 | LR: 0.000712 | Train RMSE: 0.05595\n",
      "Epoch 039 | LR: 0.000676 | Train RMSE: 0.05595\n",
      "Epoch 040 | LR: 0.000643 | Train RMSE: 0.05595\n",
      "Epoch 041 | LR: 0.000610 | Train RMSE: 0.05594\n",
      "Epoch 042 | LR: 0.000580 | Train RMSE: 0.05594\n",
      "Epoch 043 | LR: 0.000551 | Train RMSE: 0.05593\n",
      "Epoch 044 | LR: 0.000523 | Train RMSE: 0.05594\n",
      "Epoch 045 | LR: 0.000497 | Train RMSE: 0.05593\n",
      "Epoch 046 | LR: 0.000472 | Train RMSE: 0.05594\n",
      "Epoch 047 | LR: 0.000449 | Train RMSE: 0.05593\n",
      "Epoch 048 | LR: 0.000426 | Train RMSE: 0.05592\n",
      "Epoch 049 | LR: 0.000405 | Train RMSE: 0.05592\n",
      "Epoch 050 | LR: 0.000385 | Train RMSE: 0.05592\n",
      "\n",
      "Full Training RMSE: 0.05587\n",
      "\n",
      "Blended submission saved as 'submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Invoke main execution\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13760552,
     "sourceId": 91721,
     "sourceType": "competition"
    },
    {
     "datasetId": 8395648,
     "sourceId": 13477880,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 266082294,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266219643,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266226513,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266505672,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1980.239571,
   "end_time": "2025-10-26T21:09:54.741804",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-26T20:36:54.502233",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
