{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f2f9ce",
   "metadata": {
    "_cell_guid": "8e861b35-20fd-448d-b0f0-2d104793d624",
    "_uuid": "27a97ee3-1674-427b-b615-da99989fef3a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003597,
     "end_time": "2025-10-26T20:36:08.198996",
     "exception": false,
     "start_time": "2025-10-26T20:36:08.195399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ§© Inspired by [S5E10 | NN Stacking â€“ Baseline](https://www.kaggle.com/code/masayakawamata/s5e10-nn-stacking-baseline)\n",
    "> *Huge thanks to Masaya Kawamata for the inspiration behind this work.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17dec87",
   "metadata": {
    "_cell_guid": "43caa8e8-7fb7-4b07-a95d-71f64f650af5",
    "_uuid": "728b14db-b80a-4028-95b4-4ee2e8aefddc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:08.206525Z",
     "iopub.status.busy": "2025-10-26T20:36:08.205659Z",
     "iopub.status.idle": "2025-10-26T20:36:17.054331Z",
     "shell.execute_reply": "2025-10-26T20:36:17.053254Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8.854261,
     "end_time": "2025-10-26T20:36:17.056204",
     "exception": false,
     "start_time": "2025-10-26T20:36:08.201943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Import standard libraries\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Import third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define constants\n",
    "TARGET = 'accident_risk'\n",
    "BATCH_SIZE = 768\n",
    "MAX_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-3\n",
    "LR_DECAY = 0.900\n",
    "SEED_LIST = [9375, 1418, 2783, 8364, 5464, 6930, 3489, 4641, 122, 41, 54, 6, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a6dc31",
   "metadata": {
    "_cell_guid": "2494328f-8d45-4b65-87b8-e909b3c2a599",
    "_uuid": "f5ff6a05-a201-4431-9ee6-4e876e2ad77d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:17.063292Z",
     "iopub.status.busy": "2025-10-26T20:36:17.062880Z",
     "iopub.status.idle": "2025-10-26T20:36:17.067630Z",
     "shell.execute_reply": "2025-10-26T20:36:17.066752Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010078,
     "end_time": "2025-10-26T20:36:17.069240",
     "exception": false,
     "start_time": "2025-10-26T20:36:17.059162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define device selection\n",
    "def get_device():\n",
    "    # Choose CUDA if available, else CPU\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a5939a",
   "metadata": {
    "_cell_guid": "35a27c90-5119-4c8d-ac8f-b8acc18ffa67",
    "_uuid": "9171dd3b-23a0-4322-a984-76c50fc6a2b3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:17.076307Z",
     "iopub.status.busy": "2025-10-26T20:36:17.075790Z",
     "iopub.status.idle": "2025-10-26T20:36:17.081226Z",
     "shell.execute_reply": "2025-10-26T20:36:17.080351Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010391,
     "end_time": "2025-10-26T20:36:17.082601",
     "exception": false,
     "start_time": "2025-10-26T20:36:17.072210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define reproducibility setup\n",
    "def set_seed(seed):\n",
    "    # Set Python seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set NumPy seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set PyTorch CPU seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Set PyTorch CUDA seed if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Enable deterministic operations for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf7d970",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:17.089450Z",
     "iopub.status.busy": "2025-10-26T20:36:17.089153Z",
     "iopub.status.idle": "2025-10-26T20:36:17.094373Z",
     "shell.execute_reply": "2025-10-26T20:36:17.093493Z"
    },
    "papermill": {
     "duration": 0.010346,
     "end_time": "2025-10-26T20:36:17.095827",
     "exception": false,
     "start_time": "2025-10-26T20:36:17.085481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define feature engineering\n",
    "def add_engineered_features(df):\n",
    "    # Compute engineered feature\n",
    "    df['engineered_feature'] = (\n",
    "        0.3 * df[\"curvature\"] +\n",
    "        0.2 * (df[\"lighting\"] == \"night\").astype(int) +\n",
    "        0.1 * (df[\"weather\"] != \"clear\").astype(int) +\n",
    "        0.2 * (df[\"speed_limit\"] >= 60).astype(int) +\n",
    "        0.1 * (df[\"num_reported_accidents\"] > 2).astype(int)\n",
    "    )\n",
    "\n",
    "    # Return dataframe with new feature\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b70e443",
   "metadata": {
    "_cell_guid": "35743f5c-4a40-4b57-ae09-4e33247f9f0e",
    "_uuid": "4e061267-611b-49af-ad5e-dd61b5dc2be7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:17.102654Z",
     "iopub.status.busy": "2025-10-26T20:36:17.102365Z",
     "iopub.status.idle": "2025-10-26T20:36:17.107524Z",
     "shell.execute_reply": "2025-10-26T20:36:17.106712Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010306,
     "end_time": "2025-10-26T20:36:17.109067",
     "exception": false,
     "start_time": "2025-10-26T20:36:17.098761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define MLP meta-model\n",
    "class MetaMLP(nn.Module):\n",
    "    # Initialize layers\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb99dc4",
   "metadata": {
    "_cell_guid": "64c4b008-4448-4745-adef-ad8926093ee6",
    "_uuid": "564413dd-f949-4fa1-acc4-b7cf4d333408",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:17.116763Z",
     "iopub.status.busy": "2025-10-26T20:36:17.116408Z",
     "iopub.status.idle": "2025-10-26T20:36:17.125826Z",
     "shell.execute_reply": "2025-10-26T20:36:17.124871Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014933,
     "end_time": "2025-10-26T20:36:17.127301",
     "exception": false,
     "start_time": "2025-10-26T20:36:17.112368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training routine for one seed using full data\n",
    "def run_one_seed(X_train, y_train, X_test, device):\n",
    "    # Convert full training dataset to tensors\n",
    "    train_ds = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "\n",
    "    # Convert test dataset to tensor\n",
    "    test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    # Build training data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MetaMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Initialize exponential learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_DECAY)\n",
    "\n",
    "    # Initialize loss criterion\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize cumulative training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Iterate over batches in training data\n",
    "        for xb, yb in train_loader:\n",
    "            # Move batch to the selected device\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through model\n",
    "            preds = model(xb)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            # Backpropagate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate total training loss\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # Step the scheduler to decay learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute average training loss\n",
    "        train_loss /= len(train_ds)\n",
    "\n",
    "        # Compute training RMSE\n",
    "        train_rmse = np.sqrt(train_loss)\n",
    "\n",
    "        # Print epoch-level performance\n",
    "        print(f\"Epoch {epoch + 1:03d} | LR: {scheduler.get_last_lr()[0]:.6f} | Train RMSE: {train_rmse:.5f}\")\n",
    "\n",
    "    # Set model to evaluation mode for predictions\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation for prediction\n",
    "    with torch.no_grad():\n",
    "        # Generate training predictions\n",
    "        train_preds = model(\n",
    "            torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "        ).cpu().view(-1).numpy()\n",
    "\n",
    "        # Generate test predictions\n",
    "        test_preds = model(\n",
    "            test_tensor.to(device)\n",
    "        ).cpu().view(-1).numpy()\n",
    "\n",
    "    # Return training and test predictions\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9152e35",
   "metadata": {
    "_cell_guid": "31efa8f5-214a-46f8-aefe-850cb89a10ef",
    "_uuid": "a596cf45-83ef-4db3-b83d-53e5e3f670af",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:17.134610Z",
     "iopub.status.busy": "2025-10-26T20:36:17.134003Z",
     "iopub.status.idle": "2025-10-26T20:36:17.139831Z",
     "shell.execute_reply": "2025-10-26T20:36:17.138958Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01107,
     "end_time": "2025-10-26T20:36:17.141273",
     "exception": false,
     "start_time": "2025-10-26T20:36:17.130203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dataframe merge by ID\n",
    "def merge_dataframes_by_id(data_list, id_col='id', feature_col=TARGET):\n",
    "    # Select the first dataframe in the list\n",
    "    first = data_list[0]\n",
    "\n",
    "    # Rename the feature column of the first dataframe using its model name\n",
    "    merged = first['df'].rename(columns={feature_col: f\"{feature_col}_{first['name']}\"})\n",
    "\n",
    "    # Iterate over the remaining dataframes in the list\n",
    "    for data in data_list[1:]:\n",
    "        # Rename the feature column in the current dataframe\n",
    "        renamed = data['df'].rename(columns={feature_col: f\"{feature_col}_{data['name']}\"})\n",
    "\n",
    "        # Merge the renamed dataframe with the accumulated merged dataframe\n",
    "        merged = pd.merge(merged, renamed, on=id_col, how='outer')\n",
    "\n",
    "    # Return the final merged dataframe\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1adec0",
   "metadata": {
    "_cell_guid": "768fbac8-b245-48e3-a88d-ae9d3cde0fad",
    "_uuid": "3e5a4d38-92d3-4c7c-b046-851bf307bd56",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:17.148833Z",
     "iopub.status.busy": "2025-10-26T20:36:17.148421Z",
     "iopub.status.idle": "2025-10-26T20:36:17.162384Z",
     "shell.execute_reply": "2025-10-26T20:36:17.161132Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019807,
     "end_time": "2025-10-26T20:36:17.164146",
     "exception": false,
     "start_time": "2025-10-26T20:36:17.144339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the main execution\n",
    "def main():\n",
    "    # Load training data\n",
    "    train = pd.read_csv('/kaggle/input/playground-series-s5e10/train.csv')\n",
    "\n",
    "    # Load test data\n",
    "    test = pd.read_csv('/kaggle/input/playground-series-s5e10/test.csv')\n",
    "\n",
    "    # Discover all OOF (out-of-fold) prediction files\n",
    "    oof_files = glob.glob('/kaggle/input/**/oof_*.csv', recursive=True)\n",
    "\n",
    "    # Print the number of OOF files found\n",
    "    print(f\"Found {len(oof_files)} OOF files.\")\n",
    "\n",
    "    # Initialize containers for OOF and test data\n",
    "    all_oof_data = []\n",
    "    all_test_data = []\n",
    "\n",
    "    # Iterate through all OOF file paths\n",
    "    for oof_path in oof_files:\n",
    "        # Construct corresponding test file path\n",
    "        test_path = oof_path.replace('oof_', 'test_')\n",
    "\n",
    "        # Extract model name from file path\n",
    "        model_name = os.path.basename(oof_path).replace('oof_', '').replace('.csv', '')\n",
    "\n",
    "        # Load OOF data and store with model name\n",
    "        all_oof_data.append({'df': pd.read_csv(oof_path), 'name': model_name})\n",
    "\n",
    "        # Load test data and store with model name\n",
    "        all_test_data.append({'df': pd.read_csv(test_path), 'name': model_name})\n",
    "\n",
    "    # Merge OOF dataframes by ID\n",
    "    oof_df = merge_dataframes_by_id(all_oof_data)\n",
    "\n",
    "    # Merge test dataframes by ID\n",
    "    test_df = merge_dataframes_by_id(all_test_data)\n",
    "\n",
    "    # Attach ground truth target to OOF dataframe\n",
    "    oof_df[TARGET] = train[TARGET].values\n",
    "\n",
    "    # Merge in base train/test features to apply feature engineering\n",
    "    oof_df = pd.merge(oof_df, train[['id', 'curvature', 'lighting', 'weather', 'speed_limit', 'num_reported_accidents']], on='id', how='left')\n",
    "    test_df = pd.merge(test_df, test[['id', 'curvature', 'lighting', 'weather', 'speed_limit', 'num_reported_accidents']], on='id', how='left')\n",
    "\n",
    "    # Apply feature engineering\n",
    "    oof_df = add_engineered_features(oof_df)\n",
    "    test_df = add_engineered_features(test_df)\n",
    "\n",
    "    # Identify numerical feature columns\n",
    "    num_features = oof_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Exclude ID and target columns from features\n",
    "    FEATURES = [f for f in num_features if f not in ['id', TARGET]]\n",
    "\n",
    "    # Prepare feature matrix and target vector\n",
    "    X = oof_df[FEATURES]\n",
    "    y = oof_df[TARGET]\n",
    "\n",
    "    # Copy test feature matrix\n",
    "    X_test_full = test_df[FEATURES].copy()\n",
    "\n",
    "    # Initialize standard scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit scaler on full training data and transform all splits\n",
    "    X_train_scaled = scaler.fit_transform(X)\n",
    "    X_test_scaled = scaler.transform(X_test_full)\n",
    "\n",
    "    # Select computation device (CPU or GPU)\n",
    "    device = get_device()\n",
    "\n",
    "    # Initialize arrays for storing averaged predictions\n",
    "    train_pred_accum = np.zeros(len(X))\n",
    "    test_pred_accum = np.zeros(len(X_test_full))\n",
    "\n",
    "    # Loop through each random seed for model averaging\n",
    "    for seed in SEED_LIST:\n",
    "        # Set reproducible random seed\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Print current seed\n",
    "        print(f\"\\n--- Training with seed {seed} ---\")\n",
    "\n",
    "        # Train and predict with one seed\n",
    "        train_preds_seed, test_preds_seed = run_one_seed(\n",
    "            X_train_scaled,\n",
    "            y,\n",
    "            X_test_scaled,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # Accumulate averaged training predictions\n",
    "        train_pred_accum += train_preds_seed / len(SEED_LIST)\n",
    "\n",
    "        # Accumulate averaged test predictions\n",
    "        test_pred_accum += test_preds_seed / len(SEED_LIST)\n",
    "\n",
    "    # Compute RMSE on full training set\n",
    "    train_rmse = mean_squared_error(y, train_pred_accum, squared=False)\n",
    "\n",
    "    # Print overall training performance\n",
    "    print(f\"\\nFull Training RMSE: {train_rmse:.5f}\")\n",
    "\n",
    "    # Create dataframe for current model predictions\n",
    "    new_submission = pd.DataFrame({'id': test.id, TARGET: test_pred_accum})\n",
    "\n",
    "    # ======================================================\n",
    "    # Blend with existing saved submissions using weights\n",
    "    # ======================================================\n",
    "\n",
    "    # Define paths to other saved submissions\n",
    "    blend_files = [\n",
    "        '/kaggle/input/predicting-road-accident-risk-vault/autogluon15.csv',\n",
    "        '/kaggle/input/predicting-road-accident-risk-vault/submission.csv'\n",
    "    ]\n",
    "\n",
    "    # Define blending weights (must sum to 1 with new model)\n",
    "    blend_weights = [0.6, 0.375, 0.025]  # [modelA, modelB, new_model]\n",
    "\n",
    "    # Initialize blended submission with zeros\n",
    "    blended = pd.DataFrame({'id': test.id, TARGET: np.zeros(len(test))})\n",
    "\n",
    "    # Loop through saved submissions and blend\n",
    "    for path, weight in zip(blend_files, blend_weights[:-1]):\n",
    "        # Load existing submission\n",
    "        sub = pd.read_csv(path)\n",
    "\n",
    "        # Add weighted predictions\n",
    "        blended[TARGET] += sub[TARGET] * weight\n",
    "\n",
    "    # Add current model predictions with its weight\n",
    "    blended[TARGET] += new_submission[TARGET] * blend_weights[-1]\n",
    "\n",
    "    # Save final blended submission\n",
    "    blended.to_csv('submission.csv', index=False)\n",
    "\n",
    "    # Print confirmation\n",
    "    print(\"\\nBlended submission saved as 'submission.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d412ac6d",
   "metadata": {
    "_cell_guid": "10ab92fe-bb22-4ade-b20c-641131df3cff",
    "_uuid": "fc0a7dcf-b243-4bfc-a33e-5fdec90e690b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T20:36:17.172174Z",
     "iopub.status.busy": "2025-10-26T20:36:17.171868Z",
     "iopub.status.idle": "2025-10-26T21:47:08.015996Z",
     "shell.execute_reply": "2025-10-26T21:47:08.015011Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4250.851071,
     "end_time": "2025-10-26T21:47:08.018443",
     "exception": false,
     "start_time": "2025-10-26T20:36:17.167372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 OOF files.\n",
      "\n",
      "--- Training with seed 9375 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.06016\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05631\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05622\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05619\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05619\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05611\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05611\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05610\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05605\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05607\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05603\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05601\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05601\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05599\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05600\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05599\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05597\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05596\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05595\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05595\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05594\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05594\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05593\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05593\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05592\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05592\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05591\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05591\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05591\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05590\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05590\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05589\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05589\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05589\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05589\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05588\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05588\n",
      "\n",
      "--- Training with seed 1418 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.07465\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05625\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05628\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05621\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05621\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05621\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05614\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05610\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05613\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05611\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05606\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05605\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05604\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05602\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05603\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05601\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05602\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05599\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05597\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05597\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05595\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05596\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05595\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05594\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05594\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05593\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05592\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05593\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05592\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05592\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05591\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05591\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05591\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05590\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05590\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05590\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05590\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05590\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05589\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05589\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05589\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05589\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05589\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05589\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05589\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05588\n",
      "\n",
      "--- Training with seed 2783 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.05979\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05623\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05616\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05613\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05612\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05610\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05607\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05605\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05605\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05606\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05602\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05602\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05599\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05598\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05598\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05598\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05595\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05594\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05595\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05594\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05593\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05593\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05592\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05592\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05591\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05591\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05590\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05590\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05590\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05589\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05589\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05589\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05589\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05589\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05588\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05588\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05588\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05588\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05587\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05587\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05587\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05587\n",
      "\n",
      "--- Training with seed 8364 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.06390\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05631\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05625\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05619\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05622\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05615\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05613\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05612\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05608\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05608\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05607\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05604\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05601\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05601\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05600\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05598\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05596\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05596\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05596\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05596\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05594\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05595\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05593\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05593\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05592\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05592\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05592\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05591\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05590\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05590\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05590\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05590\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05589\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05589\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05589\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05589\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05588\n",
      "\n",
      "--- Training with seed 5464 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.07028\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05634\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05627\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05630\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05620\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05619\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05616\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05613\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05612\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05608\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05605\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05603\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05602\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05602\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05601\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05600\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05597\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05598\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05598\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05596\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05595\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05596\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05594\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05593\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05593\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05593\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05592\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05592\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05591\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05590\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05590\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05589\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05589\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05589\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05589\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05589\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05589\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05588\n",
      "\n",
      "--- Training with seed 6930 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.05972\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05626\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05628\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05617\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05618\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05614\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05610\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05606\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05604\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05604\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05603\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05602\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05601\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05599\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05600\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05597\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05596\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05595\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05596\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05594\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05594\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05594\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05593\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05593\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05591\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05591\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05590\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05590\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05590\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05589\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05589\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05589\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05589\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05589\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05588\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05588\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05587\n",
      "\n",
      "--- Training with seed 3489 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.06005\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05623\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05620\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05620\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05616\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05613\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05609\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05608\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05609\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05603\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05603\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05602\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05598\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05598\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05597\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05597\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05595\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05596\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05595\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05593\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05594\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05593\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05593\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05592\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05592\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05591\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05591\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05591\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05590\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05590\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05589\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05589\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05589\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05589\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05588\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05588\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05588\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05588\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05587\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05587\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05587\n",
      "\n",
      "--- Training with seed 4641 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.05801\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05626\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05622\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05619\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05614\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05609\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05607\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05605\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05606\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05601\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05599\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05599\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05598\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05597\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05597\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05595\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05595\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05595\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05594\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05594\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05592\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05592\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05592\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05592\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05592\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05591\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05590\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05591\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05590\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05590\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05589\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05589\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05589\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05589\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05588\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05588\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05588\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05588\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05588\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05587\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05587\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05587\n",
      "\n",
      "--- Training with seed 122 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.06072\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05626\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05626\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05619\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05619\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05614\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05609\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05610\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05605\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05603\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05605\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05604\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05599\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05599\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05599\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05598\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05596\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05596\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05595\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05595\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05594\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05594\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05593\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05593\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05592\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05591\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05591\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05591\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05591\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05590\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05589\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05589\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05589\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05589\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05589\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05588\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05588\n",
      "\n",
      "--- Training with seed 41 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.06170\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05623\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05625\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05617\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05616\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05616\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05613\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05607\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05608\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05605\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05605\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05603\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05602\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05599\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05600\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05599\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05597\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05597\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05595\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05596\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05595\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05594\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05593\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05594\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05593\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05592\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05591\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05591\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05591\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05590\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05590\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05590\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05589\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05589\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05589\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05589\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05589\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05588\n",
      "\n",
      "--- Training with seed 54 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.06626\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05638\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05630\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05626\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05614\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05618\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05614\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05611\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05610\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05608\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05605\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05606\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05602\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05602\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05600\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05598\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05599\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05599\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05597\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05596\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05595\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05596\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05595\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05593\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05593\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05592\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05592\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05592\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05591\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05590\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05590\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05590\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05589\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05589\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05589\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05589\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05589\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05588\n",
      "\n",
      "--- Training with seed 6 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.06189\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05620\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05614\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05615\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05612\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05610\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05612\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05608\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05608\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05605\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05604\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05603\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05601\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05600\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05600\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05599\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05596\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05596\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05594\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05594\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05594\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05594\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05593\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05592\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05592\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05591\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05591\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05591\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05590\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05590\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05589\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05589\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05589\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05589\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05588\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05588\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05588\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05588\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05587\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05587\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05587\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05587\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05587\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05587\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05587\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05587\n",
      "\n",
      "--- Training with seed 42 ---\n",
      "Epoch 001 | LR: 0.004500 | Train RMSE: 0.06425\n",
      "Epoch 002 | LR: 0.004050 | Train RMSE: 0.05637\n",
      "Epoch 003 | LR: 0.003645 | Train RMSE: 0.05630\n",
      "Epoch 004 | LR: 0.003281 | Train RMSE: 0.05624\n",
      "Epoch 005 | LR: 0.002952 | Train RMSE: 0.05619\n",
      "Epoch 006 | LR: 0.002657 | Train RMSE: 0.05618\n",
      "Epoch 007 | LR: 0.002391 | Train RMSE: 0.05611\n",
      "Epoch 008 | LR: 0.002152 | Train RMSE: 0.05619\n",
      "Epoch 009 | LR: 0.001937 | Train RMSE: 0.05606\n",
      "Epoch 010 | LR: 0.001743 | Train RMSE: 0.05608\n",
      "Epoch 011 | LR: 0.001569 | Train RMSE: 0.05607\n",
      "Epoch 012 | LR: 0.001412 | Train RMSE: 0.05605\n",
      "Epoch 013 | LR: 0.001271 | Train RMSE: 0.05603\n",
      "Epoch 014 | LR: 0.001144 | Train RMSE: 0.05601\n",
      "Epoch 015 | LR: 0.001029 | Train RMSE: 0.05601\n",
      "Epoch 016 | LR: 0.000927 | Train RMSE: 0.05599\n",
      "Epoch 017 | LR: 0.000834 | Train RMSE: 0.05597\n",
      "Epoch 018 | LR: 0.000750 | Train RMSE: 0.05597\n",
      "Epoch 019 | LR: 0.000675 | Train RMSE: 0.05597\n",
      "Epoch 020 | LR: 0.000608 | Train RMSE: 0.05597\n",
      "Epoch 021 | LR: 0.000547 | Train RMSE: 0.05595\n",
      "Epoch 022 | LR: 0.000492 | Train RMSE: 0.05594\n",
      "Epoch 023 | LR: 0.000443 | Train RMSE: 0.05593\n",
      "Epoch 024 | LR: 0.000399 | Train RMSE: 0.05593\n",
      "Epoch 025 | LR: 0.000359 | Train RMSE: 0.05593\n",
      "Epoch 026 | LR: 0.000323 | Train RMSE: 0.05592\n",
      "Epoch 027 | LR: 0.000291 | Train RMSE: 0.05592\n",
      "Epoch 028 | LR: 0.000262 | Train RMSE: 0.05591\n",
      "Epoch 029 | LR: 0.000236 | Train RMSE: 0.05591\n",
      "Epoch 030 | LR: 0.000212 | Train RMSE: 0.05591\n",
      "Epoch 031 | LR: 0.000191 | Train RMSE: 0.05590\n",
      "Epoch 032 | LR: 0.000172 | Train RMSE: 0.05590\n",
      "Epoch 033 | LR: 0.000155 | Train RMSE: 0.05590\n",
      "Epoch 034 | LR: 0.000139 | Train RMSE: 0.05590\n",
      "Epoch 035 | LR: 0.000125 | Train RMSE: 0.05589\n",
      "Epoch 036 | LR: 0.000113 | Train RMSE: 0.05589\n",
      "Epoch 037 | LR: 0.000101 | Train RMSE: 0.05589\n",
      "Epoch 038 | LR: 0.000091 | Train RMSE: 0.05589\n",
      "Epoch 039 | LR: 0.000082 | Train RMSE: 0.05588\n",
      "Epoch 040 | LR: 0.000074 | Train RMSE: 0.05588\n",
      "Epoch 041 | LR: 0.000067 | Train RMSE: 0.05588\n",
      "Epoch 042 | LR: 0.000060 | Train RMSE: 0.05588\n",
      "Epoch 043 | LR: 0.000054 | Train RMSE: 0.05588\n",
      "Epoch 044 | LR: 0.000048 | Train RMSE: 0.05588\n",
      "Epoch 045 | LR: 0.000044 | Train RMSE: 0.05588\n",
      "Epoch 046 | LR: 0.000039 | Train RMSE: 0.05588\n",
      "Epoch 047 | LR: 0.000035 | Train RMSE: 0.05588\n",
      "Epoch 048 | LR: 0.000032 | Train RMSE: 0.05588\n",
      "Epoch 049 | LR: 0.000029 | Train RMSE: 0.05588\n",
      "Epoch 050 | LR: 0.000026 | Train RMSE: 0.05587\n",
      "\n",
      "Full Training RMSE: 0.05587\n",
      "\n",
      "Blended submission saved as 'submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Invoke main execution\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13760552,
     "sourceId": 91721,
     "sourceType": "competition"
    },
    {
     "datasetId": 8395648,
     "sourceId": 13477880,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 266082294,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266219643,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266226513,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266505672,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4267.319992,
   "end_time": "2025-10-26T21:47:10.490516",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-26T20:36:03.170524",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
