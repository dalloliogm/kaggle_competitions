{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10f72a0",
   "metadata": {
    "_cell_guid": "8e861b35-20fd-448d-b0f0-2d104793d624",
    "_uuid": "27a97ee3-1674-427b-b615-da99989fef3a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004175,
     "end_time": "2025-10-26T16:01:53.438878",
     "exception": false,
     "start_time": "2025-10-26T16:01:53.434703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ§© Inspired by [S5E10 | NN Stacking â€“ Baseline](https://www.kaggle.com/code/masayakawamata/s5e10-nn-stacking-baseline)\n",
    "> *Huge thanks to Masaya Kawamata for the inspiration behind this work.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77608753",
   "metadata": {
    "_cell_guid": "43caa8e8-7fb7-4b07-a95d-71f64f650af5",
    "_uuid": "728b14db-b80a-4028-95b4-4ee2e8aefddc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T16:01:53.446512Z",
     "iopub.status.busy": "2025-10-26T16:01:53.446202Z",
     "iopub.status.idle": "2025-10-26T16:02:03.805246Z",
     "shell.execute_reply": "2025-10-26T16:02:03.804066Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 10.365458,
     "end_time": "2025-10-26T16:02:03.807521",
     "exception": false,
     "start_time": "2025-10-26T16:01:53.442063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Import standard libraries\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Import third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define constants\n",
    "TARGET = 'accident_risk'\n",
    "BATCH_SIZE = 768\n",
    "MAX_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "LR_DECAY = 0.900\n",
    "SEED_LIST = [9375, 1418, 2783, 8364, 5464, 6930, 3489, 4641, 122, 41, 54, 6, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c96309",
   "metadata": {
    "_cell_guid": "2494328f-8d45-4b65-87b8-e909b3c2a599",
    "_uuid": "f5ff6a05-a201-4431-9ee6-4e876e2ad77d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:03.816687Z",
     "iopub.status.busy": "2025-10-26T16:02:03.816134Z",
     "iopub.status.idle": "2025-10-26T16:02:03.822475Z",
     "shell.execute_reply": "2025-10-26T16:02:03.821239Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01349,
     "end_time": "2025-10-26T16:02:03.824968",
     "exception": false,
     "start_time": "2025-10-26T16:02:03.811478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define device selection\n",
    "def get_device():\n",
    "    # Choose CUDA if available, else CPU\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7a7e3a",
   "metadata": {
    "_cell_guid": "35a27c90-5119-4c8d-ac8f-b8acc18ffa67",
    "_uuid": "9171dd3b-23a0-4322-a984-76c50fc6a2b3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:03.833517Z",
     "iopub.status.busy": "2025-10-26T16:02:03.833139Z",
     "iopub.status.idle": "2025-10-26T16:02:03.840044Z",
     "shell.execute_reply": "2025-10-26T16:02:03.838938Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013789,
     "end_time": "2025-10-26T16:02:03.842212",
     "exception": false,
     "start_time": "2025-10-26T16:02:03.828423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define reproducibility setup\n",
    "def set_seed(seed):\n",
    "    # Set Python seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set NumPy seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set PyTorch CPU seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Set PyTorch CUDA seed if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Enable deterministic operations for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe5cb4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:03.850197Z",
     "iopub.status.busy": "2025-10-26T16:02:03.849825Z",
     "iopub.status.idle": "2025-10-26T16:02:03.856230Z",
     "shell.execute_reply": "2025-10-26T16:02:03.854633Z"
    },
    "papermill": {
     "duration": 0.012645,
     "end_time": "2025-10-26T16:02:03.858096",
     "exception": false,
     "start_time": "2025-10-26T16:02:03.845451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define feature engineering\n",
    "def add_engineered_features(df):\n",
    "    # Compute engineered feature\n",
    "    df['engineered_feature'] = (\n",
    "        0.3 * df[\"curvature\"] +\n",
    "        0.2 * (df[\"lighting\"] == \"night\").astype(int) +\n",
    "        0.1 * (df[\"weather\"] != \"clear\").astype(int) +\n",
    "        0.2 * (df[\"speed_limit\"] >= 60).astype(int) +\n",
    "        0.1 * (df[\"num_reported_accidents\"] > 2).astype(int)\n",
    "    )\n",
    "\n",
    "    # Return dataframe with new feature\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a9efdf",
   "metadata": {
    "_cell_guid": "35743f5c-4a40-4b57-ae09-4e33247f9f0e",
    "_uuid": "4e061267-611b-49af-ad5e-dd61b5dc2be7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:03.866128Z",
     "iopub.status.busy": "2025-10-26T16:02:03.865758Z",
     "iopub.status.idle": "2025-10-26T16:02:03.872975Z",
     "shell.execute_reply": "2025-10-26T16:02:03.871645Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013646,
     "end_time": "2025-10-26T16:02:03.874965",
     "exception": false,
     "start_time": "2025-10-26T16:02:03.861319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define MLP meta-model\n",
    "class MetaMLP(nn.Module):\n",
    "    # Initialize layers\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe3576e",
   "metadata": {
    "_cell_guid": "64c4b008-4448-4745-adef-ad8926093ee6",
    "_uuid": "564413dd-f949-4fa1-acc4-b7cf4d333408",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:03.883131Z",
     "iopub.status.busy": "2025-10-26T16:02:03.882788Z",
     "iopub.status.idle": "2025-10-26T16:02:03.893908Z",
     "shell.execute_reply": "2025-10-26T16:02:03.892652Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017429,
     "end_time": "2025-10-26T16:02:03.895865",
     "exception": false,
     "start_time": "2025-10-26T16:02:03.878436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training routine for one seed using full data\n",
    "def run_one_seed(X_train, y_train, X_test, device):\n",
    "    # Convert full training dataset to tensors\n",
    "    train_ds = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "\n",
    "    # Convert test dataset to tensor\n",
    "    test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    # Build training data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MetaMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Initialize exponential learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_DECAY)\n",
    "\n",
    "    # Initialize loss criterion\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize cumulative training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Iterate over batches in training data\n",
    "        for xb, yb in train_loader:\n",
    "            # Move batch to the selected device\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through model\n",
    "            preds = model(xb)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            # Backpropagate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate total training loss\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # Step the scheduler to decay learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute average training loss\n",
    "        train_loss /= len(train_ds)\n",
    "\n",
    "        # Compute training RMSE\n",
    "        train_rmse = np.sqrt(train_loss)\n",
    "\n",
    "        # Print epoch-level performance\n",
    "        print(f\"Epoch {epoch + 1:03d} | LR: {scheduler.get_last_lr()[0]:.6f} | Train RMSE: {train_rmse:.5f}\")\n",
    "\n",
    "    # Set model to evaluation mode for predictions\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation for prediction\n",
    "    with torch.no_grad():\n",
    "        # Generate training predictions\n",
    "        train_preds = model(\n",
    "            torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "        ).cpu().view(-1).numpy()\n",
    "\n",
    "        # Generate test predictions\n",
    "        test_preds = model(\n",
    "            test_tensor.to(device)\n",
    "        ).cpu().view(-1).numpy()\n",
    "\n",
    "    # Return training and test predictions\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c8f928",
   "metadata": {
    "_cell_guid": "31efa8f5-214a-46f8-aefe-850cb89a10ef",
    "_uuid": "a596cf45-83ef-4db3-b83d-53e5e3f670af",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:03.904410Z",
     "iopub.status.busy": "2025-10-26T16:02:03.904055Z",
     "iopub.status.idle": "2025-10-26T16:02:03.910957Z",
     "shell.execute_reply": "2025-10-26T16:02:03.909777Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013464,
     "end_time": "2025-10-26T16:02:03.912902",
     "exception": false,
     "start_time": "2025-10-26T16:02:03.899438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dataframe merge by ID\n",
    "def merge_dataframes_by_id(data_list, id_col='id', feature_col=TARGET):\n",
    "    # Select the first dataframe in the list\n",
    "    first = data_list[0]\n",
    "\n",
    "    # Rename the feature column of the first dataframe using its model name\n",
    "    merged = first['df'].rename(columns={feature_col: f\"{feature_col}_{first['name']}\"})\n",
    "\n",
    "    # Iterate over the remaining dataframes in the list\n",
    "    for data in data_list[1:]:\n",
    "        # Rename the feature column in the current dataframe\n",
    "        renamed = data['df'].rename(columns={feature_col: f\"{feature_col}_{data['name']}\"})\n",
    "\n",
    "        # Merge the renamed dataframe with the accumulated merged dataframe\n",
    "        merged = pd.merge(merged, renamed, on=id_col, how='outer')\n",
    "\n",
    "    # Return the final merged dataframe\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f212ba",
   "metadata": {
    "_cell_guid": "768fbac8-b245-48e3-a88d-ae9d3cde0fad",
    "_uuid": "3e5a4d38-92d3-4c7c-b046-851bf307bd56",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:03.922544Z",
     "iopub.status.busy": "2025-10-26T16:02:03.922176Z",
     "iopub.status.idle": "2025-10-26T16:02:03.938220Z",
     "shell.execute_reply": "2025-10-26T16:02:03.937002Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022497,
     "end_time": "2025-10-26T16:02:03.940077",
     "exception": false,
     "start_time": "2025-10-26T16:02:03.917580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the main execution\n",
    "def main():\n",
    "    # Load training data\n",
    "    train = pd.read_csv('/kaggle/input/playground-series-s5e10/train.csv')\n",
    "\n",
    "    # Load test data\n",
    "    test = pd.read_csv('/kaggle/input/playground-series-s5e10/test.csv')\n",
    "\n",
    "    # Discover all OOF (out-of-fold) prediction files\n",
    "    oof_files = glob.glob('/kaggle/input/**/oof_*.csv', recursive=True)\n",
    "\n",
    "    # Print the number of OOF files found\n",
    "    print(f\"Found {len(oof_files)} OOF files.\")\n",
    "\n",
    "    # Initialize containers for OOF and test data\n",
    "    all_oof_data = []\n",
    "    all_test_data = []\n",
    "\n",
    "    # Iterate through all OOF file paths\n",
    "    for oof_path in oof_files:\n",
    "        # Construct corresponding test file path\n",
    "        test_path = oof_path.replace('oof_', 'test_')\n",
    "\n",
    "        # Extract model name from file path\n",
    "        model_name = os.path.basename(oof_path).replace('oof_', '').replace('.csv', '')\n",
    "\n",
    "        # Load OOF data and store with model name\n",
    "        all_oof_data.append({'df': pd.read_csv(oof_path), 'name': model_name})\n",
    "\n",
    "        # Load test data and store with model name\n",
    "        all_test_data.append({'df': pd.read_csv(test_path), 'name': model_name})\n",
    "\n",
    "    # Merge OOF dataframes by ID\n",
    "    oof_df = merge_dataframes_by_id(all_oof_data)\n",
    "\n",
    "    # Merge test dataframes by ID\n",
    "    test_df = merge_dataframes_by_id(all_test_data)\n",
    "\n",
    "    # Attach ground truth target to OOF dataframe\n",
    "    oof_df[TARGET] = train[TARGET].values\n",
    "\n",
    "    # Merge in base train/test features to apply feature engineering\n",
    "    oof_df = pd.merge(oof_df, train[['id', 'curvature', 'lighting', 'weather', 'speed_limit', 'num_reported_accidents']], on='id', how='left')\n",
    "    test_df = pd.merge(test_df, test[['id', 'curvature', 'lighting', 'weather', 'speed_limit', 'num_reported_accidents']], on='id', how='left')\n",
    "\n",
    "    # Apply feature engineering\n",
    "    oof_df = add_engineered_features(oof_df)\n",
    "    test_df = add_engineered_features(test_df)\n",
    "\n",
    "    # Identify numerical feature columns\n",
    "    num_features = oof_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Exclude ID and target columns from features\n",
    "    FEATURES = [f for f in num_features if f not in ['id', TARGET]]\n",
    "\n",
    "    # Prepare feature matrix and target vector\n",
    "    X = oof_df[FEATURES]\n",
    "    y = oof_df[TARGET]\n",
    "\n",
    "    # Copy test feature matrix\n",
    "    X_test_full = test_df[FEATURES].copy()\n",
    "\n",
    "    # Initialize standard scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit scaler on full training data and transform all splits\n",
    "    X_train_scaled = scaler.fit_transform(X)\n",
    "    X_test_scaled = scaler.transform(X_test_full)\n",
    "\n",
    "    # Select computation device (CPU or GPU)\n",
    "    device = get_device()\n",
    "\n",
    "    # Initialize arrays for storing averaged predictions\n",
    "    train_pred_accum = np.zeros(len(X))\n",
    "    test_pred_accum = np.zeros(len(X_test_full))\n",
    "\n",
    "    # Loop through each random seed for model averaging\n",
    "    for seed in SEED_LIST:\n",
    "        # Set reproducible random seed\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Print current seed\n",
    "        print(f\"\\n--- Training with seed {seed} ---\")\n",
    "\n",
    "        # Train and predict with one seed\n",
    "        train_preds_seed, test_preds_seed = run_one_seed(\n",
    "            X_train_scaled,\n",
    "            y,\n",
    "            X_test_scaled,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # Accumulate averaged training predictions\n",
    "        train_pred_accum += train_preds_seed / len(SEED_LIST)\n",
    "\n",
    "        # Accumulate averaged test predictions\n",
    "        test_pred_accum += test_preds_seed / len(SEED_LIST)\n",
    "\n",
    "    # Compute RMSE on full training set\n",
    "    train_rmse = mean_squared_error(y, train_pred_accum, squared=False)\n",
    "\n",
    "    # Print overall training performance\n",
    "    print(f\"\\nFull Training RMSE: {train_rmse:.5f}\")\n",
    "\n",
    "    # Create dataframe for current model predictions\n",
    "    new_submission = pd.DataFrame({'id': test.id, TARGET: test_pred_accum})\n",
    "\n",
    "    # ======================================================\n",
    "    # Blend with existing saved submissions using weights\n",
    "    # ======================================================\n",
    "\n",
    "    # Define paths to other saved submissions\n",
    "    blend_files = [\n",
    "        '/kaggle/input/predicting-road-accident-risk-vault/autogluon15.csv',\n",
    "        '/kaggle/input/predicting-road-accident-risk-vault/submission.csv'\n",
    "    ]\n",
    "\n",
    "    # Define blending weights (must sum to 1 with new model)\n",
    "    blend_weights = [0.6, 0.375, 0.025]  # [modelA, modelB, new_model]\n",
    "\n",
    "    # Initialize blended submission with zeros\n",
    "    blended = pd.DataFrame({'id': test.id, TARGET: np.zeros(len(test))})\n",
    "\n",
    "    # Loop through saved submissions and blend\n",
    "    for path, weight in zip(blend_files, blend_weights[:-1]):\n",
    "        # Load existing submission\n",
    "        sub = pd.read_csv(path)\n",
    "\n",
    "        # Add weighted predictions\n",
    "        blended[TARGET] += sub[TARGET] * weight\n",
    "\n",
    "    # Add current model predictions with its weight\n",
    "    blended[TARGET] += new_submission[TARGET] * blend_weights[-1]\n",
    "\n",
    "    # Save final blended submission\n",
    "    blended.to_csv('submission.csv', index=False)\n",
    "\n",
    "    # Print confirmation\n",
    "    print(\"\\nBlended submission saved as 'submission.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8f98ba",
   "metadata": {
    "_cell_guid": "10ab92fe-bb22-4ade-b20c-641131df3cff",
    "_uuid": "fc0a7dcf-b243-4bfc-a33e-5fdec90e690b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:03.947814Z",
     "iopub.status.busy": "2025-10-26T16:02:03.947442Z",
     "iopub.status.idle": "2025-10-26T17:22:40.538463Z",
     "shell.execute_reply": "2025-10-26T17:22:40.536785Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4836.597125,
     "end_time": "2025-10-26T17:22:40.540468",
     "exception": false,
     "start_time": "2025-10-26T16:02:03.943343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 OOF files.\n",
      "\n",
      "--- Training with seed 9375 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.13029\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.06077\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.05772\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.05693\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05660\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05644\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05635\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05629\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05625\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05621\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05619\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05617\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05615\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05614\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05613\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05612\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05611\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05610\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05610\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05609\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05609\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05608\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05608\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05608\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05608\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05607\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05607\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05607\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05607\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05607\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05607\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05606\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05606\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05606\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05606\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05606\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05606\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05606\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05606\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05606\n",
      "\n",
      "--- Training with seed 1418 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.33385\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.11418\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.08666\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.07484\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.06621\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.06121\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05859\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05758\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05716\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05694\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05679\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05668\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05660\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05653\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05647\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05643\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05640\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05637\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05635\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05633\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05631\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05630\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05629\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05628\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05628\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05627\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05626\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05626\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05625\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05625\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05625\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05624\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05624\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05624\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05624\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05623\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05623\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05623\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05623\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05623\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05623\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05623\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05622\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05622\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05622\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05622\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05622\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05622\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05622\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05622\n",
      "\n",
      "--- Training with seed 2783 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.13012\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.05915\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.05724\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.05672\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05648\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05635\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05628\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05623\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05619\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05617\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05614\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05613\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05611\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05610\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05609\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05609\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05608\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05607\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05607\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05606\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05606\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05606\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05605\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05605\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05605\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05605\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05605\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05604\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "\n",
      "--- Training with seed 8364 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.20028\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.08061\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.06645\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.06007\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05781\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05705\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05673\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05656\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05645\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05638\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05633\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05629\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05626\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05624\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05622\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05620\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05619\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05618\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05617\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05616\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05615\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05615\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05614\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05614\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05613\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05613\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05613\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05613\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05612\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05612\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05612\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05612\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05612\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05611\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05611\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05611\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05611\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05611\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05611\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05611\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05611\n",
      "\n",
      "--- Training with seed 5464 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.27692\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.10950\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.07404\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.06157\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05837\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05745\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05707\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05686\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05671\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05662\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05654\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05648\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05644\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05640\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05637\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05635\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05633\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05631\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05629\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05628\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05627\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05626\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05625\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05625\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05624\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05623\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05623\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05622\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05622\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05622\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05622\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05621\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05621\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05621\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05621\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05621\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05620\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05620\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05620\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05620\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05620\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05620\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05620\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05620\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05620\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05620\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05620\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05620\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05620\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05619\n",
      "\n",
      "--- Training with seed 6930 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.13184\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.06242\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.05806\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.05728\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05692\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05670\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05656\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05646\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05639\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05633\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05629\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05626\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05623\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05621\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05619\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05618\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05617\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05616\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05615\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05614\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05613\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05613\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05612\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05612\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05612\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05611\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05611\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05611\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05610\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05610\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05610\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05610\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05610\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05609\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05609\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05609\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05609\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05609\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05609\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05609\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "\n",
      "--- Training with seed 3489 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.13821\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.05845\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.05710\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.05670\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05651\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05640\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05632\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05626\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05623\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05619\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05617\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05615\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05614\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05612\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05611\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05610\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05609\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05608\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05608\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05607\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05607\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05606\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05606\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05606\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05605\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05605\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05605\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05605\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05603\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05603\n",
      "\n",
      "--- Training with seed 4641 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.09141\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.05892\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.05716\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.05669\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05648\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05636\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05629\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05624\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05620\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05617\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05616\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05614\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05612\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05611\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05610\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05609\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05609\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05608\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05607\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05607\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05606\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05606\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05606\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05606\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05605\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05605\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05605\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05605\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05605\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05604\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05604\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05604\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05604\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05604\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05604\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05604\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05604\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05604\n",
      "\n",
      "--- Training with seed 122 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.15136\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.06578\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.05925\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.05751\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05696\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05670\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05655\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05644\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05637\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05631\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05627\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05623\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05620\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05618\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05617\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05615\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05614\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05613\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05612\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05611\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05611\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05610\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05610\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05610\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05609\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05609\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05609\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05608\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05608\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05608\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05608\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05608\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05608\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05607\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05607\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05607\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05607\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05607\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05607\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05607\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05607\n",
      "\n",
      "--- Training with seed 41 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.17721\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.06476\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.05985\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.05797\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05717\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05679\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05658\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05645\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05636\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05629\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05624\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05620\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05618\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05616\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05614\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05613\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05612\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05611\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05610\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05609\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05609\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05609\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05608\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05608\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05607\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05607\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05607\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05607\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05607\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05606\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05606\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05606\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05606\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05606\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05605\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05605\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05605\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05605\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05605\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05605\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05605\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05605\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05605\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05605\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05605\n",
      "\n",
      "--- Training with seed 54 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.23474\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.09599\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.06619\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.06040\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05880\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05809\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05766\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05736\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05713\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05695\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05682\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05671\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05663\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05656\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05651\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05647\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05643\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05640\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05638\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05636\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05634\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05633\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05632\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05631\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05630\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05629\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05628\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05628\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05627\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05627\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05626\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05626\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05626\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05626\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05625\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05625\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05625\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05625\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05625\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05624\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05624\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05624\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05624\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05624\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05624\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05624\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05624\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05624\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05624\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05624\n",
      "\n",
      "--- Training with seed 6 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.16158\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.05903\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.05752\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.05698\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05671\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05657\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05647\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05640\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05635\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05631\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05628\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05625\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05623\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05621\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05619\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05618\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05617\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05616\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05615\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05614\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05613\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05613\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05613\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05612\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05612\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05612\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05611\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05611\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05611\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05611\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05610\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05610\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05610\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05610\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05609\n",
      "\n",
      "--- Training with seed 42 ---\n",
      "Epoch 001 | LR: 0.000045 | Train RMSE: 0.20323\n",
      "Epoch 002 | LR: 0.000041 | Train RMSE: 0.08151\n",
      "Epoch 003 | LR: 0.000036 | Train RMSE: 0.06616\n",
      "Epoch 004 | LR: 0.000033 | Train RMSE: 0.06048\n",
      "Epoch 005 | LR: 0.000030 | Train RMSE: 0.05829\n",
      "Epoch 006 | LR: 0.000027 | Train RMSE: 0.05740\n",
      "Epoch 007 | LR: 0.000024 | Train RMSE: 0.05698\n",
      "Epoch 008 | LR: 0.000022 | Train RMSE: 0.05675\n",
      "Epoch 009 | LR: 0.000019 | Train RMSE: 0.05660\n",
      "Epoch 010 | LR: 0.000017 | Train RMSE: 0.05650\n",
      "Epoch 011 | LR: 0.000016 | Train RMSE: 0.05643\n",
      "Epoch 012 | LR: 0.000014 | Train RMSE: 0.05638\n",
      "Epoch 013 | LR: 0.000013 | Train RMSE: 0.05633\n",
      "Epoch 014 | LR: 0.000011 | Train RMSE: 0.05630\n",
      "Epoch 015 | LR: 0.000010 | Train RMSE: 0.05628\n",
      "Epoch 016 | LR: 0.000009 | Train RMSE: 0.05626\n",
      "Epoch 017 | LR: 0.000008 | Train RMSE: 0.05624\n",
      "Epoch 018 | LR: 0.000008 | Train RMSE: 0.05622\n",
      "Epoch 019 | LR: 0.000007 | Train RMSE: 0.05621\n",
      "Epoch 020 | LR: 0.000006 | Train RMSE: 0.05620\n",
      "Epoch 021 | LR: 0.000005 | Train RMSE: 0.05619\n",
      "Epoch 022 | LR: 0.000005 | Train RMSE: 0.05619\n",
      "Epoch 023 | LR: 0.000004 | Train RMSE: 0.05618\n",
      "Epoch 024 | LR: 0.000004 | Train RMSE: 0.05618\n",
      "Epoch 025 | LR: 0.000004 | Train RMSE: 0.05617\n",
      "Epoch 026 | LR: 0.000003 | Train RMSE: 0.05617\n",
      "Epoch 027 | LR: 0.000003 | Train RMSE: 0.05616\n",
      "Epoch 028 | LR: 0.000003 | Train RMSE: 0.05616\n",
      "Epoch 029 | LR: 0.000002 | Train RMSE: 0.05616\n",
      "Epoch 030 | LR: 0.000002 | Train RMSE: 0.05616\n",
      "Epoch 031 | LR: 0.000002 | Train RMSE: 0.05615\n",
      "Epoch 032 | LR: 0.000002 | Train RMSE: 0.05615\n",
      "Epoch 033 | LR: 0.000002 | Train RMSE: 0.05615\n",
      "Epoch 034 | LR: 0.000001 | Train RMSE: 0.05615\n",
      "Epoch 035 | LR: 0.000001 | Train RMSE: 0.05615\n",
      "Epoch 036 | LR: 0.000001 | Train RMSE: 0.05615\n",
      "Epoch 037 | LR: 0.000001 | Train RMSE: 0.05614\n",
      "Epoch 038 | LR: 0.000001 | Train RMSE: 0.05614\n",
      "Epoch 039 | LR: 0.000001 | Train RMSE: 0.05614\n",
      "Epoch 040 | LR: 0.000001 | Train RMSE: 0.05614\n",
      "Epoch 041 | LR: 0.000001 | Train RMSE: 0.05614\n",
      "Epoch 042 | LR: 0.000001 | Train RMSE: 0.05614\n",
      "Epoch 043 | LR: 0.000001 | Train RMSE: 0.05614\n",
      "Epoch 044 | LR: 0.000000 | Train RMSE: 0.05614\n",
      "Epoch 045 | LR: 0.000000 | Train RMSE: 0.05614\n",
      "Epoch 046 | LR: 0.000000 | Train RMSE: 0.05614\n",
      "Epoch 047 | LR: 0.000000 | Train RMSE: 0.05614\n",
      "Epoch 048 | LR: 0.000000 | Train RMSE: 0.05614\n",
      "Epoch 049 | LR: 0.000000 | Train RMSE: 0.05614\n",
      "Epoch 050 | LR: 0.000000 | Train RMSE: 0.05614\n",
      "\n",
      "Full Training RMSE: 0.05591\n",
      "\n",
      "Blended submission saved as 'submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Invoke main execution\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13760552,
     "sourceId": 91721,
     "sourceType": "competition"
    },
    {
     "datasetId": 8395648,
     "sourceId": 13477880,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 266082294,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266219643,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266226513,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 266505672,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4856.381925,
   "end_time": "2025-10-26T17:22:43.706886",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-26T16:01:47.324961",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
