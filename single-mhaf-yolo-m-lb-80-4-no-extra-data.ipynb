{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc34de2",
   "metadata": {
    "papermill": {
     "duration": 0.003901,
     "end_time": "2025-05-05T16:31:22.882293",
     "exception": false,
     "start_time": "2025-05-05T16:31:22.878392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d7cca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:31:22.889703Z",
     "iopub.status.busy": "2025-05-05T16:31:22.889442Z",
     "iopub.status.idle": "2025-05-05T16:31:24.027977Z",
     "shell.execute_reply": "2025-05-05T16:31:24.026793Z"
    },
    "papermill": {
     "duration": 1.144127,
     "end_time": "2025-05-05T16:31:24.029751",
     "exception": false,
     "start_time": "2025-05-05T16:31:22.885624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/mhafyolo/pytorch/default/1/MHAF-YOLO-main /kaggle/working/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfb24f",
   "metadata": {
    "papermill": {
     "duration": 0.003019,
     "end_time": "2025-05-05T16:31:24.036414",
     "exception": false,
     "start_time": "2025-05-05T16:31:24.033395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Note\n",
    "This result was obtained solely by my own model, MHAF-YOLO-M, without using any additional datasets, extra data augmentation techniques, large-scale training or inference, special pre-processing or post-processing techniques, or model fusion. A similar notebook was previously made public but later deleted, with the initial LB score of 71.7. After the test set was updated, the LB score improved to **80.4**. It’s important to note that this does not mean our subsequent optimal LB score was achieved by this model—it is merely intended to provide insights and alternative YOLO-based solutions for fellow researchers. Feel free to star the project if you find it helpful! The inference code is credited to @yukiZ—many thanks for his contribution!\n",
    "\n",
    "Training Data:\n",
    "only official image data with num_motors>0 used (no external data, no negative sampling).\n",
    "80% training, 20% validation\n",
    "\n",
    "Image Size:\n",
    "(640, 640, 3) (both training and inference)\n",
    "\n",
    "Project and code: [MHAF-YOLO](https://github.com/yang-0201/MHAF-YOLO)\n",
    "\n",
    "Weights: https://www.kaggle.com/datasets/yyyy0201/mhaf-yolo-m-best\n",
    "\n",
    "LB: 80.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4542252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:31:24.043592Z",
     "iopub.status.busy": "2025-05-05T16:31:24.043299Z",
     "iopub.status.idle": "2025-05-05T16:31:24.047815Z",
     "shell.execute_reply": "2025-05-05T16:31:24.047095Z"
    },
    "papermill": {
     "duration": 0.009509,
     "end_time": "2025-05-05T16:31:24.048920",
     "exception": false,
     "start_time": "2025-05-05T16:31:24.039411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Train Model \"\"\"\n",
    "model_path = \"/kaggle/input/mhaf-yolo-m-best/36_data_96_663.pt\"\n",
    "\n",
    "\"\"\" [IMPORTANT]\n",
    "* This parameter has a significant impact on the value of LB since it is the threshold for the prediction score inferred by the model.\n",
    "* In my experiments, 0.5 to 0.55 is optimal for local CV, but when submitting, 0.35 to 0.45 seems to give better results, so there is a difference.\n",
    "\"\"\"\n",
    "CONFIDENCE_THRESHOLD = 0.45\n",
    "\n",
    "MAX_DETECTIONS_PER_TOMO = 1\n",
    "NMS_IOU_THRESHOLD = 0.2\n",
    "CONCENTRATION = 1\n",
    "BATCH_SIZE = 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e54ce",
   "metadata": {
    "papermill": {
     "duration": 0.002836,
     "end_time": "2025-05-05T16:31:24.054936",
     "exception": false,
     "start_time": "2025-05-05T16:31:24.052100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **》》》 Ultralytics Offline Install**(v8.3.88[2025/03/11 ReleaseVersion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b800099d",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-05-05T16:31:24.061940Z",
     "iopub.status.busy": "2025-05-05T16:31:24.061733Z",
     "iopub.status.idle": "2025-05-05T16:31:24.066976Z",
     "shell.execute_reply": "2025-05-05T16:31:24.066341Z"
    },
    "papermill": {
     "duration": 0.010212,
     "end_time": "2025-05-05T16:31:24.068142",
     "exception": false,
     "start_time": "2025-05-05T16:31:24.057930",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INFO]\\n* This notebookinstall Ultralytics v8.3.88(2025/03/11 ReleaseVersion)\\n  Can use YOLO12 is latest family version. \\n* If you need a newer version, you can make it available by running and attaching the notebook.\\n  https://www.kaggle.com/code/hideyukizushi/ultralytics-offlineinstall-yolo12-weights\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"[INFO]\n",
    "* This notebookinstall Ultralytics v8.3.88(2025/03/11 ReleaseVersion)\n",
    "  Can use YOLO12 is latest family version. \n",
    "* If you need a newer version, you can make it available by running and attaching the notebook.\n",
    "  https://www.kaggle.com/code/hideyukizushi/ultralytics-offlineinstall-yolo12-weights\n",
    "\"\"\"\n",
    "# !tar xfvz /kaggle/input/ultralytics-offlineinstall-yolo12-weights/archive.tar.gz\n",
    "# !pip install --no-index --find-links=./packages ultralytics\n",
    "# !rm -rf ./packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748516f",
   "metadata": {
    "papermill": {
     "duration": 0.003435,
     "end_time": "2025-05-05T16:31:24.075097",
     "exception": false,
     "start_time": "2025-05-05T16:31:24.071662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **》》》 Import Libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba2af26",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-05-05T16:31:24.082245Z",
     "iopub.status.busy": "2025-05-05T16:31:24.081987Z",
     "iopub.status.idle": "2025-05-05T16:31:39.563480Z",
     "shell.execute_reply": "2025-05-05T16:31:39.562552Z"
    },
    "papermill": {
     "duration": 15.486987,
     "end_time": "2025-05-05T16:31:39.565252",
     "exception": false,
     "start_time": "2025-05-05T16:31:24.078265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_dir: /kaggle/working\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "print(\"this_dir:\", current_dir)\n",
    "\n",
    "target_dir = Path(\"/kaggle/working/MHAF-YOLO-main\") \n",
    "os.chdir(target_dir)  \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLOv10\n",
    "import threading\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82394e11",
   "metadata": {
    "papermill": {
     "duration": 0.00309,
     "end_time": "2025-05-05T16:31:39.572167",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.569077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **》》》 Seed Fix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58e6779",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-05-05T16:31:39.579498Z",
     "iopub.status.busy": "2025-05-05T16:31:39.579059Z",
     "iopub.status.idle": "2025-05-05T16:31:39.587576Z",
     "shell.execute_reply": "2025-05-05T16:31:39.586880Z"
    },
    "papermill": {
     "duration": 0.0135,
     "end_time": "2025-05-05T16:31:39.588817",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.575317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7a0dc8b89dd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebae509",
   "metadata": {
    "papermill": {
     "duration": 0.003038,
     "end_time": "2025-05-05T16:31:39.595277",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.592239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **》》》 Inference&Submission**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ced025",
   "metadata": {
    "papermill": {
     "duration": 0.004247,
     "end_time": "2025-05-05T16:31:39.603137",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.598890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f584d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:31:39.610640Z",
     "iopub.status.busy": "2025-05-05T16:31:39.610310Z",
     "iopub.status.idle": "2025-05-05T16:31:39.613701Z",
     "shell.execute_reply": "2025-05-05T16:31:39.613065Z"
    },
    "papermill": {
     "duration": 0.008369,
     "end_time": "2025-05-05T16:31:39.614851",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.606482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/\"\n",
    "test_dir = os.path.join(data_path, \"test\")\n",
    "submission_path = \"/kaggle/working/submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b67091",
   "metadata": {
    "papermill": {
     "duration": 0.003064,
     "end_time": "2025-05-05T16:31:39.621254",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.618190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* GPU Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa654b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:31:39.628566Z",
     "iopub.status.busy": "2025-05-05T16:31:39.628354Z",
     "iopub.status.idle": "2025-05-05T16:31:39.706935Z",
     "shell.execute_reply": "2025-05-05T16:31:39.706207Z"
    },
    "papermill": {
     "duration": 0.083933,
     "end_time": "2025-05-05T16:31:39.708421",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.624488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla P100-PCIE-16GB with 17.06 GB memory\n",
      "Dynamic batch size set to 32 based on 17.06GB free memory\n"
     ]
    }
   ],
   "source": [
    "class GPUProfiler:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.start_time = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed = time.time() - self.start_time\n",
    "        # print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "if device.startswith('cuda'):\n",
    "    # Set CUDA optimization flags\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Print GPU info\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
    "    print(f\"Using GPU: {gpu_name} with {gpu_mem:.2f} GB memory\")\n",
    "    \n",
    "    # Get available GPU memory and set batch size accordingly\n",
    "    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n",
    "    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))  # 4 images per GB as rough estimate\n",
    "    print(f\"Dynamic batch size set to {BATCH_SIZE} based on {free_mem:.2f}GB free memory\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")\n",
    "    BATCH_SIZE = 4  # Reduce batch size for CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0ba33",
   "metadata": {
    "papermill": {
     "duration": 0.003228,
     "end_time": "2025-05-05T16:31:39.715258",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.712030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf353d1",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-05-05T16:31:39.722828Z",
     "iopub.status.busy": "2025-05-05T16:31:39.722602Z",
     "iopub.status.idle": "2025-05-05T16:33:23.679974Z",
     "shell.execute_reply": "2025-05-05T16:33:23.679023Z"
    },
    "papermill": {
     "duration": 103.972352,
     "end_time": "2025-05-05T16:33:23.690896",
     "exception": false,
     "start_time": "2025-05-05T16:31:39.718544",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/MHAF-YOLO-main/ultralytics/nn/tasks.py:751: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "MAF-YOLOv10m-v2 summary: 838 layers, 15799254 parameters, 824896 gradients\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Motor found in tomo_003acc at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Motor found in tomo_00e047 at position: z=165, y=546, x=603\n",
      "Current detection rate: 2/2 (100.0%)\n",
      "Motor found in tomo_01a877 at position: z=139, y=638, x=287\n",
      "Current detection rate: 3/3 (100.0%)\n",
      "==================================================\n",
      "= Submission preview:\n",
      "==================================================\n",
      "       tomo_id  Motor axis 0  Motor axis 1  Motor axis 2\n",
      "0  tomo_003acc            -1            -1            -1\n",
      "1  tomo_00e047           165           546           603\n",
      "2  tomo_01a877           139           638           287\n",
      "\n",
      "Total execution time: 103.93 seconds (1.73 minutes)\n"
     ]
    }
   ],
   "source": [
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    Normalize slice data using 2nd and 98th percentiles for better contrast\n",
    "    \"\"\"\n",
    "    p2 = np.percentile(slice_data, 2)\n",
    "    p98 = np.percentile(slice_data, 98)\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    return np.uint8(normalized)\n",
    "\n",
    "def preload_image_batch(file_paths):\n",
    "    \"\"\"Preload a batch of images to CPU memory\"\"\"\n",
    "    images = []\n",
    "    for path in file_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            # Try with PIL as fallback\n",
    "            img = np.array(Image.open(path))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "def process_tomogram(tomo_id, model, index=0, total=1):\n",
    "    \"\"\"\n",
    "    Process a single tomogram and return the most confident motor detection\n",
    "    \"\"\"\n",
    "    # print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n",
    "    \n",
    "    # Get all slice files for this tomogram\n",
    "    tomo_dir = os.path.join(test_dir, tomo_id)\n",
    "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    # Apply CONCENTRATION to reduce the number of slices processed\n",
    "    # This will process approximately CONCENTRATION fraction of all slices\n",
    "    selected_indices = np.linspace(0, len(slice_files)-1, int(len(slice_files) * CONCENTRATION))\n",
    "    selected_indices = np.round(selected_indices).astype(int)\n",
    "    slice_files = [slice_files[i] for i in selected_indices]\n",
    "    \n",
    "\n",
    "    all_detections = []\n",
    "    if device.startswith('cuda'):\n",
    "        streams = [torch.cuda.Stream() for _ in range(min(4, BATCH_SIZE))]\n",
    "    else:\n",
    "        streams = [None]\n",
    "    \n",
    "    # Variables for preloading\n",
    "    next_batch_thread = None\n",
    "    next_batch_images = None\n",
    "    \n",
    "    # Process slices in batches\n",
    "    for batch_start in range(0, len(slice_files), BATCH_SIZE):\n",
    "        # Wait for previous preload thread if it exists\n",
    "        if next_batch_thread is not None:\n",
    "            next_batch_thread.join()\n",
    "            next_batch_images = None\n",
    "            \n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(slice_files))\n",
    "        batch_files = slice_files[batch_start:batch_end]\n",
    "        \n",
    "        # Start preloading next batch\n",
    "        next_batch_start = batch_end\n",
    "        next_batch_end = min(next_batch_start + BATCH_SIZE, len(slice_files))\n",
    "        next_batch_files = slice_files[next_batch_start:next_batch_end] if next_batch_start < len(slice_files) else []\n",
    "        \n",
    "        if next_batch_files:\n",
    "            next_batch_paths = [os.path.join(tomo_dir, f) for f in next_batch_files]\n",
    "            next_batch_thread = threading.Thread(target=preload_image_batch, args=(next_batch_paths,))\n",
    "            next_batch_thread.start()\n",
    "        else:\n",
    "            next_batch_thread = None\n",
    "        \n",
    "        # Split batch across streams for parallel processing\n",
    "        sub_batches = np.array_split(batch_files, len(streams))\n",
    "        sub_batch_results = []\n",
    "        \n",
    "        for i, sub_batch in enumerate(sub_batches):\n",
    "            if len(sub_batch) == 0:\n",
    "                continue\n",
    "                \n",
    "            stream = streams[i % len(streams)]\n",
    "            with torch.cuda.stream(stream) if stream and device.startswith('cuda') else nullcontext():\n",
    "                # Process sub-batch\n",
    "                sub_batch_paths = [os.path.join(tomo_dir, slice_file) for slice_file in sub_batch]\n",
    "                sub_batch_slice_nums = [int(slice_file.split('_')[1].split('.')[0]) for slice_file in sub_batch]\n",
    "                \n",
    "                # Run inference with profiling\n",
    "                with GPUProfiler(f\"Inference batch {i+1}/{len(sub_batches)}\"):\n",
    "                    sub_results = model(sub_batch_paths, verbose=False)\n",
    "                \n",
    "                # Process each result in this sub-batch\n",
    "                for j, result in enumerate(sub_results):\n",
    "                    if len(result.boxes) > 0:\n",
    "                        boxes = result.boxes\n",
    "                        for box_idx, confidence in enumerate(boxes.conf):\n",
    "                            if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                                # Get bounding box coordinates\n",
    "                                x1, y1, x2, y2 = boxes.xyxy[box_idx].cpu().numpy()\n",
    "                                \n",
    "                                # Calculate center coordinates\n",
    "                                x_center = (x1 + x2) / 2\n",
    "                                y_center = (y1 + y2) / 2\n",
    "                                \n",
    "                                # Store detection with 3D coordinates\n",
    "                                all_detections.append({\n",
    "                                    'z': round(sub_batch_slice_nums[j]),\n",
    "                                    'y': round(y_center),\n",
    "                                    'x': round(x_center),\n",
    "                                    'confidence': float(confidence)\n",
    "                                })\n",
    "        \n",
    "        # Synchronize streams\n",
    "        if device.startswith('cuda'):\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    # Clean up thread if still running\n",
    "    if next_batch_thread is not None:\n",
    "        next_batch_thread.join()\n",
    "    \n",
    "    # 3D Non-Maximum Suppression to merge nearby detections across slices\n",
    "    final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n",
    "    \n",
    "    # Sort detections by confidence (highest first)\n",
    "    final_detections.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # If there are no detections, return NA values\n",
    "    if not final_detections:\n",
    "        return {\n",
    "            'tomo_id': tomo_id,\n",
    "            'Motor axis 0': -1,\n",
    "            'Motor axis 1': -1,\n",
    "            'Motor axis 2': -1\n",
    "        }\n",
    "    \n",
    "    # Take the detection with highest confidence\n",
    "    best_detection = final_detections[0]\n",
    "    \n",
    "    # Return result with integer coordinates\n",
    "    return {\n",
    "        'tomo_id': tomo_id,\n",
    "        'Motor axis 0': round(best_detection['z']),\n",
    "        'Motor axis 1': round(best_detection['y']),\n",
    "        'Motor axis 2': round(best_detection['x'])\n",
    "    }\n",
    "\n",
    "def perform_3d_nms(detections, iou_threshold):\n",
    "    \"\"\"\n",
    "    Perform 3D Non-Maximum Suppression on detections to merge nearby motors\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "    \n",
    "    # Sort by confidence (highest first)\n",
    "    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # List to store final detections after NMS\n",
    "    final_detections = []\n",
    "    \n",
    "    # Define 3D distance function\n",
    "    def distance_3d(d1, d2):\n",
    "        return np.sqrt((d1['z'] - d2['z'])**2 + \n",
    "                       (d1['y'] - d2['y'])**2 + \n",
    "                       (d1['x'] - d2['x'])**2)\n",
    "    \n",
    "    # Maximum distance threshold (based on box size and slice gap)\n",
    "    box_size = 24  # Same as annotation box size\n",
    "    distance_threshold = box_size * iou_threshold\n",
    "    \n",
    "    # Process each detection\n",
    "    while detections:\n",
    "        # Take the detection with highest confidence\n",
    "        best_detection = detections.pop(0)\n",
    "        final_detections.append(best_detection)\n",
    "        \n",
    "        # Filter out detections that are too close to the best detection\n",
    "        detections = [d for d in detections if distance_3d(d, best_detection) > distance_threshold]\n",
    "    \n",
    "    return final_detections\n",
    "\n",
    "def debug_image_loading(tomo_id):\n",
    "    \"\"\"\n",
    "    Debug function to check image loading\n",
    "    \"\"\"\n",
    "    tomo_dir = os.path.join(test_dir, tomo_id)\n",
    "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    if not slice_files:\n",
    "        print(f\"No image files found in {tomo_dir}\")\n",
    "        return\n",
    "        \n",
    "    sample_file = slice_files[len(slice_files)//2]  # Middle slice\n",
    "    img_path = os.path.join(tomo_dir, sample_file)\n",
    "    \n",
    "    # Try different loading methods\n",
    "    try:\n",
    "        # Method 1: PIL\n",
    "        img_pil = Image.open(img_path)\n",
    "        img_array_pil = np.array(img_pil)\n",
    "        \n",
    "        # Method 2: OpenCV\n",
    "        img_cv2 = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # print(f\"OpenCV Image shape: {img_cv2.shape}, dtype: {img_cv2.dtype}\")\n",
    "        \n",
    "        # Method 3: Convert to RGB\n",
    "        img_rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        \n",
    "    # Also test with YOLO's built-in loader\n",
    "    try:\n",
    "        test_model = YOLOv10(model_path)\n",
    "        test_results = test_model([img_path], verbose=False)\n",
    "        # print(\"YOLO model successfully processed the test image\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with YOLO processing: {e}\")\n",
    "\n",
    "def generate_submission():\n",
    "    \"\"\"\n",
    "    Main function to generate the submission file\n",
    "    \"\"\"\n",
    "    test_tomos = sorted([d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))])\n",
    "    total_tomos = len(test_tomos)\n",
    "    \n",
    "    if test_tomos:\n",
    "        debug_image_loading(test_tomos[0])\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model = YOLOv10(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Additional optimizations for inference\n",
    "    if device.startswith('cuda'):\n",
    "        # Fuse conv and bn layers for faster inference\n",
    "        model.fuse()\n",
    "        \n",
    "        # Enable model half precision (FP16) if on compatible GPU\n",
    "        if torch.cuda.get_device_capability(0)[0] >= 7:  # Volta or newer\n",
    "            model.model.half()\n",
    "    \n",
    "    # Process tomograms with parallelization\n",
    "    results = []\n",
    "    motors_found = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        future_to_tomo = {}\n",
    "        \n",
    "        # Submit all tomograms for processing\n",
    "        for i, tomo_id in enumerate(test_tomos, 1):\n",
    "            future = executor.submit(process_tomogram, tomo_id, model, i, total_tomos)\n",
    "            future_to_tomo[future] = tomo_id\n",
    "        \n",
    "        # Process completed futures as they complete\n",
    "        for future in future_to_tomo:\n",
    "            tomo_id = future_to_tomo[future]\n",
    "            try:\n",
    "                # Clear CUDA cache between tomograms\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update motors found count\n",
    "                has_motor = not pd.isna(result['Motor axis 0'])\n",
    "                if has_motor:\n",
    "                    motors_found += 1\n",
    "                    print(f\"Motor found in {tomo_id} at position: \"\n",
    "                          f\"z={result['Motor axis 0']}, y={result['Motor axis 1']}, x={result['Motor axis 2']}\")\n",
    "                else:\n",
    "                    print(f\"No motor detected in {tomo_id}\")\n",
    "                    \n",
    "                print(f\"Current detection rate: {motors_found}/{len(results)} ({motors_found/len(results)*100:.1f}%)\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {tomo_id}: {e}\")\n",
    "                # Create a default entry for failed tomograms\n",
    "                results.append({\n",
    "                    'tomo_id': tomo_id,\n",
    "                    'Motor axis 0': -1,\n",
    "                    'Motor axis 1': -1,\n",
    "                    'Motor axis 2': -1\n",
    "                })\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure proper column order\n",
    "    submission_df = submission_df[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\n",
    "    \n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(\"=\"*50)\n",
    "    print(\"= Submission preview:\")\n",
    "    print(\"=\"*50)\n",
    "    print(submission_df.head())\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "# Run the submission pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Time entire process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate submission\n",
    "    submission = generate_submission()\n",
    "    \n",
    "    # Print total execution time\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nTotal execution time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11294684,
     "sourceId": 91249,
     "sourceType": "competition"
    },
    {
     "datasetId": 7241173,
     "sourceId": 11652579,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 295634,
     "modelInstanceId": 274744,
     "sourceId": 327336,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.835489,
   "end_time": "2025-05-05T16:33:25.990652",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T16:31:20.155163",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
