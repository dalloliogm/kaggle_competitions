{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741fee23",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-06T13:07:46.938337Z",
     "iopub.status.busy": "2025-04-06T13:07:46.937967Z",
     "iopub.status.idle": "2025-04-06T13:08:01.809827Z",
     "shell.execute_reply": "2025-04-06T13:08:01.809049Z"
    },
    "papermill": {
     "duration": 14.878215,
     "end_time": "2025-04-06T13:08:01.811490",
     "exception": false,
     "start_time": "2025-04-06T13:07:46.933275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.spatial import distance_matrix\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "class CellTypeGATModel:\n",
    "    \"\"\"\n",
    "    A Graph Attention Network (GAT) pipeline for cell type prediction that leverages\n",
    "    spatial relationships between cells with attention mechanisms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h5_file_path, random_state=42):\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.train_spot_tables = {}\n",
    "        self.cell_type_columns = None\n",
    "        self.model = None\n",
    "        self.random_state = random_state\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.history = None\n",
    "        self.k_neighbors = 15  # Number of neighbors to consider for each node\n",
    "\n",
    "    def load_train_data(self):\n",
    "        \"\"\"\n",
    "        Loads training data from the H5 file and converts each slide to a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            train_spots = f[\"spots/Train\"]\n",
    "            for slide_name in train_spots.keys():\n",
    "                spot_array = np.array(train_spots[slide_name])\n",
    "                df = pd.DataFrame(spot_array)\n",
    "                self.train_spot_tables[slide_name] = df\n",
    "        print(f\"Training data loaded successfully. Found {len(self.train_spot_tables)} slides.\")\n",
    "        return self.train_spot_tables\n",
    "\n",
    "    def create_graph_features(self, df, k_neighbors=None):\n",
    "        \"\"\"\n",
    "        Creates graph-based features for each spot based on its spatial coordinates.\n",
    "        \n",
    "        Parameters:\n",
    "            df (DataFrame): DataFrame containing spot data with 'x' and 'y' columns\n",
    "            k_neighbors (int): Number of nearest neighbors to consider for each spot\n",
    "            \n",
    "        Returns:\n",
    "            node_features (np.array): Node features for each spot\n",
    "            adjacency_lists (list): List of adjacency matrices for each spot\n",
    "            edge_features (np.array): Edge features for GAT attention mechanism\n",
    "        \"\"\"\n",
    "        if k_neighbors is None:\n",
    "            k_neighbors = self.k_neighbors\n",
    "            \n",
    "        # Extract coordinates\n",
    "        positions = df[['x', 'y']].values\n",
    "        \n",
    "        # Calculate distance matrix\n",
    "        dist_matrix = distance_matrix(positions, positions)\n",
    "        \n",
    "        # For each node, find k nearest neighbors\n",
    "        nearest_indices = np.argsort(dist_matrix, axis=1)[:, 1:k_neighbors+1]  # Exclude self\n",
    "        \n",
    "        # Create basic node features (position, engineered features)\n",
    "        node_features = []\n",
    "        \n",
    "        for i, pos in enumerate(positions):\n",
    "            x, y = pos\n",
    "            \n",
    "            # Basic features\n",
    "            features = [x, y]\n",
    "            \n",
    "            # Add engineered features\n",
    "            r = np.sqrt(x**2 + y**2)  # Radius from origin\n",
    "            theta = np.arctan2(y, x)  # Angle\n",
    "            features.extend([r, theta, x**2, y**2, x*y])\n",
    "            \n",
    "            # Add neighborhood features\n",
    "            neighbors = nearest_indices[i]\n",
    "            neighbor_positions = positions[neighbors]\n",
    "            \n",
    "            # Calculate mean and std of neighbor positions\n",
    "            mean_x = np.mean(neighbor_positions[:, 0])\n",
    "            mean_y = np.mean(neighbor_positions[:, 1])\n",
    "            std_x = np.std(neighbor_positions[:, 0])\n",
    "            std_y = np.std(neighbor_positions[:, 1])\n",
    "            \n",
    "            # Calculate distance statistics to neighbors\n",
    "            distances = dist_matrix[i, neighbors]\n",
    "            mean_dist = np.mean(distances)\n",
    "            min_dist = np.min(distances)\n",
    "            max_dist = np.max(distances)\n",
    "            \n",
    "            # Add neighborhood features\n",
    "            features.extend([mean_x, mean_y, std_x, std_y, mean_dist, min_dist, max_dist])\n",
    "            \n",
    "            node_features.append(features)\n",
    "            \n",
    "        # Convert to numpy array\n",
    "        node_features = np.array(node_features)\n",
    "        \n",
    "        # Create adjacency lists and edge features\n",
    "        adjacency_lists = []\n",
    "        edge_features_list = []\n",
    "        \n",
    "        for i in range(len(positions)):\n",
    "            # Get neighbors\n",
    "            neighbors = nearest_indices[i]\n",
    "            \n",
    "            # Create adjacency list with weights\n",
    "            adj_list = []\n",
    "            edge_feats = []\n",
    "            \n",
    "            for j in neighbors:\n",
    "                # Skip if same node\n",
    "                if i == j:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate edge weight (inverse distance)\n",
    "                weight = 1.0 / (dist_matrix[i, j] + 1e-6)  # Add small epsilon to avoid division by zero\n",
    "                \n",
    "                # Create edge features (useful for attention mechanism)\n",
    "                dx = positions[i][0] - positions[j][0]\n",
    "                dy = positions[i][1] - positions[j][1]\n",
    "                distance = dist_matrix[i, j]\n",
    "                angle = np.arctan2(dy, dx)\n",
    "                \n",
    "                # Edge features: distance, dx, dy, angle, weight\n",
    "                edge_feat = [distance, dx, dy, angle, weight]\n",
    "                \n",
    "                # Add edge\n",
    "                adj_list.append((j, weight))\n",
    "                edge_feats.append(edge_feat)\n",
    "                \n",
    "            adjacency_lists.append(adj_list)\n",
    "            edge_features_list.append(edge_feats)\n",
    "            \n",
    "        # Converting edge features to proper format for the GAT model\n",
    "        edge_features = np.array([np.array(ef) for ef in edge_features_list], dtype=object)\n",
    "            \n",
    "        return node_features, adjacency_lists, edge_features\n",
    "\n",
    "    def prepare_training_set(self, slide_ids=None, normalize=True):\n",
    "        \"\"\"\n",
    "        Prepares training features and targets with graph structure.\n",
    "        \n",
    "        Parameters:\n",
    "            slide_ids (list): List of slide IDs to include. If None, uses all slides.\n",
    "            normalize (bool): Whether to normalize features.\n",
    "            \n",
    "        Returns:\n",
    "            node_features (np.array): Processed node feature array.\n",
    "            adjacency_lists (list): List of adjacency lists for each node.\n",
    "            edge_features (np.array): Edge features for GAT attention mechanism.\n",
    "            y (np.array): Target array.\n",
    "        \"\"\"\n",
    "        if not self.train_spot_tables:\n",
    "            self.load_train_data()\n",
    "            \n",
    "        if slide_ids is None:\n",
    "            slide_ids = list(self.train_spot_tables.keys())\n",
    "        elif isinstance(slide_ids, str):\n",
    "            slide_ids = [slide_ids]\n",
    "            \n",
    "        # Combine data from all specified slides\n",
    "        dfs = []\n",
    "        for slide_id in slide_ids:\n",
    "            if slide_id not in self.train_spot_tables:\n",
    "                raise ValueError(f\"Slide {slide_id} not found in training data.\")\n",
    "            dfs.append(self.train_spot_tables[slide_id])\n",
    "        \n",
    "        combined_df = pd.concat(dfs, axis=0)\n",
    "        combined_df = shuffle(combined_df, random_state=self.random_state)\n",
    "        \n",
    "        # Basic features for identifying target columns\n",
    "        feature_cols = ['x', 'y']\n",
    "        \n",
    "        # Identify target columns\n",
    "        target_cols = [col for col in combined_df.columns if col not in feature_cols]\n",
    "        self.cell_type_columns = target_cols\n",
    "        \n",
    "        # Generate graph features\n",
    "        node_features, adjacency_lists, edge_features = self.create_graph_features(combined_df)\n",
    "        \n",
    "        # Extract targets\n",
    "        y = combined_df[target_cols].values.astype(float)\n",
    "        \n",
    "        # Normalize features if requested\n",
    "        if normalize:\n",
    "            node_features = self.feature_scaler.fit_transform(node_features)\n",
    "            \n",
    "        print(f\"Training set prepared with {node_features.shape[0]} samples, {node_features.shape[1]} node features, and {y.shape[1]} target variables.\")\n",
    "        return node_features, adjacency_lists, edge_features, y\n",
    "\n",
    "    def load_test_data(self, slide_id, normalize=True):\n",
    "        \"\"\"\n",
    "        Loads and processes test data for a given slide with graph structure.\n",
    "        \n",
    "        Parameters:\n",
    "            slide_id (str): ID of the slide to load.\n",
    "            normalize (bool): Whether to normalize features.\n",
    "            \n",
    "        Returns:\n",
    "            df (DataFrame): Processed test data.\n",
    "            node_features (np.array): Processed node feature array.\n",
    "            adjacency_lists (list): List of adjacency lists for each node.\n",
    "            edge_features (np.array): Edge features for GAT attention mechanism.\n",
    "        \"\"\"\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            test_spots = f[\"spots/Test\"]\n",
    "            if slide_id not in test_spots:\n",
    "                raise ValueError(f\"Slide {slide_id} not found in test data.\")\n",
    "            spot_array = np.array(test_spots[slide_id])\n",
    "            df = pd.DataFrame(spot_array)\n",
    "        \n",
    "        # Generate graph features\n",
    "        node_features, adjacency_lists, edge_features = self.create_graph_features(df)\n",
    "        \n",
    "        # Normalize features if requested\n",
    "        if normalize:\n",
    "            if not hasattr(self.feature_scaler, 'mean_'):\n",
    "                raise ValueError(\"Feature scaler has not been fit. Run prepare_training_set first.\")\n",
    "            node_features = self.feature_scaler.transform(node_features)\n",
    "            \n",
    "        print(f\"Test data for slide {slide_id} loaded with {node_features.shape[0]} samples and {node_features.shape[1]} node features.\")\n",
    "        return df, node_features, adjacency_lists, edge_features\n",
    "    \n",
    "    def gat_attention_layer(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Graph Attention mechanism implementation.\n",
    "        \n",
    "        Parameters:\n",
    "            query (tf.Tensor): Query tensor\n",
    "            key (tf.Tensor): Key tensor\n",
    "            value (tf.Tensor): Value tensor\n",
    "            mask (tf.Tensor): Mask tensor\n",
    "            \n",
    "        Returns:\n",
    "            output (tf.Tensor): Attention weighted output\n",
    "            attention_weights (tf.Tensor): Attention weights\n",
    "        \"\"\"\n",
    "        # Calculate attention scores\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        \n",
    "        # Scale attention scores\n",
    "        dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def make_adjacency_matrix(self, adjacency_lists, num_nodes):\n",
    "        \"\"\"\n",
    "        Converts adjacency lists to adjacency matrix format.\n",
    "        \n",
    "        Parameters:\n",
    "            adjacency_lists (list): List of adjacency lists\n",
    "            num_nodes (int): Number of nodes in the graph\n",
    "            \n",
    "        Returns:\n",
    "            adj_matrix (tf.Tensor): Adjacency matrix\n",
    "        \"\"\"\n",
    "        # Initialize adjacency matrix with zeros\n",
    "        adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "        \n",
    "        # Fill in adjacency matrix\n",
    "        for i, adj_list in enumerate(adjacency_lists):\n",
    "            for j, weight in adj_list:\n",
    "                adj_matrix[i, j] = weight\n",
    "                \n",
    "        return tf.convert_to_tensor(adj_matrix, dtype=tf.float32)\n",
    "    \n",
    "    def gat_layer(self, inputs, adjacency_matrix, out_features, n_heads=4, \n",
    "                  dropout_rate=0.2, activation='relu', concat=True):\n",
    "        \"\"\"\n",
    "        Graph Attention Network layer implementation.\n",
    "        \n",
    "        Parameters:\n",
    "            inputs (tf.Tensor): Input node features\n",
    "            adjacency_matrix (tf.Tensor): Adjacency matrix\n",
    "            out_features (int): Number of output features\n",
    "            n_heads (int): Number of attention heads\n",
    "            dropout_rate (float): Dropout rate\n",
    "            activation (str): Activation function\n",
    "            concat (bool): Whether to concatenate or average multi-head attention\n",
    "            \n",
    "        Returns:\n",
    "            outputs (tf.Tensor): Updated node features\n",
    "        \"\"\"\n",
    "        batch_size, n_nodes, in_features = tf.shape(inputs)\n",
    "        \n",
    "        # Feature transformation for each head\n",
    "        heads_outputs = []\n",
    "        for head in range(n_heads):\n",
    "            # Linear transformation for queries, keys, and values\n",
    "            q_dense = tf.keras.layers.Dense(out_features // n_heads)\n",
    "            k_dense = tf.keras.layers.Dense(out_features // n_heads)\n",
    "            v_dense = tf.keras.layers.Dense(out_features // n_heads)\n",
    "            \n",
    "            # Apply transformations\n",
    "            queries = q_dense(inputs)\n",
    "            keys = k_dense(inputs)\n",
    "            values = v_dense(inputs)\n",
    "            \n",
    "            # Create mask from adjacency matrix\n",
    "            mask = tf.cast(adjacency_matrix <= 0, tf.float32)\n",
    "            \n",
    "            # Apply attention\n",
    "            attended_values, attention_weights = self.gat_attention_layer(\n",
    "                queries, keys, values, mask\n",
    "            )\n",
    "            \n",
    "            # Apply dropout\n",
    "            attended_values = tf.keras.layers.Dropout(dropout_rate)(attended_values)\n",
    "            \n",
    "            heads_outputs.append(attended_values)\n",
    "        \n",
    "        # Combine heads\n",
    "        if concat:\n",
    "            # Concatenate along feature dimension\n",
    "            outputs = tf.concat(heads_outputs, axis=-1)\n",
    "        else:\n",
    "            # Average heads\n",
    "            outputs = tf.reduce_mean(tf.stack(heads_outputs), axis=0)\n",
    "        \n",
    "        # Apply final activation\n",
    "        if activation is not None:\n",
    "            outputs = tf.keras.layers.Activation(activation)(outputs)\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def build_model(self, input_dim=None, dropout_rate=0.5, l2_reg=0.001, \n",
    "                    learning_rate=0.0001, n_heads=4):\n",
    "        \"\"\"\n",
    "        Builds a Graph Attention Network model for cell type prediction.\n",
    "        \n",
    "        Parameters:\n",
    "            input_dim (int): Dimension of input node features\n",
    "            dropout_rate (float): Dropout rate for regularization\n",
    "            l2_reg (float): L2 regularization weight\n",
    "            learning_rate (float): Learning rate for optimizer\n",
    "            n_heads (int): Number of attention heads in GAT layers\n",
    "            \n",
    "        Returns:\n",
    "            model (tf.keras.Model): Compiled GAT model\n",
    "        \"\"\"\n",
    "        if input_dim is None:\n",
    "            raise ValueError(\"Input dimension must be provided\")\n",
    "            \n",
    "        # Define inputs for node features and adjacency matrix\n",
    "        node_features_input = layers.Input(shape=(input_dim,), name='node_features')\n",
    "        adjacency_input = layers.Input(shape=(None,), name='adjacency_matrix', sparse=True)\n",
    "        \n",
    "        # First, transform node features\n",
    "        x = layers.Dense(\n",
    "            128, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(l2_reg)\n",
    "        )(node_features_input)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # Use Keras Functional API for feature transformations\n",
    "        # Since we can't directly implement GAT in the model definition,\n",
    "        # we'll apply standard transformations here and implement GAT logic in forward pass\n",
    "        \n",
    "        # First feature transformation (equivalent to first GAT layer)\n",
    "        x = layers.Dense(\n",
    "            256, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        x = layers.Dense(\n",
    "            512, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        x = layers.Dense(\n",
    "            1024, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(\n",
    "            len(self.cell_type_columns),\n",
    "            activation=None\n",
    "        )(x)\n",
    "        \n",
    "        # Create model\n",
    "        # Note: For simplicity in the Keras model, we use node_features_input only\n",
    "        # The GAT attention logic will be implemented in custom training steps\n",
    "        model = models.Model(inputs=node_features_input, outputs=outputs)\n",
    "\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.model = model\n",
    "        print(\"Graph Attention Network model built.\")\n",
    "        return model\n",
    "    \n",
    "    # Implementation of GAT forward pass for training\n",
    "    def custom_gat_forward(self, node_features, adjacency_lists, edge_features=None):\n",
    "        \"\"\"\n",
    "        Custom GAT forward pass to be used during manual training.\n",
    "        \n",
    "        Parameters:\n",
    "            node_features (np.array): Node features\n",
    "            adjacency_lists (list): Adjacency lists for each node\n",
    "            edge_features (np.array): Edge features for attention computation\n",
    "            \n",
    "        Returns:\n",
    "            updated_features (np.array): Updated node features after GAT forward pass\n",
    "        \"\"\"\n",
    "        num_nodes = len(node_features)\n",
    "        feature_dim = node_features.shape[1]\n",
    "        \n",
    "        # Create adjacency matrix\n",
    "        adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "        for i, neighbors in enumerate(adjacency_lists):\n",
    "            for j, weight in neighbors:\n",
    "                adj_matrix[i, j] = weight\n",
    "        \n",
    "        # Number of attention heads\n",
    "        n_heads = 8\n",
    "        head_dim = 32  # Dimension per head\n",
    "        \n",
    "        # Initialize attention parameters\n",
    "        W = np.random.normal(0, 0.1, (n_heads, feature_dim, head_dim))\n",
    "        a1 = np.random.normal(0, 0.1, (n_heads, head_dim, 1))\n",
    "        a2 = np.random.normal(0, 0.1, (n_heads, head_dim, 1))\n",
    "        \n",
    "        # Initialize output\n",
    "        multi_head_output = np.zeros((num_nodes, n_heads * head_dim))\n",
    "        \n",
    "        # For each attention head\n",
    "        for h in range(n_heads):\n",
    "            # Transform node features\n",
    "            transformed_features = np.dot(node_features, W[h])  # (num_nodes, head_dim)\n",
    "            \n",
    "            # Compute attention coefficients\n",
    "            e = np.zeros((num_nodes, num_nodes))\n",
    "            \n",
    "            for i in range(num_nodes):\n",
    "                neighbors = [j for j, _ in adjacency_lists[i]]\n",
    "                if not neighbors:\n",
    "                    continue\n",
    "                    \n",
    "                # Self-attention\n",
    "                query = transformed_features[i].reshape(-1, 1)  # (head_dim, 1)\n",
    "                \n",
    "                # Compute attention with each neighbor\n",
    "                for j in neighbors:\n",
    "                    key = transformed_features[j].reshape(-1, 1)  # (head_dim, 1)\n",
    "                    \n",
    "                    # LeakyReLU(a^T [Wh_i || Wh_j])\n",
    "                    e_ij = np.dot(a1[h].T, query) + np.dot(a2[h].T, key)\n",
    "                    e_ij = max(0.01 * e_ij, e_ij)  # LeakyReLU with alpha=0.01\n",
    "                    \n",
    "                    # Add edge feature influence if available\n",
    "                    if edge_features is not None:\n",
    "                        # Find edge feature for this pair\n",
    "                        for idx, (neigh, _) in enumerate(adjacency_lists[i]):\n",
    "                            if neigh == j and idx < len(edge_features[i]):\n",
    "                                edge_feat = edge_features[i][idx]\n",
    "                                # Simple weight based on distance\n",
    "                                edge_weight = 1.0 / (edge_feat[0] + 1e-6)\n",
    "                                e_ij *= edge_weight\n",
    "                                break\n",
    "                    \n",
    "                    e[i, j] = e_ij\n",
    "            \n",
    "            # Apply softmax to get attention weights\n",
    "            max_e = np.max(e, axis=1, keepdims=True)\n",
    "            exp_e = np.exp(e - max_e)\n",
    "            \n",
    "            # Mask with adjacency matrix (only consider existing edges)\n",
    "            masked_exp_e = exp_e * (adj_matrix > 0)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            sum_exp_e = np.sum(masked_exp_e, axis=1, keepdims=True)\n",
    "            sum_exp_e = np.where(sum_exp_e == 0, 1.0, sum_exp_e)\n",
    "            \n",
    "            attention = masked_exp_e / sum_exp_e\n",
    "            \n",
    "            # Apply attention weights to get output\n",
    "            head_output = np.zeros((num_nodes, head_dim))\n",
    "            \n",
    "            for i in range(num_nodes):\n",
    "                weighted_sum = np.zeros(head_dim)\n",
    "                for j in range(num_nodes):\n",
    "                    if attention[i, j] > 0:\n",
    "                        weighted_sum += attention[i, j] * transformed_features[j]\n",
    "                head_output[i] = weighted_sum\n",
    "                \n",
    "            # Apply ReLU activation\n",
    "            head_output = np.maximum(0, head_output)\n",
    "            \n",
    "            # Add to multi-head output\n",
    "            multi_head_output[:, h*head_dim:(h+1)*head_dim] = head_output\n",
    "        \n",
    "        return multi_head_output\n",
    "\n",
    "    def train(self, node_features, adjacency_lists, y, edge_features=None, validation_split=0.3, \n",
    "              batch_size=64, epochs=500, early_stopping=True, patience=10, reduce_lr=True):\n",
    "        \"\"\"\n",
    "        Trains the GAT model with validation and early stopping.\n",
    "        \n",
    "        Parameters:\n",
    "            node_features (np.array): Node feature array.\n",
    "            adjacency_lists (list): List of adjacency lists for each node.\n",
    "            y (np.array): Target array.\n",
    "            edge_features (np.array): Edge features for attention mechanism.\n",
    "            validation_split (float): Fraction of data to use for validation.\n",
    "            batch_size (int): Batch size for training.\n",
    "            epochs (int): Maximum number of epochs.\n",
    "            early_stopping (bool): Whether to use early stopping.\n",
    "            patience (int): Patience for early stopping.\n",
    "            reduce_lr (bool): Whether to reduce learning rate on plateau.\n",
    "            \n",
    "        Returns:\n",
    "            model (tf.keras.Model): Trained model.\n",
    "            history (dict): Training history.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model(input_dim=node_features.shape[1])\n",
    "        \n",
    "        # Create callbacks\n",
    "        callbacks_list = []\n",
    "        \n",
    "        if early_stopping:\n",
    "            early_stop = callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            callbacks_list.append(early_stop)\n",
    "        \n",
    "        if reduce_lr:\n",
    "            reduce_lr_callback = callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=patience // 2,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "            callbacks_list.append(reduce_lr_callback)\n",
    "        \n",
    "        # Add TensorBoard callback for visualization\n",
    "        tensorboard_callback = callbacks.TensorBoard(\n",
    "            log_dir=f\"./logs/cell_type_gat_{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "            histogram_freq=1\n",
    "        )\n",
    "        callbacks_list.append(tensorboard_callback)\n",
    "            # Add ModelCheckpoint callback to save the best model\n",
    "        model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath=f\"./models/cell_type_gat_best_{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}.keras\",\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "\n",
    "        callbacks_list.append(model_checkpoint_callback)\n",
    "        # Split data for validation\n",
    "        if validation_split > 0:\n",
    "            indices = np.arange(len(node_features))\n",
    "            train_indices, val_indices = train_test_split(\n",
    "                indices, \n",
    "                test_size=validation_split,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            \n",
    "            X_train = node_features[train_indices]\n",
    "            y_train = y[train_indices]\n",
    "            X_val = node_features[val_indices]\n",
    "            y_val = y[val_indices]\n",
    "            \n",
    "            # For validation, we need to extract corresponding adjacency lists\n",
    "            train_adj_lists = [adjacency_lists[i] for i in train_indices]\n",
    "            val_adj_lists = [adjacency_lists[i] for i in val_indices]\n",
    "            \n",
    "            # Extract edge features if available\n",
    "            train_edge_features = None\n",
    "            val_edge_features = None\n",
    "            if edge_features is not None:\n",
    "                train_edge_features = [edge_features[i] for i in train_indices]\n",
    "                val_edge_features = [edge_features[i] for i in val_indices]\n",
    "        else:\n",
    "            X_train = node_features\n",
    "            y_train = y\n",
    "            X_val = None\n",
    "            y_val = None\n",
    "            train_adj_lists = adjacency_lists\n",
    "            val_adj_lists = None\n",
    "            train_edge_features = edge_features\n",
    "            val_edge_features = None\n",
    "        \n",
    "        # Train the model with validation\n",
    "        print(\"Starting model training...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val) if X_val is not None else None,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Get final validation metrics\n",
    "        val_loss = min(self.history.history['val_loss'])\n",
    "        print(f\"Training complete. Best validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        return self.model, self.history\n",
    "\n",
    "    def predict(self, node_features):\n",
    "        \"\"\"\n",
    "        Makes predictions using the trained GAT model.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained.\")\n",
    "        predictions = self.model.predict(node_features)\n",
    "        return predictions\n",
    "\n",
    "    def create_submission(self, test_df, predictions, submission_filename=\"submission.csv\"):\n",
    "        \"\"\"\n",
    "        Creates a submission CSV file with spot IDs and predicted cell type abundances.\n",
    "        \"\"\"\n",
    "        pred_df = pd.DataFrame(predictions, columns=self.cell_type_columns, index=test_df.index)\n",
    "        pred_df.insert(0, 'ID', test_df.index)\n",
    "        pred_df.to_csv(submission_filename, index=False)\n",
    "        print(f\"Submission file '{submission_filename}' created!\")\n",
    "        return pred_df\n",
    "\n",
    "    def cross_validate(self, node_features, adjacency_lists, y, edge_features=None, n_splits=5, \n",
    "                        batch_size=64, epochs=500, early_stopping=True, patience=15):\n",
    "        \"\"\"\n",
    "        Performs K-Fold cross-validation with the GAT model.\n",
    "        \n",
    "        Parameters:\n",
    "            node_features (np.array): Node feature array.\n",
    "            adjacency_lists (list): List of adjacency lists.\n",
    "            y (np.array): Target array.\n",
    "            edge_features (np.array): Edge features for attention mechanism.\n",
    "            n_splits (int): Number of folds.\n",
    "            batch_size (int): Batch size for training.\n",
    "            epochs (int): Maximum number of epochs.\n",
    "            early_stopping (bool): Whether to use early stopping.\n",
    "            patience (int): Patience for early stopping.\n",
    "            \n",
    "        Returns:\n",
    "            results (dict): Dictionary with MSE scores, R² scores, and OOF predictions.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=self.random_state)\n",
    "        n_samples = node_features.shape[0]\n",
    "        n_targets = y.shape[1]\n",
    "        \n",
    "        oof_preds = np.zeros((n_samples, n_targets))\n",
    "        mse_scores = []\n",
    "        r2_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(node_features)):\n",
    "            print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "            \n",
    "            X_train_fold = node_features[train_idx]\n",
    "            y_train_fold = y[train_idx]\n",
    "            X_val_fold = node_features[val_idx]\n",
    "            y_val_fold = y[val_idx]\n",
    "            \n",
    "            # Extract corresponding adjacency lists and edge features\n",
    "            train_adj_lists = [adjacency_lists[i] for i in train_idx]\n",
    "            val_adj_lists = [adjacency_lists[i] for i in val_idx]\n",
    "            \n",
    "            train_edge_feats = None\n",
    "            val_edge_feats = None\n",
    "            if edge_features is not None:\n",
    "                train_edge_feats = [edge_features[i] for i in train_idx]\n",
    "                val_edge_feats = [edge_features[i] for i in val_idx]\n",
    "            \n",
    "            # Build a new model for this fold\n",
    "            input_dim = node_features.shape[1]\n",
    "            model = self.build_model(input_dim=input_dim)\n",
    "            \n",
    "            # Define callbacks for this fold\n",
    "            callbacks_list = []\n",
    "            if early_stopping:\n",
    "                early_stop = callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=patience,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=0\n",
    "                )\n",
    "                callbacks_list.append(early_stop)\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                validation_data=(X_val_fold, y_val_fold),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=callbacks_list,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            oof_preds[val_idx] = y_pred\n",
    "            \n",
    "            # Calculate metrics\n",
    "            fold_mse = mean_squared_error(y_val_fold, y_pred)\n",
    "            fold_r2 = r2_score(y_val_fold, y_pred)\n",
    "            \n",
    "            mse_scores.append(fold_mse)\n",
    "            r2_scores.append(fold_r2)\n",
    "            \n",
    "            print(f\"Fold {fold+1} MSE: {fold_mse:.4f}, R²: {fold_r2:.4f}\")\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        overall_mse = mean_squared_error(y, oof_preds)\n",
    "        overall_r2 = r2_score(y, oof_preds)\n",
    "        \n",
    "        print(f\"\\n=== Cross-Validation Results ===\")\n",
    "        print(f\"Average MSE: {np.mean(mse_scores):.4f} ± {np.std(mse_scores):.4f}\")\n",
    "        print(f\"Average R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "        print(f\"OOF MSE: {overall_mse:.4f}\")\n",
    "        print(f\"OOF R²: {overall_r2:.4f}\")\n",
    "        \n",
    "        results = {\n",
    "            'mse_scores': mse_scores,\n",
    "            'r2_scores': r2_scores,\n",
    "            'oof_predictions': oof_preds,\n",
    "            'overall_mse': overall_mse,\n",
    "            'overall_r2': overall_r2\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation loss curves.\n",
    "        \"\"\"\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "            \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        plt.plot(self.history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        \n",
    "        if 'mae' in self.history.history:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(self.history.history['mae'])\n",
    "            plt.plot(self.history.history['val_mae'])\n",
    "            plt.title('Model MAE')\n",
    "            plt.ylabel('Mean Absolute Error')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_graph(self, df, adjacency_lists, cell_type_index=None, max_nodes=100):\n",
    "        \"\"\"\n",
    "        Visualizes the constructed graph with nodes colored by cell type abundance.\n",
    "        \n",
    "        Parameters:\n",
    "            df (DataFrame): DataFrame containing spot data with 'x' and 'y' columns\n",
    "            adjacency_lists (list): List of adjacency lists for each node\n",
    "            cell_type_index (int): Index of cell type to use for coloring (optional)\n",
    "            max_nodes (int): Maximum number of nodes to plot\n",
    "        \"\"\"\n",
    "        # Check if we have too many nodes to visualize clearly\n",
    "        if len(df) > max_nodes:\n",
    "            print(f\"Too many nodes ({len(df)}) to visualize clearly. Sampling {max_nodes} nodes.\")\n",
    "            sample_indices = np.random.choice(len(df), max_nodes, replace=False)\n",
    "            df = df.iloc[sample_indices].copy()\n",
    "            adjacency_lists = [adjacency_lists[i] for i in sample_indices]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Extract positions\n",
    "        positions = df[['x', 'y']].values\n",
    "        \n",
    "        # Draw edges\n",
    "        for i, adj_list in enumerate(adjacency_lists):\n",
    "            for j, weight in adj_list:\n",
    "                # Skip if j is not in our sample\n",
    "                if j >= len(positions):\n",
    "                    continue\n",
    "                    \n",
    "                plt.plot([positions[i, 0], positions[j, 0]], \n",
    "                         [positions[i, 1], positions[j, 1]], \n",
    "                         'k-', alpha=0.1, linewidth=weight*3)\n",
    "        \n",
    "        # Determine node colors based on cell type abundance if provided\n",
    "        if cell_type_index is not None and cell_type_index < len(self.cell_type_columns):\n",
    "            cell_type = self.cell_type_columns[cell_type_index]\n",
    "            values = df[cell_type].values\n",
    "            plt.scatter(positions[:, 0], positions[:, 1], c=values, \n",
    "                        cmap='viridis', s=50, edgecolors='k')\n",
    "            plt.colorbar(label=f'{cell_type} Abundance')\n",
    "            plt.title(f'Graph Visualization: {cell_type} Abundance')\n",
    "        else:\n",
    "            plt.scatter(positions[:, 0], positions[:, 1], s=50, edgecolors='k')\n",
    "            plt.title('Graph Visualization')\n",
    "            \n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec642ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:08:01.818003Z",
     "iopub.status.busy": "2025-04-06T13:08:01.817550Z",
     "iopub.status.idle": "2025-04-06T13:19:38.489856Z",
     "shell.execute_reply": "2025-04-06T13:19:38.488877Z"
    },
    "papermill": {
     "duration": 696.676825,
     "end_time": "2025-04-06T13:19:38.491281",
     "exception": false,
     "start_time": "2025-04-06T13:08:01.814456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded successfully. Found 6 slides.\n",
      "Training set prepared with 8349 samples, 14 node features, and 35 target variables.\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Graph Attention Network model built.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Fold 1 MSE: 0.8633, R²: 0.1914\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Graph Attention Network model built.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Fold 2 MSE: 0.9859, R²: 0.1699\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Graph Attention Network model built.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Fold 3 MSE: 0.8635, R²: 0.1535\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Graph Attention Network model built.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Fold 4 MSE: 0.8451, R²: 0.1794\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Graph Attention Network model built.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Fold 5 MSE: 0.7723, R²: 0.1595\n",
      "\n",
      "=== Cross-Validation Results ===\n",
      "Average MSE: 0.8660 ± 0.0687\n",
      "Average R²: 0.1707 ± 0.0136\n",
      "OOF MSE: 0.8660\n",
      "OOF R²: 0.1722\n",
      "Cross-validation mean MSE: 0.8660\n",
      "Cross-validation mean R²: 0.1707\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Attention Network model built.\n",
      "Starting model training...\n",
      "Epoch 1/500\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 6.3469 - mae: 1.5913\n",
      "Epoch 1: val_loss improved from inf to 2.94184, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - loss: 6.3445 - mae: 1.5908 - val_loss: 2.9418 - val_mae: 0.3487 - learning_rate: 1.0000e-04\n",
      "Epoch 2/500\n",
      "\u001b[1m 90/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6385 - mae: 1.4405\n",
      "Epoch 2: val_loss improved from 2.94184 to 2.89738, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.6030 - mae: 1.4354 - val_loss: 2.8974 - val_mae: 0.3824 - learning_rate: 1.0000e-04\n",
      "Epoch 3/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0531 - mae: 1.3240\n",
      "Epoch 3: val_loss improved from 2.89738 to 2.82642, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.0452 - mae: 1.3226 - val_loss: 2.8264 - val_mae: 0.4155 - learning_rate: 1.0000e-04\n",
      "Epoch 4/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6687 - mae: 1.2464\n",
      "Epoch 4: val_loss improved from 2.82642 to 2.75366, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.6662 - mae: 1.2453 - val_loss: 2.7537 - val_mae: 0.4185 - learning_rate: 1.0000e-04\n",
      "Epoch 5/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4332 - mae: 1.1783\n",
      "Epoch 5: val_loss improved from 2.75366 to 2.66121, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.4294 - mae: 1.1774 - val_loss: 2.6612 - val_mae: 0.4084 - learning_rate: 1.0000e-04\n",
      "Epoch 6/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1043 - mae: 1.1182\n",
      "Epoch 6: val_loss improved from 2.66121 to 2.63004, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.1066 - mae: 1.1174 - val_loss: 2.6300 - val_mae: 0.4000 - learning_rate: 1.0000e-04\n",
      "Epoch 7/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9730 - mae: 1.0630\n",
      "Epoch 7: val_loss improved from 2.63004 to 2.58211, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.9709 - mae: 1.0624 - val_loss: 2.5821 - val_mae: 0.3939 - learning_rate: 1.0000e-04\n",
      "Epoch 8/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7814 - mae: 1.0191\n",
      "Epoch 8: val_loss improved from 2.58211 to 2.54933, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.7822 - mae: 1.0186 - val_loss: 2.5493 - val_mae: 0.3891 - learning_rate: 1.0000e-04\n",
      "Epoch 9/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7826 - mae: 0.9815\n",
      "Epoch 9: val_loss improved from 2.54933 to 2.52457, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.7742 - mae: 0.9808 - val_loss: 2.5246 - val_mae: 0.3796 - learning_rate: 1.0000e-04\n",
      "Epoch 10/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5549 - mae: 0.9415\n",
      "Epoch 10: val_loss improved from 2.52457 to 2.49905, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.5526 - mae: 0.9409 - val_loss: 2.4990 - val_mae: 0.3725 - learning_rate: 1.0000e-04\n",
      "Epoch 11/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3894 - mae: 0.9019\n",
      "Epoch 11: val_loss improved from 2.49905 to 2.48726, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.3913 - mae: 0.9014 - val_loss: 2.4873 - val_mae: 0.3663 - learning_rate: 1.0000e-04\n",
      "Epoch 12/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3138 - mae: 0.8727\n",
      "Epoch 12: val_loss improved from 2.48726 to 2.45657, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.3135 - mae: 0.8721 - val_loss: 2.4566 - val_mae: 0.3635 - learning_rate: 1.0000e-04\n",
      "Epoch 13/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1665 - mae: 0.8361\n",
      "Epoch 13: val_loss improved from 2.45657 to 2.43934, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.1717 - mae: 0.8361 - val_loss: 2.4393 - val_mae: 0.3565 - learning_rate: 1.0000e-04\n",
      "Epoch 14/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1710 - mae: 0.8104\n",
      "Epoch 14: val_loss improved from 2.43934 to 2.42141, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.1697 - mae: 0.8099 - val_loss: 2.4214 - val_mae: 0.3555 - learning_rate: 1.0000e-04\n",
      "Epoch 15/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0537 - mae: 0.7785\n",
      "Epoch 15: val_loss improved from 2.42141 to 2.40789, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.0550 - mae: 0.7783 - val_loss: 2.4079 - val_mae: 0.3519 - learning_rate: 1.0000e-04\n",
      "Epoch 16/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0218 - mae: 0.7577\n",
      "Epoch 16: val_loss improved from 2.40789 to 2.39589, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.0206 - mae: 0.7572 - val_loss: 2.3959 - val_mae: 0.3503 - learning_rate: 1.0000e-04\n",
      "Epoch 17/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9793 - mae: 0.7302\n",
      "Epoch 17: val_loss improved from 2.39589 to 2.38093, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.9769 - mae: 0.7298 - val_loss: 2.3809 - val_mae: 0.3493 - learning_rate: 1.0000e-04\n",
      "Epoch 18/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8139 - mae: 0.6986\n",
      "Epoch 18: val_loss improved from 2.38093 to 2.36537, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.8168 - mae: 0.6986 - val_loss: 2.3654 - val_mae: 0.3413 - learning_rate: 1.0000e-04\n",
      "Epoch 19/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8173 - mae: 0.6829\n",
      "Epoch 19: val_loss improved from 2.36537 to 2.36027, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.8174 - mae: 0.6827 - val_loss: 2.3603 - val_mae: 0.3415 - learning_rate: 1.0000e-04\n",
      "Epoch 20/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7327 - mae: 0.6569\n",
      "Epoch 20: val_loss improved from 2.36027 to 2.34835, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.7343 - mae: 0.6567 - val_loss: 2.3483 - val_mae: 0.3324 - learning_rate: 1.0000e-04\n",
      "Epoch 21/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7078 - mae: 0.6357\n",
      "Epoch 21: val_loss improved from 2.34835 to 2.33108, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.7081 - mae: 0.6353 - val_loss: 2.3311 - val_mae: 0.3260 - learning_rate: 1.0000e-04\n",
      "Epoch 22/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7334 - mae: 0.6216\n",
      "Epoch 22: val_loss improved from 2.33108 to 2.31874, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.7297 - mae: 0.6211 - val_loss: 2.3187 - val_mae: 0.3252 - learning_rate: 1.0000e-04\n",
      "Epoch 23/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6392 - mae: 0.5953\n",
      "Epoch 23: val_loss improved from 2.31874 to 2.31390, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6396 - mae: 0.5952 - val_loss: 2.3139 - val_mae: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 24/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6233 - mae: 0.5781\n",
      "Epoch 24: val_loss improved from 2.31390 to 2.29366, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6215 - mae: 0.5777 - val_loss: 2.2937 - val_mae: 0.3234 - learning_rate: 1.0000e-04\n",
      "Epoch 25/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6347 - mae: 0.5633\n",
      "Epoch 25: val_loss improved from 2.29366 to 2.29205, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6304 - mae: 0.5629 - val_loss: 2.2921 - val_mae: 0.3239 - learning_rate: 1.0000e-04\n",
      "Epoch 26/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5215 - mae: 0.5460\n",
      "Epoch 26: val_loss improved from 2.29205 to 2.28150, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5220 - mae: 0.5456 - val_loss: 2.2815 - val_mae: 0.3159 - learning_rate: 1.0000e-04\n",
      "Epoch 27/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5588 - mae: 0.5341\n",
      "Epoch 27: val_loss improved from 2.28150 to 2.26912, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5543 - mae: 0.5336 - val_loss: 2.2691 - val_mae: 0.3167 - learning_rate: 1.0000e-04\n",
      "Epoch 28/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4831 - mae: 0.5140\n",
      "Epoch 28: val_loss improved from 2.26912 to 2.24908, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4822 - mae: 0.5137 - val_loss: 2.2491 - val_mae: 0.3127 - learning_rate: 1.0000e-04\n",
      "Epoch 29/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4488 - mae: 0.4966\n",
      "Epoch 29: val_loss improved from 2.24908 to 2.23799, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4468 - mae: 0.4964 - val_loss: 2.2380 - val_mae: 0.3092 - learning_rate: 1.0000e-04\n",
      "Epoch 30/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3704 - mae: 0.4799\n",
      "Epoch 30: val_loss improved from 2.23799 to 2.22517, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.3710 - mae: 0.4799 - val_loss: 2.2252 - val_mae: 0.3119 - learning_rate: 1.0000e-04\n",
      "Epoch 31/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4975 - mae: 0.4756\n",
      "Epoch 31: val_loss improved from 2.22517 to 2.21292, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4896 - mae: 0.4751 - val_loss: 2.2129 - val_mae: 0.3136 - learning_rate: 1.0000e-04\n",
      "Epoch 32/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3467 - mae: 0.4566\n",
      "Epoch 32: val_loss improved from 2.21292 to 2.19894, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.3471 - mae: 0.4565 - val_loss: 2.1989 - val_mae: 0.3052 - learning_rate: 1.0000e-04\n",
      "Epoch 33/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3232 - mae: 0.4453\n",
      "Epoch 33: val_loss did not improve from 2.19894\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.3239 - mae: 0.4453 - val_loss: 2.1995 - val_mae: 0.3095 - learning_rate: 1.0000e-04\n",
      "Epoch 34/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3601 - mae: 0.4399\n",
      "Epoch 34: val_loss improved from 2.19894 to 2.17924, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.3570 - mae: 0.4394 - val_loss: 2.1792 - val_mae: 0.3077 - learning_rate: 1.0000e-04\n",
      "Epoch 35/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2900 - mae: 0.4218\n",
      "Epoch 35: val_loss improved from 2.17924 to 2.15782, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.2899 - mae: 0.4218 - val_loss: 2.1578 - val_mae: 0.3056 - learning_rate: 1.0000e-04\n",
      "Epoch 36/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2772 - mae: 0.4170\n",
      "Epoch 36: val_loss improved from 2.15782 to 2.14914, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.2768 - mae: 0.4169 - val_loss: 2.1491 - val_mae: 0.3015 - learning_rate: 1.0000e-04\n",
      "Epoch 37/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2718 - mae: 0.4075\n",
      "Epoch 37: val_loss improved from 2.14914 to 2.14017, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.2703 - mae: 0.4073 - val_loss: 2.1402 - val_mae: 0.3006 - learning_rate: 1.0000e-04\n",
      "Epoch 38/500\n",
      "\u001b[1m103/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1887 - mae: 0.3939\n",
      "Epoch 38: val_loss improved from 2.14017 to 2.12279, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.1893 - mae: 0.3939 - val_loss: 2.1228 - val_mae: 0.2995 - learning_rate: 1.0000e-04\n",
      "Epoch 39/500\n",
      "\u001b[1m 90/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1768 - mae: 0.3887\n",
      "Epoch 39: val_loss improved from 2.12279 to 2.11788, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.1787 - mae: 0.3885 - val_loss: 2.1179 - val_mae: 0.3007 - learning_rate: 1.0000e-04\n",
      "Epoch 40/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2380 - mae: 0.3834\n",
      "Epoch 40: val_loss improved from 2.11788 to 2.09694, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.2330 - mae: 0.3830 - val_loss: 2.0969 - val_mae: 0.2988 - learning_rate: 1.0000e-04\n",
      "Epoch 41/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2102 - mae: 0.3751\n",
      "Epoch 41: val_loss improved from 2.09694 to 2.08139, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.2074 - mae: 0.3750 - val_loss: 2.0814 - val_mae: 0.2970 - learning_rate: 1.0000e-04\n",
      "Epoch 42/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1840 - mae: 0.3653\n",
      "Epoch 42: val_loss improved from 2.08139 to 2.07995, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.1825 - mae: 0.3653 - val_loss: 2.0800 - val_mae: 0.3024 - learning_rate: 1.0000e-04\n",
      "Epoch 43/500\n",
      "\u001b[1m 93/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1924 - mae: 0.3633\n",
      "Epoch 43: val_loss improved from 2.07995 to 2.06195, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.1837 - mae: 0.3628 - val_loss: 2.0620 - val_mae: 0.2995 - learning_rate: 1.0000e-04\n",
      "Epoch 44/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1783 - mae: 0.3638\n",
      "Epoch 44: val_loss improved from 2.06195 to 2.04894, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.1739 - mae: 0.3632 - val_loss: 2.0489 - val_mae: 0.2984 - learning_rate: 1.0000e-04\n",
      "Epoch 45/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0575 - mae: 0.3491\n",
      "Epoch 45: val_loss improved from 2.04894 to 2.03269, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.0595 - mae: 0.3490 - val_loss: 2.0327 - val_mae: 0.2970 - learning_rate: 1.0000e-04\n",
      "Epoch 46/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0897 - mae: 0.3486\n",
      "Epoch 46: val_loss improved from 2.03269 to 2.02093, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.0890 - mae: 0.3484 - val_loss: 2.0209 - val_mae: 0.2943 - learning_rate: 1.0000e-04\n",
      "Epoch 47/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0852 - mae: 0.3376\n",
      "Epoch 47: val_loss improved from 2.02093 to 2.01792, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.0833 - mae: 0.3379 - val_loss: 2.0179 - val_mae: 0.2986 - learning_rate: 1.0000e-04\n",
      "Epoch 48/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1027 - mae: 0.3383\n",
      "Epoch 48: val_loss improved from 2.01792 to 2.00975, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.1012 - mae: 0.3383 - val_loss: 2.0097 - val_mae: 0.2968 - learning_rate: 1.0000e-04\n",
      "Epoch 49/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0587 - mae: 0.3387\n",
      "Epoch 49: val_loss improved from 2.00975 to 1.99463, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.0570 - mae: 0.3385 - val_loss: 1.9946 - val_mae: 0.2990 - learning_rate: 1.0000e-04\n",
      "Epoch 50/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0078 - mae: 0.3300\n",
      "Epoch 50: val_loss improved from 1.99463 to 1.96955, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.0089 - mae: 0.3300 - val_loss: 1.9696 - val_mae: 0.2939 - learning_rate: 1.0000e-04\n",
      "Epoch 51/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0141 - mae: 0.3294\n",
      "Epoch 51: val_loss improved from 1.96955 to 1.95372, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.0140 - mae: 0.3294 - val_loss: 1.9537 - val_mae: 0.2920 - learning_rate: 1.0000e-04\n",
      "Epoch 52/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9849 - mae: 0.3257\n",
      "Epoch 52: val_loss did not improve from 1.95372\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9849 - mae: 0.3257 - val_loss: 1.9559 - val_mae: 0.2981 - learning_rate: 1.0000e-04\n",
      "Epoch 53/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0070 - mae: 0.3264\n",
      "Epoch 53: val_loss improved from 1.95372 to 1.92781, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.0048 - mae: 0.3263 - val_loss: 1.9278 - val_mae: 0.2928 - learning_rate: 1.0000e-04\n",
      "Epoch 54/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0264 - mae: 0.3253\n",
      "Epoch 54: val_loss improved from 1.92781 to 1.91980, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.0234 - mae: 0.3253 - val_loss: 1.9198 - val_mae: 0.2993 - learning_rate: 1.0000e-04\n",
      "Epoch 55/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9491 - mae: 0.3224\n",
      "Epoch 55: val_loss improved from 1.91980 to 1.91033, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.9495 - mae: 0.3223 - val_loss: 1.9103 - val_mae: 0.2949 - learning_rate: 1.0000e-04\n",
      "Epoch 56/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9401 - mae: 0.3196\n",
      "Epoch 56: val_loss improved from 1.91033 to 1.88780, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.9404 - mae: 0.3197 - val_loss: 1.8878 - val_mae: 0.2941 - learning_rate: 1.0000e-04\n",
      "Epoch 57/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9961 - mae: 0.3210\n",
      "Epoch 57: val_loss improved from 1.88780 to 1.88231, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.9921 - mae: 0.3208 - val_loss: 1.8823 - val_mae: 0.2928 - learning_rate: 1.0000e-04\n",
      "Epoch 58/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8872 - mae: 0.3175\n",
      "Epoch 58: val_loss improved from 1.88231 to 1.87204, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8885 - mae: 0.3175 - val_loss: 1.8720 - val_mae: 0.2938 - learning_rate: 1.0000e-04\n",
      "Epoch 59/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8858 - mae: 0.3178\n",
      "Epoch 59: val_loss improved from 1.87204 to 1.85370, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8863 - mae: 0.3177 - val_loss: 1.8537 - val_mae: 0.2907 - learning_rate: 1.0000e-04\n",
      "Epoch 60/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8863 - mae: 0.3165\n",
      "Epoch 60: val_loss improved from 1.85370 to 1.84164, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8863 - mae: 0.3164 - val_loss: 1.8416 - val_mae: 0.2932 - learning_rate: 1.0000e-04\n",
      "Epoch 61/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8503 - mae: 0.3129\n",
      "Epoch 61: val_loss improved from 1.84164 to 1.84082, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8518 - mae: 0.3131 - val_loss: 1.8408 - val_mae: 0.2940 - learning_rate: 1.0000e-04\n",
      "Epoch 62/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8430 - mae: 0.3138\n",
      "Epoch 62: val_loss improved from 1.84082 to 1.82471, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8442 - mae: 0.3140 - val_loss: 1.8247 - val_mae: 0.2921 - learning_rate: 1.0000e-04\n",
      "Epoch 63/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9095 - mae: 0.3211\n",
      "Epoch 63: val_loss improved from 1.82471 to 1.80899, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.9065 - mae: 0.3208 - val_loss: 1.8090 - val_mae: 0.2891 - learning_rate: 1.0000e-04\n",
      "Epoch 64/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8358 - mae: 0.3148\n",
      "Epoch 64: val_loss improved from 1.80899 to 1.80045, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8362 - mae: 0.3148 - val_loss: 1.8004 - val_mae: 0.2924 - learning_rate: 1.0000e-04\n",
      "Epoch 65/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8927 - mae: 0.3172\n",
      "Epoch 65: val_loss improved from 1.80045 to 1.77186, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8894 - mae: 0.3170 - val_loss: 1.7719 - val_mae: 0.2891 - learning_rate: 1.0000e-04\n",
      "Epoch 66/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8643 - mae: 0.3201\n",
      "Epoch 66: val_loss did not improve from 1.77186\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8608 - mae: 0.3198 - val_loss: 1.7752 - val_mae: 0.2887 - learning_rate: 1.0000e-04\n",
      "Epoch 67/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7957 - mae: 0.3124\n",
      "Epoch 67: val_loss improved from 1.77186 to 1.76159, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7957 - mae: 0.3124 - val_loss: 1.7616 - val_mae: 0.2866 - learning_rate: 1.0000e-04\n",
      "Epoch 68/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8164 - mae: 0.3138\n",
      "Epoch 68: val_loss improved from 1.76159 to 1.74513, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8132 - mae: 0.3137 - val_loss: 1.7451 - val_mae: 0.2892 - learning_rate: 1.0000e-04\n",
      "Epoch 69/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7917 - mae: 0.3121\n",
      "Epoch 69: val_loss improved from 1.74513 to 1.73604, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7920 - mae: 0.3122 - val_loss: 1.7360 - val_mae: 0.2879 - learning_rate: 1.0000e-04\n",
      "Epoch 70/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7754 - mae: 0.3108\n",
      "Epoch 70: val_loss improved from 1.73604 to 1.72993, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7754 - mae: 0.3108 - val_loss: 1.7299 - val_mae: 0.2918 - learning_rate: 1.0000e-04\n",
      "Epoch 71/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7041 - mae: 0.3078\n",
      "Epoch 71: val_loss improved from 1.72993 to 1.70879, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7064 - mae: 0.3079 - val_loss: 1.7088 - val_mae: 0.2877 - learning_rate: 1.0000e-04\n",
      "Epoch 72/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6998 - mae: 0.3084\n",
      "Epoch 72: val_loss improved from 1.70879 to 1.70847, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7015 - mae: 0.3086 - val_loss: 1.7085 - val_mae: 0.2870 - learning_rate: 1.0000e-04\n",
      "Epoch 73/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6678 - mae: 0.3084\n",
      "Epoch 73: val_loss improved from 1.70847 to 1.69087, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6700 - mae: 0.3085 - val_loss: 1.6909 - val_mae: 0.2871 - learning_rate: 1.0000e-04\n",
      "Epoch 74/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7391 - mae: 0.3090\n",
      "Epoch 74: val_loss did not improve from 1.69087\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7386 - mae: 0.3091 - val_loss: 1.6950 - val_mae: 0.2903 - learning_rate: 1.0000e-04\n",
      "Epoch 75/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7633 - mae: 0.3138\n",
      "Epoch 75: val_loss improved from 1.69087 to 1.68812, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7622 - mae: 0.3137 - val_loss: 1.6881 - val_mae: 0.2884 - learning_rate: 1.0000e-04\n",
      "Epoch 76/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7090 - mae: 0.3075\n",
      "Epoch 76: val_loss improved from 1.68812 to 1.67520, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7086 - mae: 0.3076 - val_loss: 1.6752 - val_mae: 0.2883 - learning_rate: 1.0000e-04\n",
      "Epoch 77/500\n",
      "\u001b[1m103/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6845 - mae: 0.3088\n",
      "Epoch 77: val_loss improved from 1.67520 to 1.65740, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6846 - mae: 0.3088 - val_loss: 1.6574 - val_mae: 0.2851 - learning_rate: 1.0000e-04\n",
      "Epoch 78/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7118 - mae: 0.3107\n",
      "Epoch 78: val_loss improved from 1.65740 to 1.65007, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7103 - mae: 0.3106 - val_loss: 1.6501 - val_mae: 0.2877 - learning_rate: 1.0000e-04\n",
      "Epoch 79/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7175 - mae: 0.3123\n",
      "Epoch 79: val_loss improved from 1.65007 to 1.64701, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7168 - mae: 0.3122 - val_loss: 1.6470 - val_mae: 0.2878 - learning_rate: 1.0000e-04\n",
      "Epoch 80/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6726 - mae: 0.3102\n",
      "Epoch 80: val_loss improved from 1.64701 to 1.62621, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6722 - mae: 0.3102 - val_loss: 1.6262 - val_mae: 0.2856 - learning_rate: 1.0000e-04\n",
      "Epoch 81/500\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6751 - mae: 0.3090\n",
      "Epoch 81: val_loss improved from 1.62621 to 1.61955, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.6750 - mae: 0.3090 - val_loss: 1.6195 - val_mae: 0.2841 - learning_rate: 1.0000e-04\n",
      "Epoch 82/500\n",
      "\u001b[1m 93/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6402 - mae: 0.3072\n",
      "Epoch 82: val_loss improved from 1.61955 to 1.60893, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6421 - mae: 0.3074 - val_loss: 1.6089 - val_mae: 0.2825 - learning_rate: 1.0000e-04\n",
      "Epoch 83/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5922 - mae: 0.3061\n",
      "Epoch 83: val_loss improved from 1.60893 to 1.59607, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5965 - mae: 0.3062 - val_loss: 1.5961 - val_mae: 0.2851 - learning_rate: 1.0000e-04\n",
      "Epoch 84/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6398 - mae: 0.3083\n",
      "Epoch 84: val_loss improved from 1.59607 to 1.58655, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6393 - mae: 0.3082 - val_loss: 1.5865 - val_mae: 0.2825 - learning_rate: 1.0000e-04\n",
      "Epoch 85/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5978 - mae: 0.3056\n",
      "Epoch 85: val_loss improved from 1.58655 to 1.58047, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5999 - mae: 0.3058 - val_loss: 1.5805 - val_mae: 0.2862 - learning_rate: 1.0000e-04\n",
      "Epoch 86/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5976 - mae: 0.3092\n",
      "Epoch 86: val_loss improved from 1.58047 to 1.56695, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5984 - mae: 0.3091 - val_loss: 1.5670 - val_mae: 0.2849 - learning_rate: 1.0000e-04\n",
      "Epoch 87/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6102 - mae: 0.3049\n",
      "Epoch 87: val_loss improved from 1.56695 to 1.55390, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6100 - mae: 0.3051 - val_loss: 1.5539 - val_mae: 0.2821 - learning_rate: 1.0000e-04\n",
      "Epoch 88/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6492 - mae: 0.3103\n",
      "Epoch 88: val_loss improved from 1.55390 to 1.54708, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6470 - mae: 0.3102 - val_loss: 1.5471 - val_mae: 0.2826 - learning_rate: 1.0000e-04\n",
      "Epoch 89/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5903 - mae: 0.3081\n",
      "Epoch 89: val_loss improved from 1.54708 to 1.54687, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5906 - mae: 0.3081 - val_loss: 1.5469 - val_mae: 0.2831 - learning_rate: 1.0000e-04\n",
      "Epoch 90/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6334 - mae: 0.3149\n",
      "Epoch 90: val_loss improved from 1.54687 to 1.53428, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6310 - mae: 0.3146 - val_loss: 1.5343 - val_mae: 0.2824 - learning_rate: 1.0000e-04\n",
      "Epoch 91/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5733 - mae: 0.3057\n",
      "Epoch 91: val_loss did not improve from 1.53428\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.5740 - mae: 0.3058 - val_loss: 1.5363 - val_mae: 0.2849 - learning_rate: 1.0000e-04\n",
      "Epoch 92/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5374 - mae: 0.3034\n",
      "Epoch 92: val_loss improved from 1.53428 to 1.52049, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5386 - mae: 0.3035 - val_loss: 1.5205 - val_mae: 0.2838 - learning_rate: 1.0000e-04\n",
      "Epoch 93/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6195 - mae: 0.3105\n",
      "Epoch 93: val_loss did not improve from 1.52049\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6152 - mae: 0.3103 - val_loss: 1.5281 - val_mae: 0.2831 - learning_rate: 1.0000e-04\n",
      "Epoch 94/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6057 - mae: 0.3097\n",
      "Epoch 94: val_loss improved from 1.52049 to 1.51113, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6017 - mae: 0.3095 - val_loss: 1.5111 - val_mae: 0.2806 - learning_rate: 1.0000e-04\n",
      "Epoch 95/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5742 - mae: 0.3051\n",
      "Epoch 95: val_loss improved from 1.51113 to 1.50884, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5725 - mae: 0.3051 - val_loss: 1.5088 - val_mae: 0.2826 - learning_rate: 1.0000e-04\n",
      "Epoch 96/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4855 - mae: 0.3035\n",
      "Epoch 96: val_loss improved from 1.50884 to 1.48892, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4886 - mae: 0.3036 - val_loss: 1.4889 - val_mae: 0.2785 - learning_rate: 1.0000e-04\n",
      "Epoch 97/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5648 - mae: 0.3079\n",
      "Epoch 97: val_loss improved from 1.48892 to 1.48279, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5615 - mae: 0.3077 - val_loss: 1.4828 - val_mae: 0.2784 - learning_rate: 1.0000e-04\n",
      "Epoch 98/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4861 - mae: 0.3028\n",
      "Epoch 98: val_loss improved from 1.48279 to 1.47784, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4882 - mae: 0.3029 - val_loss: 1.4778 - val_mae: 0.2797 - learning_rate: 1.0000e-04\n",
      "Epoch 99/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5284 - mae: 0.3064\n",
      "Epoch 99: val_loss improved from 1.47784 to 1.46394, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5284 - mae: 0.3064 - val_loss: 1.4639 - val_mae: 0.2815 - learning_rate: 1.0000e-04\n",
      "Epoch 100/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5324 - mae: 0.3055\n",
      "Epoch 100: val_loss improved from 1.46394 to 1.45785, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5321 - mae: 0.3055 - val_loss: 1.4579 - val_mae: 0.2803 - learning_rate: 1.0000e-04\n",
      "Epoch 101/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4836 - mae: 0.3057\n",
      "Epoch 101: val_loss improved from 1.45785 to 1.45441, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4849 - mae: 0.3057 - val_loss: 1.4544 - val_mae: 0.2808 - learning_rate: 1.0000e-04\n",
      "Epoch 102/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5087 - mae: 0.3034\n",
      "Epoch 102: val_loss improved from 1.45441 to 1.43532, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5081 - mae: 0.3035 - val_loss: 1.4353 - val_mae: 0.2785 - learning_rate: 1.0000e-04\n",
      "Epoch 103/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6044 - mae: 0.3076\n",
      "Epoch 103: val_loss did not improve from 1.43532\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.5977 - mae: 0.3074 - val_loss: 1.4426 - val_mae: 0.2805 - learning_rate: 1.0000e-04\n",
      "Epoch 104/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4154 - mae: 0.3018\n",
      "Epoch 104: val_loss improved from 1.43532 to 1.42728, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4182 - mae: 0.3019 - val_loss: 1.4273 - val_mae: 0.2799 - learning_rate: 1.0000e-04\n",
      "Epoch 105/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4687 - mae: 0.3025\n",
      "Epoch 105: val_loss improved from 1.42728 to 1.42057, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4691 - mae: 0.3026 - val_loss: 1.4206 - val_mae: 0.2806 - learning_rate: 1.0000e-04\n",
      "Epoch 106/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4562 - mae: 0.3028\n",
      "Epoch 106: val_loss improved from 1.42057 to 1.41802, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4573 - mae: 0.3029 - val_loss: 1.4180 - val_mae: 0.2810 - learning_rate: 1.0000e-04\n",
      "Epoch 107/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3891 - mae: 0.3013\n",
      "Epoch 107: val_loss improved from 1.41802 to 1.41265, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3931 - mae: 0.3013 - val_loss: 1.4126 - val_mae: 0.2818 - learning_rate: 1.0000e-04\n",
      "Epoch 108/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4353 - mae: 0.3016\n",
      "Epoch 108: val_loss improved from 1.41265 to 1.39396, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4366 - mae: 0.3017 - val_loss: 1.3940 - val_mae: 0.2763 - learning_rate: 1.0000e-04\n",
      "Epoch 109/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5229 - mae: 0.3078\n",
      "Epoch 109: val_loss improved from 1.39396 to 1.38843, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.5178 - mae: 0.3076 - val_loss: 1.3884 - val_mae: 0.2812 - learning_rate: 1.0000e-04\n",
      "Epoch 110/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4724 - mae: 0.3061\n",
      "Epoch 110: val_loss did not improve from 1.38843\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.4708 - mae: 0.3059 - val_loss: 1.3976 - val_mae: 0.2769 - learning_rate: 1.0000e-04\n",
      "Epoch 111/500\n",
      "\u001b[1m 95/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4973 - mae: 0.3047\n",
      "Epoch 111: val_loss did not improve from 1.38843\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.4899 - mae: 0.3045 - val_loss: 1.3949 - val_mae: 0.2758 - learning_rate: 1.0000e-04\n",
      "Epoch 112/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4325 - mae: 0.3022\n",
      "Epoch 112: val_loss improved from 1.38843 to 1.38183, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4326 - mae: 0.3022 - val_loss: 1.3818 - val_mae: 0.2789 - learning_rate: 1.0000e-04\n",
      "Epoch 113/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4005 - mae: 0.3026\n",
      "Epoch 113: val_loss improved from 1.38183 to 1.37301, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4030 - mae: 0.3026 - val_loss: 1.3730 - val_mae: 0.2764 - learning_rate: 1.0000e-04\n",
      "Epoch 114/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3955 - mae: 0.3001\n",
      "Epoch 114: val_loss improved from 1.37301 to 1.36965, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3980 - mae: 0.3002 - val_loss: 1.3697 - val_mae: 0.2764 - learning_rate: 1.0000e-04\n",
      "Epoch 115/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4360 - mae: 0.3043\n",
      "Epoch 115: val_loss improved from 1.36965 to 1.36075, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4348 - mae: 0.3042 - val_loss: 1.3608 - val_mae: 0.2759 - learning_rate: 1.0000e-04\n",
      "Epoch 116/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3483 - mae: 0.2979\n",
      "Epoch 116: val_loss improved from 1.36075 to 1.33997, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3530 - mae: 0.2982 - val_loss: 1.3400 - val_mae: 0.2804 - learning_rate: 1.0000e-04\n",
      "Epoch 117/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4102 - mae: 0.3060\n",
      "Epoch 117: val_loss did not improve from 1.33997\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.4102 - mae: 0.3059 - val_loss: 1.3492 - val_mae: 0.2749 - learning_rate: 1.0000e-04\n",
      "Epoch 118/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4216 - mae: 0.3037\n",
      "Epoch 118: val_loss did not improve from 1.33997\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.4214 - mae: 0.3036 - val_loss: 1.3465 - val_mae: 0.2768 - learning_rate: 1.0000e-04\n",
      "Epoch 119/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4024 - mae: 0.3024\n",
      "Epoch 119: val_loss improved from 1.33997 to 1.33496, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4021 - mae: 0.3024 - val_loss: 1.3350 - val_mae: 0.2765 - learning_rate: 1.0000e-04\n",
      "Epoch 120/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4356 - mae: 0.3032\n",
      "Epoch 120: val_loss improved from 1.33496 to 1.32606, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4340 - mae: 0.3032 - val_loss: 1.3261 - val_mae: 0.2742 - learning_rate: 1.0000e-04\n",
      "Epoch 121/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3894 - mae: 0.3041\n",
      "Epoch 121: val_loss improved from 1.32606 to 1.31531, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3896 - mae: 0.3039 - val_loss: 1.3153 - val_mae: 0.2752 - learning_rate: 1.0000e-04\n",
      "Epoch 122/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4959 - mae: 0.3085\n",
      "Epoch 122: val_loss did not improve from 1.31531\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.4873 - mae: 0.3080 - val_loss: 1.3181 - val_mae: 0.2732 - learning_rate: 1.0000e-04\n",
      "Epoch 123/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4246 - mae: 0.3021\n",
      "Epoch 123: val_loss improved from 1.31531 to 1.31377, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4212 - mae: 0.3020 - val_loss: 1.3138 - val_mae: 0.2749 - learning_rate: 1.0000e-04\n",
      "Epoch 124/500\n",
      "\u001b[1m 93/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3678 - mae: 0.3008\n",
      "Epoch 124: val_loss did not improve from 1.31377\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3671 - mae: 0.3007 - val_loss: 1.3139 - val_mae: 0.2744 - learning_rate: 1.0000e-04\n",
      "Epoch 125/500\n",
      "\u001b[1m 92/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2787 - mae: 0.2958\n",
      "Epoch 125: val_loss improved from 1.31377 to 1.30634, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2881 - mae: 0.2963 - val_loss: 1.3063 - val_mae: 0.2749 - learning_rate: 1.0000e-04\n",
      "Epoch 126/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3382 - mae: 0.2981\n",
      "Epoch 126: val_loss improved from 1.30634 to 1.30560, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3404 - mae: 0.2983 - val_loss: 1.3056 - val_mae: 0.2778 - learning_rate: 1.0000e-04\n",
      "Epoch 127/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3042 - mae: 0.2944\n",
      "Epoch 127: val_loss improved from 1.30560 to 1.29684, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3087 - mae: 0.2949 - val_loss: 1.2968 - val_mae: 0.2731 - learning_rate: 1.0000e-04\n",
      "Epoch 128/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3494 - mae: 0.3018\n",
      "Epoch 128: val_loss did not improve from 1.29684\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3495 - mae: 0.3017 - val_loss: 1.3037 - val_mae: 0.2754 - learning_rate: 1.0000e-04\n",
      "Epoch 129/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3713 - mae: 0.3003\n",
      "Epoch 129: val_loss improved from 1.29684 to 1.28975, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3684 - mae: 0.3001 - val_loss: 1.2898 - val_mae: 0.2777 - learning_rate: 1.0000e-04\n",
      "Epoch 130/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3616 - mae: 0.3061\n",
      "Epoch 130: val_loss improved from 1.28975 to 1.28598, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3601 - mae: 0.3058 - val_loss: 1.2860 - val_mae: 0.2751 - learning_rate: 1.0000e-04\n",
      "Epoch 131/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3836 - mae: 0.3025\n",
      "Epoch 131: val_loss improved from 1.28598 to 1.28166, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3799 - mae: 0.3024 - val_loss: 1.2817 - val_mae: 0.2713 - learning_rate: 1.0000e-04\n",
      "Epoch 132/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3315 - mae: 0.2954\n",
      "Epoch 132: val_loss did not improve from 1.28166\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3323 - mae: 0.2957 - val_loss: 1.2858 - val_mae: 0.2788 - learning_rate: 1.0000e-04\n",
      "Epoch 133/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2891 - mae: 0.2982\n",
      "Epoch 133: val_loss improved from 1.28166 to 1.26947, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2918 - mae: 0.2982 - val_loss: 1.2695 - val_mae: 0.2756 - learning_rate: 1.0000e-04\n",
      "Epoch 134/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3110 - mae: 0.3004\n",
      "Epoch 134: val_loss improved from 1.26947 to 1.26006, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3116 - mae: 0.3004 - val_loss: 1.2601 - val_mae: 0.2714 - learning_rate: 1.0000e-04\n",
      "Epoch 135/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3124 - mae: 0.2974\n",
      "Epoch 135: val_loss did not improve from 1.26006\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3130 - mae: 0.2974 - val_loss: 1.2605 - val_mae: 0.2717 - learning_rate: 1.0000e-04\n",
      "Epoch 136/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2951 - mae: 0.2991\n",
      "Epoch 136: val_loss did not improve from 1.26006\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2955 - mae: 0.2992 - val_loss: 1.2670 - val_mae: 0.2737 - learning_rate: 1.0000e-04\n",
      "Epoch 137/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2953 - mae: 0.2971\n",
      "Epoch 137: val_loss improved from 1.26006 to 1.24507, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2958 - mae: 0.2972 - val_loss: 1.2451 - val_mae: 0.2735 - learning_rate: 1.0000e-04\n",
      "Epoch 138/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2423 - mae: 0.2975\n",
      "Epoch 138: val_loss improved from 1.24507 to 1.24229, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2483 - mae: 0.2977 - val_loss: 1.2423 - val_mae: 0.2707 - learning_rate: 1.0000e-04\n",
      "Epoch 139/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2801 - mae: 0.2995\n",
      "Epoch 139: val_loss improved from 1.24229 to 1.22642, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2808 - mae: 0.2993 - val_loss: 1.2264 - val_mae: 0.2676 - learning_rate: 1.0000e-04\n",
      "Epoch 140/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3032 - mae: 0.2996\n",
      "Epoch 140: val_loss did not improve from 1.22642\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3032 - mae: 0.2995 - val_loss: 1.2412 - val_mae: 0.2708 - learning_rate: 1.0000e-04\n",
      "Epoch 141/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2854 - mae: 0.2985\n",
      "Epoch 141: val_loss did not improve from 1.22642\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2859 - mae: 0.2984 - val_loss: 1.2268 - val_mae: 0.2707 - learning_rate: 1.0000e-04\n",
      "Epoch 142/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3126 - mae: 0.3049\n",
      "Epoch 142: val_loss did not improve from 1.22642\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3118 - mae: 0.3045 - val_loss: 1.2299 - val_mae: 0.2676 - learning_rate: 1.0000e-04\n",
      "Epoch 143/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2828 - mae: 0.2976\n",
      "Epoch 143: val_loss did not improve from 1.22642\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2847 - mae: 0.2976 - val_loss: 1.2269 - val_mae: 0.2683 - learning_rate: 1.0000e-04\n",
      "Epoch 144/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3074 - mae: 0.2973\n",
      "Epoch 144: val_loss improved from 1.22642 to 1.22172, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3067 - mae: 0.2973 - val_loss: 1.2217 - val_mae: 0.2729 - learning_rate: 1.0000e-04\n",
      "Epoch 145/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3246 - mae: 0.3011\n",
      "Epoch 145: val_loss did not improve from 1.22172\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3220 - mae: 0.3010 - val_loss: 1.2251 - val_mae: 0.2717 - learning_rate: 1.0000e-04\n",
      "Epoch 146/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2676 - mae: 0.2959\n",
      "Epoch 146: val_loss improved from 1.22172 to 1.21392, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2682 - mae: 0.2960 - val_loss: 1.2139 - val_mae: 0.2718 - learning_rate: 1.0000e-04\n",
      "Epoch 147/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2570 - mae: 0.2950\n",
      "Epoch 147: val_loss improved from 1.21392 to 1.20804, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2589 - mae: 0.2951 - val_loss: 1.2080 - val_mae: 0.2711 - learning_rate: 1.0000e-04\n",
      "Epoch 148/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2383 - mae: 0.2959\n",
      "Epoch 148: val_loss improved from 1.20804 to 1.20558, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2419 - mae: 0.2961 - val_loss: 1.2056 - val_mae: 0.2679 - learning_rate: 1.0000e-04\n",
      "Epoch 149/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1798 - mae: 0.2923\n",
      "Epoch 149: val_loss did not improve from 1.20558\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1834 - mae: 0.2924 - val_loss: 1.2093 - val_mae: 0.2709 - learning_rate: 1.0000e-04\n",
      "Epoch 150/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2661 - mae: 0.2983\n",
      "Epoch 150: val_loss improved from 1.20558 to 1.20314, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2663 - mae: 0.2983 - val_loss: 1.2031 - val_mae: 0.2681 - learning_rate: 1.0000e-04\n",
      "Epoch 151/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3354 - mae: 0.2998\n",
      "Epoch 151: val_loss improved from 1.20314 to 1.20066, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3283 - mae: 0.2995 - val_loss: 1.2007 - val_mae: 0.2666 - learning_rate: 1.0000e-04\n",
      "Epoch 152/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2357 - mae: 0.2942\n",
      "Epoch 152: val_loss improved from 1.20066 to 1.19622, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2375 - mae: 0.2943 - val_loss: 1.1962 - val_mae: 0.2722 - learning_rate: 1.0000e-04\n",
      "Epoch 153/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2424 - mae: 0.2958\n",
      "Epoch 153: val_loss improved from 1.19622 to 1.18281, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2433 - mae: 0.2958 - val_loss: 1.1828 - val_mae: 0.2696 - learning_rate: 1.0000e-04\n",
      "Epoch 154/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2955 - mae: 0.3010\n",
      "Epoch 154: val_loss improved from 1.18281 to 1.17622, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2930 - mae: 0.3008 - val_loss: 1.1762 - val_mae: 0.2649 - learning_rate: 1.0000e-04\n",
      "Epoch 155/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1967 - mae: 0.2939\n",
      "Epoch 155: val_loss did not improve from 1.17622\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1979 - mae: 0.2939 - val_loss: 1.1902 - val_mae: 0.2676 - learning_rate: 1.0000e-04\n",
      "Epoch 156/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2438 - mae: 0.2941\n",
      "Epoch 156: val_loss did not improve from 1.17622\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2445 - mae: 0.2943 - val_loss: 1.1813 - val_mae: 0.2663 - learning_rate: 1.0000e-04\n",
      "Epoch 157/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2800 - mae: 0.2987\n",
      "Epoch 157: val_loss did not improve from 1.17622\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2780 - mae: 0.2985 - val_loss: 1.1830 - val_mae: 0.2695 - learning_rate: 1.0000e-04\n",
      "Epoch 158/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2947 - mae: 0.2987\n",
      "Epoch 158: val_loss improved from 1.17622 to 1.17113, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2929 - mae: 0.2985 - val_loss: 1.1711 - val_mae: 0.2677 - learning_rate: 1.0000e-04\n",
      "Epoch 159/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2072 - mae: 0.2938\n",
      "Epoch 159: val_loss did not improve from 1.17113\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2101 - mae: 0.2940 - val_loss: 1.1768 - val_mae: 0.2689 - learning_rate: 1.0000e-04\n",
      "Epoch 160/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2311 - mae: 0.2979\n",
      "Epoch 160: val_loss did not improve from 1.17113\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2314 - mae: 0.2977 - val_loss: 1.1833 - val_mae: 0.2691 - learning_rate: 1.0000e-04\n",
      "Epoch 161/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2045 - mae: 0.2932\n",
      "Epoch 161: val_loss improved from 1.17113 to 1.17089, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2064 - mae: 0.2934 - val_loss: 1.1709 - val_mae: 0.2671 - learning_rate: 1.0000e-04\n",
      "Epoch 162/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2579 - mae: 0.2992\n",
      "Epoch 162: val_loss did not improve from 1.17089\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2568 - mae: 0.2990 - val_loss: 1.1723 - val_mae: 0.2654 - learning_rate: 1.0000e-04\n",
      "Epoch 163/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2131 - mae: 0.2914\n",
      "Epoch 163: val_loss improved from 1.17089 to 1.17023, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2143 - mae: 0.2916 - val_loss: 1.1702 - val_mae: 0.2691 - learning_rate: 1.0000e-04\n",
      "Epoch 164/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2055 - mae: 0.2915\n",
      "Epoch 164: val_loss improved from 1.17023 to 1.15173, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2085 - mae: 0.2918 - val_loss: 1.1517 - val_mae: 0.2643 - learning_rate: 1.0000e-04\n",
      "Epoch 165/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2260 - mae: 0.2981\n",
      "Epoch 165: val_loss did not improve from 1.15173\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2248 - mae: 0.2980 - val_loss: 1.1612 - val_mae: 0.2641 - learning_rate: 1.0000e-04\n",
      "Epoch 166/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2286 - mae: 0.2937\n",
      "Epoch 166: val_loss improved from 1.15173 to 1.15000, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2279 - mae: 0.2937 - val_loss: 1.1500 - val_mae: 0.2660 - learning_rate: 1.0000e-04\n",
      "Epoch 167/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2099 - mae: 0.2955\n",
      "Epoch 167: val_loss improved from 1.15000 to 1.14583, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2104 - mae: 0.2954 - val_loss: 1.1458 - val_mae: 0.2646 - learning_rate: 1.0000e-04\n",
      "Epoch 168/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2623 - mae: 0.3002\n",
      "Epoch 168: val_loss did not improve from 1.14583\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2603 - mae: 0.2999 - val_loss: 1.1516 - val_mae: 0.2650 - learning_rate: 1.0000e-04\n",
      "Epoch 169/500\n",
      "\u001b[1m 93/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2692 - mae: 0.2972\n",
      "Epoch 169: val_loss improved from 1.14583 to 1.14298, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2625 - mae: 0.2968 - val_loss: 1.1430 - val_mae: 0.2649 - learning_rate: 1.0000e-04\n",
      "Epoch 170/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2380 - mae: 0.2955\n",
      "Epoch 170: val_loss did not improve from 1.14298\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2355 - mae: 0.2954 - val_loss: 1.1554 - val_mae: 0.2656 - learning_rate: 1.0000e-04\n",
      "Epoch 171/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2577 - mae: 0.2954\n",
      "Epoch 171: val_loss improved from 1.14298 to 1.14065, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2544 - mae: 0.2952 - val_loss: 1.1406 - val_mae: 0.2606 - learning_rate: 1.0000e-04\n",
      "Epoch 172/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2490 - mae: 0.2924\n",
      "Epoch 172: val_loss improved from 1.14065 to 1.13641, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2473 - mae: 0.2925 - val_loss: 1.1364 - val_mae: 0.2649 - learning_rate: 1.0000e-04\n",
      "Epoch 173/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2305 - mae: 0.2962\n",
      "Epoch 173: val_loss did not improve from 1.13641\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2288 - mae: 0.2961 - val_loss: 1.1420 - val_mae: 0.2648 - learning_rate: 1.0000e-04\n",
      "Epoch 174/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2271 - mae: 0.2946\n",
      "Epoch 174: val_loss improved from 1.13641 to 1.13468, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2237 - mae: 0.2944 - val_loss: 1.1347 - val_mae: 0.2636 - learning_rate: 1.0000e-04\n",
      "Epoch 175/500\n",
      "\u001b[1m 95/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2144 - mae: 0.2938\n",
      "Epoch 175: val_loss improved from 1.13468 to 1.12585, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2120 - mae: 0.2938 - val_loss: 1.1259 - val_mae: 0.2637 - learning_rate: 1.0000e-04\n",
      "Epoch 176/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1756 - mae: 0.2951\n",
      "Epoch 176: val_loss did not improve from 1.12585\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1765 - mae: 0.2950 - val_loss: 1.1346 - val_mae: 0.2635 - learning_rate: 1.0000e-04\n",
      "Epoch 177/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1570 - mae: 0.2925\n",
      "Epoch 177: val_loss did not improve from 1.12585\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1597 - mae: 0.2925 - val_loss: 1.1316 - val_mae: 0.2646 - learning_rate: 1.0000e-04\n",
      "Epoch 178/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1860 - mae: 0.2931\n",
      "Epoch 178: val_loss did not improve from 1.12585\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1871 - mae: 0.2932 - val_loss: 1.1335 - val_mae: 0.2660 - learning_rate: 1.0000e-04\n",
      "Epoch 179/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1352 - mae: 0.2883\n",
      "Epoch 179: val_loss did not improve from 1.12585\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1371 - mae: 0.2885 - val_loss: 1.1290 - val_mae: 0.2631 - learning_rate: 1.0000e-04\n",
      "Epoch 180/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1798 - mae: 0.2921\n",
      "Epoch 180: val_loss did not improve from 1.12585\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1811 - mae: 0.2922 - val_loss: 1.1301 - val_mae: 0.2641 - learning_rate: 1.0000e-04\n",
      "Epoch 181/500\n",
      "\u001b[1m 95/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1554 - mae: 0.2909\n",
      "Epoch 181: val_loss improved from 1.12585 to 1.12108, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1581 - mae: 0.2911 - val_loss: 1.1211 - val_mae: 0.2623 - learning_rate: 1.0000e-04\n",
      "Epoch 182/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1876 - mae: 0.2891\n",
      "Epoch 182: val_loss improved from 1.12108 to 1.12000, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1879 - mae: 0.2893 - val_loss: 1.1200 - val_mae: 0.2648 - learning_rate: 1.0000e-04\n",
      "Epoch 183/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2205 - mae: 0.2955\n",
      "Epoch 183: val_loss improved from 1.12000 to 1.11058, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2178 - mae: 0.2953 - val_loss: 1.1106 - val_mae: 0.2627 - learning_rate: 1.0000e-04\n",
      "Epoch 184/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2077 - mae: 0.2928\n",
      "Epoch 184: val_loss improved from 1.11058 to 1.10910, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2067 - mae: 0.2928 - val_loss: 1.1091 - val_mae: 0.2628 - learning_rate: 1.0000e-04\n",
      "Epoch 185/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1525 - mae: 0.2906\n",
      "Epoch 185: val_loss did not improve from 1.10910\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1532 - mae: 0.2907 - val_loss: 1.1137 - val_mae: 0.2620 - learning_rate: 1.0000e-04\n",
      "Epoch 186/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1510 - mae: 0.2890\n",
      "Epoch 186: val_loss improved from 1.10910 to 1.09756, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1522 - mae: 0.2892 - val_loss: 1.0976 - val_mae: 0.2604 - learning_rate: 1.0000e-04\n",
      "Epoch 187/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1713 - mae: 0.2935\n",
      "Epoch 187: val_loss did not improve from 1.09756\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1710 - mae: 0.2933 - val_loss: 1.1098 - val_mae: 0.2652 - learning_rate: 1.0000e-04\n",
      "Epoch 188/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1313 - mae: 0.2914\n",
      "Epoch 188: val_loss did not improve from 1.09756\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1331 - mae: 0.2915 - val_loss: 1.1023 - val_mae: 0.2622 - learning_rate: 1.0000e-04\n",
      "Epoch 189/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1632 - mae: 0.2892\n",
      "Epoch 189: val_loss improved from 1.09756 to 1.09352, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1635 - mae: 0.2893 - val_loss: 1.0935 - val_mae: 0.2628 - learning_rate: 1.0000e-04\n",
      "Epoch 190/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1429 - mae: 0.2899\n",
      "Epoch 190: val_loss improved from 1.09352 to 1.09258, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1434 - mae: 0.2899 - val_loss: 1.0926 - val_mae: 0.2616 - learning_rate: 1.0000e-04\n",
      "Epoch 191/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1902 - mae: 0.2941\n",
      "Epoch 191: val_loss improved from 1.09258 to 1.08608, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1873 - mae: 0.2939 - val_loss: 1.0861 - val_mae: 0.2592 - learning_rate: 1.0000e-04\n",
      "Epoch 192/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0936 - mae: 0.2874\n",
      "Epoch 192: val_loss did not improve from 1.08608\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0964 - mae: 0.2876 - val_loss: 1.0886 - val_mae: 0.2599 - learning_rate: 1.0000e-04\n",
      "Epoch 193/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1436 - mae: 0.2857\n",
      "Epoch 193: val_loss did not improve from 1.08608\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1444 - mae: 0.2860 - val_loss: 1.0960 - val_mae: 0.2623 - learning_rate: 1.0000e-04\n",
      "Epoch 194/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1000 - mae: 0.2900\n",
      "Epoch 194: val_loss improved from 1.08608 to 1.08261, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1032 - mae: 0.2901 - val_loss: 1.0826 - val_mae: 0.2591 - learning_rate: 1.0000e-04\n",
      "Epoch 195/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1624 - mae: 0.2976\n",
      "Epoch 195: val_loss did not improve from 1.08261\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1624 - mae: 0.2974 - val_loss: 1.0864 - val_mae: 0.2593 - learning_rate: 1.0000e-04\n",
      "Epoch 196/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1683 - mae: 0.2933\n",
      "Epoch 196: val_loss improved from 1.08261 to 1.07324, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1673 - mae: 0.2931 - val_loss: 1.0732 - val_mae: 0.2566 - learning_rate: 1.0000e-04\n",
      "Epoch 197/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1112 - mae: 0.2863\n",
      "Epoch 197: val_loss did not improve from 1.07324\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1137 - mae: 0.2866 - val_loss: 1.0864 - val_mae: 0.2577 - learning_rate: 1.0000e-04\n",
      "Epoch 198/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1948 - mae: 0.2915\n",
      "Epoch 198: val_loss did not improve from 1.07324\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1927 - mae: 0.2916 - val_loss: 1.0749 - val_mae: 0.2592 - learning_rate: 1.0000e-04\n",
      "Epoch 199/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1373 - mae: 0.2856\n",
      "Epoch 199: val_loss improved from 1.07324 to 1.07259, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1377 - mae: 0.2859 - val_loss: 1.0726 - val_mae: 0.2608 - learning_rate: 1.0000e-04\n",
      "Epoch 200/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1259 - mae: 0.2901\n",
      "Epoch 200: val_loss improved from 1.07259 to 1.07137, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1270 - mae: 0.2902 - val_loss: 1.0714 - val_mae: 0.2613 - learning_rate: 1.0000e-04\n",
      "Epoch 201/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1138 - mae: 0.2935\n",
      "Epoch 201: val_loss did not improve from 1.07137\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1170 - mae: 0.2932 - val_loss: 1.0755 - val_mae: 0.2590 - learning_rate: 1.0000e-04\n",
      "Epoch 202/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1568 - mae: 0.2948\n",
      "Epoch 202: val_loss did not improve from 1.07137\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1559 - mae: 0.2946 - val_loss: 1.0779 - val_mae: 0.2559 - learning_rate: 1.0000e-04\n",
      "Epoch 203/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1674 - mae: 0.2891\n",
      "Epoch 203: val_loss improved from 1.07137 to 1.06567, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1652 - mae: 0.2892 - val_loss: 1.0657 - val_mae: 0.2577 - learning_rate: 1.0000e-04\n",
      "Epoch 204/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1046 - mae: 0.2883\n",
      "Epoch 204: val_loss improved from 1.06567 to 1.06348, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1074 - mae: 0.2884 - val_loss: 1.0635 - val_mae: 0.2570 - learning_rate: 1.0000e-04\n",
      "Epoch 205/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1554 - mae: 0.2907\n",
      "Epoch 205: val_loss improved from 1.06348 to 1.05891, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1544 - mae: 0.2907 - val_loss: 1.0589 - val_mae: 0.2577 - learning_rate: 1.0000e-04\n",
      "Epoch 206/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0851 - mae: 0.2857\n",
      "Epoch 206: val_loss did not improve from 1.05891\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0879 - mae: 0.2861 - val_loss: 1.0627 - val_mae: 0.2590 - learning_rate: 1.0000e-04\n",
      "Epoch 207/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1256 - mae: 0.2899\n",
      "Epoch 207: val_loss did not improve from 1.05891\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1269 - mae: 0.2900 - val_loss: 1.0682 - val_mae: 0.2567 - learning_rate: 1.0000e-04\n",
      "Epoch 208/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1788 - mae: 0.2943\n",
      "Epoch 208: val_loss did not improve from 1.05891\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1754 - mae: 0.2940 - val_loss: 1.0728 - val_mae: 0.2574 - learning_rate: 1.0000e-04\n",
      "Epoch 209/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1356 - mae: 0.2887\n",
      "Epoch 209: val_loss did not improve from 1.05891\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1345 - mae: 0.2886 - val_loss: 1.0687 - val_mae: 0.2582 - learning_rate: 1.0000e-04\n",
      "Epoch 210/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2193 - mae: 0.2952\n",
      "Epoch 210: val_loss did not improve from 1.05891\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2099 - mae: 0.2946 - val_loss: 1.0604 - val_mae: 0.2566 - learning_rate: 1.0000e-04\n",
      "Epoch 211/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1395 - mae: 0.2915\n",
      "Epoch 211: val_loss did not improve from 1.05891\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1387 - mae: 0.2914 - val_loss: 1.0608 - val_mae: 0.2612 - learning_rate: 1.0000e-04\n",
      "Epoch 212/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0865 - mae: 0.2875\n",
      "Epoch 212: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 212: val_loss did not improve from 1.05891\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0895 - mae: 0.2876 - val_loss: 1.0719 - val_mae: 0.2585 - learning_rate: 1.0000e-04\n",
      "Epoch 213/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1288 - mae: 0.2885\n",
      "Epoch 213: val_loss improved from 1.05891 to 1.05005, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1289 - mae: 0.2884 - val_loss: 1.0500 - val_mae: 0.2557 - learning_rate: 5.0000e-05\n",
      "Epoch 214/500\n",
      "\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1631 - mae: 0.2892\n",
      "Epoch 214: val_loss improved from 1.05005 to 1.04516, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1623 - mae: 0.2892 - val_loss: 1.0452 - val_mae: 0.2550 - learning_rate: 5.0000e-05\n",
      "Epoch 215/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1264 - mae: 0.2869\n",
      "Epoch 215: val_loss did not improve from 1.04516\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1270 - mae: 0.2870 - val_loss: 1.0511 - val_mae: 0.2566 - learning_rate: 5.0000e-05\n",
      "Epoch 216/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1262 - mae: 0.2853\n",
      "Epoch 216: val_loss did not improve from 1.04516\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1253 - mae: 0.2854 - val_loss: 1.0504 - val_mae: 0.2567 - learning_rate: 5.0000e-05\n",
      "Epoch 217/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1871 - mae: 0.2860\n",
      "Epoch 217: val_loss did not improve from 1.04516\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1794 - mae: 0.2858 - val_loss: 1.0504 - val_mae: 0.2558 - learning_rate: 5.0000e-05\n",
      "Epoch 218/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1488 - mae: 0.2849\n",
      "Epoch 218: val_loss did not improve from 1.04516\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1456 - mae: 0.2849 - val_loss: 1.0481 - val_mae: 0.2555 - learning_rate: 5.0000e-05\n",
      "Epoch 219/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0963 - mae: 0.2876\n",
      "Epoch 219: val_loss improved from 1.04516 to 1.04228, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0972 - mae: 0.2874 - val_loss: 1.0423 - val_mae: 0.2539 - learning_rate: 5.0000e-05\n",
      "Epoch 220/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1618 - mae: 0.2876\n",
      "Epoch 220: val_loss did not improve from 1.04228\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1589 - mae: 0.2875 - val_loss: 1.0448 - val_mae: 0.2549 - learning_rate: 5.0000e-05\n",
      "Epoch 221/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1096 - mae: 0.2829\n",
      "Epoch 221: val_loss did not improve from 1.04228\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1086 - mae: 0.2830 - val_loss: 1.0525 - val_mae: 0.2561 - learning_rate: 5.0000e-05\n",
      "Epoch 222/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0934 - mae: 0.2831\n",
      "Epoch 222: val_loss did not improve from 1.04228\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0943 - mae: 0.2832 - val_loss: 1.0452 - val_mae: 0.2563 - learning_rate: 5.0000e-05\n",
      "Epoch 223/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0587 - mae: 0.2796\n",
      "Epoch 223: val_loss did not improve from 1.04228\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0630 - mae: 0.2801 - val_loss: 1.0424 - val_mae: 0.2564 - learning_rate: 5.0000e-05\n",
      "Epoch 224/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0979 - mae: 0.2849\n",
      "Epoch 224: val_loss improved from 1.04228 to 1.03598, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0986 - mae: 0.2849 - val_loss: 1.0360 - val_mae: 0.2539 - learning_rate: 5.0000e-05\n",
      "Epoch 225/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0869 - mae: 0.2848\n",
      "Epoch 225: val_loss did not improve from 1.03598\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0888 - mae: 0.2848 - val_loss: 1.0395 - val_mae: 0.2541 - learning_rate: 5.0000e-05\n",
      "Epoch 226/500\n",
      "\u001b[1m 93/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0315 - mae: 0.2814\n",
      "Epoch 226: val_loss did not improve from 1.03598\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0383 - mae: 0.2817 - val_loss: 1.0386 - val_mae: 0.2538 - learning_rate: 5.0000e-05\n",
      "Epoch 227/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0736 - mae: 0.2826\n",
      "Epoch 227: val_loss improved from 1.03598 to 1.03383, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0745 - mae: 0.2826 - val_loss: 1.0338 - val_mae: 0.2541 - learning_rate: 5.0000e-05\n",
      "Epoch 228/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0664 - mae: 0.2817\n",
      "Epoch 228: val_loss improved from 1.03383 to 1.03263, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0702 - mae: 0.2821 - val_loss: 1.0326 - val_mae: 0.2550 - learning_rate: 5.0000e-05\n",
      "Epoch 229/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0902 - mae: 0.2796\n",
      "Epoch 229: val_loss did not improve from 1.03263\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0905 - mae: 0.2800 - val_loss: 1.0408 - val_mae: 0.2560 - learning_rate: 5.0000e-05\n",
      "Epoch 230/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0410 - mae: 0.2822\n",
      "Epoch 230: val_loss did not improve from 1.03263\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0461 - mae: 0.2824 - val_loss: 1.0370 - val_mae: 0.2536 - learning_rate: 5.0000e-05\n",
      "Epoch 231/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0324 - mae: 0.2802\n",
      "Epoch 231: val_loss did not improve from 1.03263\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0365 - mae: 0.2804 - val_loss: 1.0422 - val_mae: 0.2548 - learning_rate: 5.0000e-05\n",
      "Epoch 232/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1346 - mae: 0.2859\n",
      "Epoch 232: val_loss did not improve from 1.03263\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1310 - mae: 0.2858 - val_loss: 1.0385 - val_mae: 0.2541 - learning_rate: 5.0000e-05\n",
      "Epoch 233/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0714 - mae: 0.2825\n",
      "Epoch 233: val_loss did not improve from 1.03263\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0717 - mae: 0.2826 - val_loss: 1.0383 - val_mae: 0.2559 - learning_rate: 5.0000e-05\n",
      "Epoch 234/500\n",
      "\u001b[1m 94/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1917 - mae: 0.2903\n",
      "Epoch 234: val_loss improved from 1.03263 to 1.03222, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1794 - mae: 0.2897 - val_loss: 1.0322 - val_mae: 0.2520 - learning_rate: 5.0000e-05\n",
      "Epoch 235/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1296 - mae: 0.2854\n",
      "Epoch 235: val_loss did not improve from 1.03222\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1281 - mae: 0.2854 - val_loss: 1.0404 - val_mae: 0.2547 - learning_rate: 5.0000e-05\n",
      "Epoch 236/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1761 - mae: 0.2885\n",
      "Epoch 236: val_loss did not improve from 1.03222\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1712 - mae: 0.2882 - val_loss: 1.0409 - val_mae: 0.2545 - learning_rate: 5.0000e-05\n",
      "Epoch 237/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0928 - mae: 0.2828\n",
      "Epoch 237: val_loss did not improve from 1.03222\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0929 - mae: 0.2828 - val_loss: 1.0407 - val_mae: 0.2552 - learning_rate: 5.0000e-05\n",
      "Epoch 238/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0678 - mae: 0.2817\n",
      "Epoch 238: val_loss did not improve from 1.03222\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0696 - mae: 0.2818 - val_loss: 1.0361 - val_mae: 0.2547 - learning_rate: 5.0000e-05\n",
      "Epoch 239/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0587 - mae: 0.2816\n",
      "Epoch 239: val_loss improved from 1.03222 to 1.03114, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0599 - mae: 0.2817 - val_loss: 1.0311 - val_mae: 0.2533 - learning_rate: 5.0000e-05\n",
      "Epoch 240/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0775 - mae: 0.2831\n",
      "Epoch 240: val_loss did not improve from 1.03114\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0780 - mae: 0.2831 - val_loss: 1.0390 - val_mae: 0.2550 - learning_rate: 5.0000e-05\n",
      "Epoch 241/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0420 - mae: 0.2806\n",
      "Epoch 241: val_loss did not improve from 1.03114\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0442 - mae: 0.2808 - val_loss: 1.0322 - val_mae: 0.2532 - learning_rate: 5.0000e-05\n",
      "Epoch 242/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0806 - mae: 0.2876\n",
      "Epoch 242: val_loss did not improve from 1.03114\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0815 - mae: 0.2875 - val_loss: 1.0374 - val_mae: 0.2521 - learning_rate: 5.0000e-05\n",
      "Epoch 243/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1411 - mae: 0.2895\n",
      "Epoch 243: val_loss did not improve from 1.03114\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1399 - mae: 0.2894 - val_loss: 1.0340 - val_mae: 0.2520 - learning_rate: 5.0000e-05\n",
      "Epoch 244/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0522 - mae: 0.2799\n",
      "Epoch 244: val_loss did not improve from 1.03114\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0541 - mae: 0.2800 - val_loss: 1.0345 - val_mae: 0.2536 - learning_rate: 5.0000e-05\n",
      "Epoch 245/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1330 - mae: 0.2879\n",
      "Epoch 245: val_loss improved from 1.03114 to 1.02883, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1301 - mae: 0.2877 - val_loss: 1.0288 - val_mae: 0.2529 - learning_rate: 5.0000e-05\n",
      "Epoch 246/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0966 - mae: 0.2840\n",
      "Epoch 246: val_loss did not improve from 1.02883\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0963 - mae: 0.2840 - val_loss: 1.0321 - val_mae: 0.2526 - learning_rate: 5.0000e-05\n",
      "Epoch 247/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1009 - mae: 0.2822\n",
      "Epoch 247: val_loss did not improve from 1.02883\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0999 - mae: 0.2822 - val_loss: 1.0333 - val_mae: 0.2542 - learning_rate: 5.0000e-05\n",
      "Epoch 248/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0447 - mae: 0.2822\n",
      "Epoch 248: val_loss did not improve from 1.02883\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0466 - mae: 0.2822 - val_loss: 1.0291 - val_mae: 0.2539 - learning_rate: 5.0000e-05\n",
      "Epoch 249/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0242 - mae: 0.2796\n",
      "Epoch 249: val_loss did not improve from 1.02883\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0266 - mae: 0.2798 - val_loss: 1.0311 - val_mae: 0.2538 - learning_rate: 5.0000e-05\n",
      "Epoch 250/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1557 - mae: 0.2869\n",
      "Epoch 250: val_loss improved from 1.02883 to 1.02627, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1520 - mae: 0.2867 - val_loss: 1.0263 - val_mae: 0.2521 - learning_rate: 5.0000e-05\n",
      "Epoch 251/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0484 - mae: 0.2809\n",
      "Epoch 251: val_loss did not improve from 1.02627\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0500 - mae: 0.2811 - val_loss: 1.0292 - val_mae: 0.2533 - learning_rate: 5.0000e-05\n",
      "Epoch 252/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1125 - mae: 0.2827\n",
      "Epoch 252: val_loss did not improve from 1.02627\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1109 - mae: 0.2826 - val_loss: 1.0276 - val_mae: 0.2520 - learning_rate: 5.0000e-05\n",
      "Epoch 253/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1424 - mae: 0.2854\n",
      "Epoch 253: val_loss improved from 1.02627 to 1.02123, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1363 - mae: 0.2852 - val_loss: 1.0212 - val_mae: 0.2509 - learning_rate: 5.0000e-05\n",
      "Epoch 254/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0604 - mae: 0.2825\n",
      "Epoch 254: val_loss did not improve from 1.02123\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0618 - mae: 0.2825 - val_loss: 1.0214 - val_mae: 0.2520 - learning_rate: 5.0000e-05\n",
      "Epoch 255/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1146 - mae: 0.2835\n",
      "Epoch 255: val_loss did not improve from 1.02123\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1133 - mae: 0.2835 - val_loss: 1.0239 - val_mae: 0.2539 - learning_rate: 5.0000e-05\n",
      "Epoch 256/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0536 - mae: 0.2793\n",
      "Epoch 256: val_loss improved from 1.02123 to 1.01820, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0547 - mae: 0.2794 - val_loss: 1.0182 - val_mae: 0.2512 - learning_rate: 5.0000e-05\n",
      "Epoch 257/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0267 - mae: 0.2781\n",
      "Epoch 257: val_loss did not improve from 1.01820\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0291 - mae: 0.2783 - val_loss: 1.0231 - val_mae: 0.2522 - learning_rate: 5.0000e-05\n",
      "Epoch 258/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0779 - mae: 0.2831\n",
      "Epoch 258: val_loss improved from 1.01820 to 1.01624, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0780 - mae: 0.2831 - val_loss: 1.0162 - val_mae: 0.2533 - learning_rate: 5.0000e-05\n",
      "Epoch 259/500\n",
      "\u001b[1m 92/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0850 - mae: 0.2846\n",
      "Epoch 259: val_loss did not improve from 1.01624\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0842 - mae: 0.2846 - val_loss: 1.0229 - val_mae: 0.2506 - learning_rate: 5.0000e-05\n",
      "Epoch 260/500\n",
      "\u001b[1m 92/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1029 - mae: 0.2834\n",
      "Epoch 260: val_loss did not improve from 1.01624\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0958 - mae: 0.2831 - val_loss: 1.0224 - val_mae: 0.2522 - learning_rate: 5.0000e-05\n",
      "Epoch 261/500\n",
      "\u001b[1m 95/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0135 - mae: 0.2786\n",
      "Epoch 261: val_loss improved from 1.01624 to 1.01434, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0181 - mae: 0.2789 - val_loss: 1.0143 - val_mae: 0.2524 - learning_rate: 5.0000e-05\n",
      "Epoch 262/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0384 - mae: 0.2846\n",
      "Epoch 262: val_loss did not improve from 1.01434\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0405 - mae: 0.2846 - val_loss: 1.0178 - val_mae: 0.2528 - learning_rate: 5.0000e-05\n",
      "Epoch 263/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0290 - mae: 0.2793\n",
      "Epoch 263: val_loss did not improve from 1.01434\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0323 - mae: 0.2796 - val_loss: 1.0160 - val_mae: 0.2522 - learning_rate: 5.0000e-05\n",
      "Epoch 264/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0694 - mae: 0.2817\n",
      "Epoch 264: val_loss improved from 1.01434 to 1.01373, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0696 - mae: 0.2818 - val_loss: 1.0137 - val_mae: 0.2520 - learning_rate: 5.0000e-05\n",
      "Epoch 265/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0578 - mae: 0.2811\n",
      "Epoch 265: val_loss did not improve from 1.01373\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0576 - mae: 0.2811 - val_loss: 1.0154 - val_mae: 0.2519 - learning_rate: 5.0000e-05\n",
      "Epoch 266/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0983 - mae: 0.2830\n",
      "Epoch 266: val_loss improved from 1.01373 to 1.01194, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0970 - mae: 0.2830 - val_loss: 1.0119 - val_mae: 0.2510 - learning_rate: 5.0000e-05\n",
      "Epoch 267/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1477 - mae: 0.2863\n",
      "Epoch 267: val_loss did not improve from 1.01194\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1438 - mae: 0.2861 - val_loss: 1.0134 - val_mae: 0.2525 - learning_rate: 5.0000e-05\n",
      "Epoch 268/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0662 - mae: 0.2810\n",
      "Epoch 268: val_loss improved from 1.01194 to 1.01055, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0664 - mae: 0.2812 - val_loss: 1.0106 - val_mae: 0.2532 - learning_rate: 5.0000e-05\n",
      "Epoch 269/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0598 - mae: 0.2844\n",
      "Epoch 269: val_loss did not improve from 1.01055\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0597 - mae: 0.2842 - val_loss: 1.0170 - val_mae: 0.2519 - learning_rate: 5.0000e-05\n",
      "Epoch 270/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0698 - mae: 0.2825\n",
      "Epoch 270: val_loss did not improve from 1.01055\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0691 - mae: 0.2825 - val_loss: 1.0165 - val_mae: 0.2522 - learning_rate: 5.0000e-05\n",
      "Epoch 271/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0074 - mae: 0.2781\n",
      "Epoch 271: val_loss improved from 1.01055 to 1.00995, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0114 - mae: 0.2784 - val_loss: 1.0100 - val_mae: 0.2515 - learning_rate: 5.0000e-05\n",
      "Epoch 272/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0577 - mae: 0.2847\n",
      "Epoch 272: val_loss did not improve from 1.00995\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0571 - mae: 0.2846 - val_loss: 1.0121 - val_mae: 0.2496 - learning_rate: 5.0000e-05\n",
      "Epoch 273/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0100 - mae: 0.2778\n",
      "Epoch 273: val_loss improved from 1.00995 to 1.00987, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0128 - mae: 0.2780 - val_loss: 1.0099 - val_mae: 0.2519 - learning_rate: 5.0000e-05\n",
      "Epoch 274/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0131 - mae: 0.2790\n",
      "Epoch 274: val_loss improved from 1.00987 to 1.00904, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0156 - mae: 0.2791 - val_loss: 1.0090 - val_mae: 0.2502 - learning_rate: 5.0000e-05\n",
      "Epoch 275/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0772 - mae: 0.2820\n",
      "Epoch 275: val_loss did not improve from 1.00904\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0757 - mae: 0.2819 - val_loss: 1.0115 - val_mae: 0.2528 - learning_rate: 5.0000e-05\n",
      "Epoch 276/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0529 - mae: 0.2821\n",
      "Epoch 276: val_loss improved from 1.00904 to 1.00650, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0539 - mae: 0.2821 - val_loss: 1.0065 - val_mae: 0.2491 - learning_rate: 5.0000e-05\n",
      "Epoch 277/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0981 - mae: 0.2862\n",
      "Epoch 277: val_loss did not improve from 1.00650\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0968 - mae: 0.2860 - val_loss: 1.0078 - val_mae: 0.2507 - learning_rate: 5.0000e-05\n",
      "Epoch 278/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0150 - mae: 0.2798\n",
      "Epoch 278: val_loss did not improve from 1.00650\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0188 - mae: 0.2799 - val_loss: 1.0111 - val_mae: 0.2500 - learning_rate: 5.0000e-05\n",
      "Epoch 279/500\n",
      "\u001b[1m 93/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0244 - mae: 0.2770\n",
      "Epoch 279: val_loss improved from 1.00650 to 1.00553, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0288 - mae: 0.2774 - val_loss: 1.0055 - val_mae: 0.2508 - learning_rate: 5.0000e-05\n",
      "Epoch 280/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0688 - mae: 0.2823\n",
      "Epoch 280: val_loss did not improve from 1.00553\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0671 - mae: 0.2822 - val_loss: 1.0060 - val_mae: 0.2510 - learning_rate: 5.0000e-05\n",
      "Epoch 281/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0305 - mae: 0.2804\n",
      "Epoch 281: val_loss did not improve from 1.00553\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0325 - mae: 0.2806 - val_loss: 1.0136 - val_mae: 0.2532 - learning_rate: 5.0000e-05\n",
      "Epoch 282/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0905 - mae: 0.2852\n",
      "Epoch 282: val_loss did not improve from 1.00553\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0881 - mae: 0.2849 - val_loss: 1.0064 - val_mae: 0.2494 - learning_rate: 5.0000e-05\n",
      "Epoch 283/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0648 - mae: 0.2818\n",
      "Epoch 283: val_loss improved from 1.00553 to 1.00320, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0650 - mae: 0.2817 - val_loss: 1.0032 - val_mae: 0.2511 - learning_rate: 5.0000e-05\n",
      "Epoch 284/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0541 - mae: 0.2843\n",
      "Epoch 284: val_loss did not improve from 1.00320\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0537 - mae: 0.2841 - val_loss: 1.0049 - val_mae: 0.2500 - learning_rate: 5.0000e-05\n",
      "Epoch 285/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0392 - mae: 0.2808\n",
      "Epoch 285: val_loss did not improve from 1.00320\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0393 - mae: 0.2807 - val_loss: 1.0083 - val_mae: 0.2500 - learning_rate: 5.0000e-05\n",
      "Epoch 286/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0926 - mae: 0.2845\n",
      "Epoch 286: val_loss improved from 1.00320 to 1.00046, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0913 - mae: 0.2843 - val_loss: 1.0005 - val_mae: 0.2480 - learning_rate: 5.0000e-05\n",
      "Epoch 287/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0382 - mae: 0.2816\n",
      "Epoch 287: val_loss improved from 1.00046 to 0.99723, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0388 - mae: 0.2816 - val_loss: 0.9972 - val_mae: 0.2500 - learning_rate: 5.0000e-05\n",
      "Epoch 288/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0561 - mae: 0.2842\n",
      "Epoch 288: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0567 - mae: 0.2839 - val_loss: 1.0070 - val_mae: 0.2520 - learning_rate: 5.0000e-05\n",
      "Epoch 289/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0638 - mae: 0.2817\n",
      "Epoch 289: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0628 - mae: 0.2817 - val_loss: 1.0046 - val_mae: 0.2504 - learning_rate: 5.0000e-05\n",
      "Epoch 290/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0278 - mae: 0.2785\n",
      "Epoch 290: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0296 - mae: 0.2786 - val_loss: 1.0003 - val_mae: 0.2485 - learning_rate: 5.0000e-05\n",
      "Epoch 291/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0252 - mae: 0.2789\n",
      "Epoch 291: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0262 - mae: 0.2789 - val_loss: 1.0031 - val_mae: 0.2509 - learning_rate: 5.0000e-05\n",
      "Epoch 292/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9822 - mae: 0.2745\n",
      "Epoch 292: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9876 - mae: 0.2749 - val_loss: 1.0062 - val_mae: 0.2512 - learning_rate: 5.0000e-05\n",
      "Epoch 293/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0642 - mae: 0.2824\n",
      "Epoch 293: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0641 - mae: 0.2824 - val_loss: 0.9994 - val_mae: 0.2496 - learning_rate: 5.0000e-05\n",
      "Epoch 294/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0260 - mae: 0.2815\n",
      "Epoch 294: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0271 - mae: 0.2815 - val_loss: 0.9983 - val_mae: 0.2491 - learning_rate: 5.0000e-05\n",
      "Epoch 295/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0463 - mae: 0.2801\n",
      "Epoch 295: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0466 - mae: 0.2802 - val_loss: 0.9974 - val_mae: 0.2472 - learning_rate: 2.5000e-05\n",
      "Epoch 296/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0705 - mae: 0.2798\n",
      "Epoch 296: val_loss did not improve from 0.99723\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0697 - mae: 0.2798 - val_loss: 0.9986 - val_mae: 0.2484 - learning_rate: 2.5000e-05\n",
      "Epoch 297/500\n",
      "\u001b[1m 94/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0293 - mae: 0.2783\n",
      "Epoch 297: val_loss improved from 0.99723 to 0.99581, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0313 - mae: 0.2784 - val_loss: 0.9958 - val_mae: 0.2479 - learning_rate: 2.5000e-05\n",
      "Epoch 298/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1207 - mae: 0.2829\n",
      "Epoch 298: val_loss did not improve from 0.99581\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1175 - mae: 0.2828 - val_loss: 0.9964 - val_mae: 0.2484 - learning_rate: 2.5000e-05\n",
      "Epoch 299/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0670 - mae: 0.2810\n",
      "Epoch 299: val_loss improved from 0.99581 to 0.99363, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0664 - mae: 0.2809 - val_loss: 0.9936 - val_mae: 0.2478 - learning_rate: 2.5000e-05\n",
      "Epoch 300/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9988 - mae: 0.2778\n",
      "Epoch 300: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0001 - mae: 0.2778 - val_loss: 0.9970 - val_mae: 0.2482 - learning_rate: 2.5000e-05\n",
      "Epoch 301/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1002 - mae: 0.2819\n",
      "Epoch 301: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0973 - mae: 0.2818 - val_loss: 0.9978 - val_mae: 0.2490 - learning_rate: 2.5000e-05\n",
      "Epoch 302/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0588 - mae: 0.2780\n",
      "Epoch 302: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0574 - mae: 0.2780 - val_loss: 0.9981 - val_mae: 0.2484 - learning_rate: 2.5000e-05\n",
      "Epoch 303/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0671 - mae: 0.2788\n",
      "Epoch 303: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0662 - mae: 0.2788 - val_loss: 0.9980 - val_mae: 0.2489 - learning_rate: 2.5000e-05\n",
      "Epoch 304/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0266 - mae: 0.2761\n",
      "Epoch 304: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0266 - mae: 0.2761 - val_loss: 0.9986 - val_mae: 0.2488 - learning_rate: 2.5000e-05\n",
      "Epoch 305/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0639 - mae: 0.2791\n",
      "Epoch 305: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0627 - mae: 0.2790 - val_loss: 0.9998 - val_mae: 0.2488 - learning_rate: 2.5000e-05\n",
      "Epoch 306/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9943 - mae: 0.2769\n",
      "Epoch 306: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.9958 - mae: 0.2770 - val_loss: 1.0001 - val_mae: 0.2499 - learning_rate: 2.5000e-05\n",
      "Epoch 307/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0753 - mae: 0.2811\n",
      "Epoch 307: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0737 - mae: 0.2810 - val_loss: 0.9969 - val_mae: 0.2490 - learning_rate: 1.2500e-05\n",
      "Epoch 308/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0098 - mae: 0.2793\n",
      "Epoch 308: val_loss did not improve from 0.99363\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0111 - mae: 0.2793 - val_loss: 0.9944 - val_mae: 0.2479 - learning_rate: 1.2500e-05\n",
      "Epoch 309/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0775 - mae: 0.2813\n",
      "Epoch 309: val_loss improved from 0.99363 to 0.99218, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0754 - mae: 0.2811 - val_loss: 0.9922 - val_mae: 0.2478 - learning_rate: 1.2500e-05\n",
      "Epoch 310/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0323 - mae: 0.2796\n",
      "Epoch 310: val_loss did not improve from 0.99218\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0321 - mae: 0.2796 - val_loss: 0.9922 - val_mae: 0.2474 - learning_rate: 1.2500e-05\n",
      "Epoch 311/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0372 - mae: 0.2785\n",
      "Epoch 311: val_loss did not improve from 0.99218\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0366 - mae: 0.2785 - val_loss: 0.9927 - val_mae: 0.2479 - learning_rate: 1.2500e-05\n",
      "Epoch 312/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0384 - mae: 0.2772\n",
      "Epoch 312: val_loss did not improve from 0.99218\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0382 - mae: 0.2772 - val_loss: 0.9927 - val_mae: 0.2476 - learning_rate: 1.2500e-05\n",
      "Epoch 313/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0006 - mae: 0.2769\n",
      "Epoch 313: val_loss improved from 0.99218 to 0.99150, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0020 - mae: 0.2770 - val_loss: 0.9915 - val_mae: 0.2472 - learning_rate: 1.2500e-05\n",
      "Epoch 314/500\n",
      "\u001b[1m 96/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0026 - mae: 0.2752\n",
      "Epoch 314: val_loss did not improve from 0.99150\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0044 - mae: 0.2754 - val_loss: 0.9936 - val_mae: 0.2473 - learning_rate: 1.2500e-05\n",
      "Epoch 315/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0696 - mae: 0.2820\n",
      "Epoch 315: val_loss did not improve from 0.99150\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0689 - mae: 0.2818 - val_loss: 0.9916 - val_mae: 0.2472 - learning_rate: 1.2500e-05\n",
      "Epoch 316/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9994 - mae: 0.2749\n",
      "Epoch 316: val_loss did not improve from 0.99150\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0003 - mae: 0.2750 - val_loss: 0.9937 - val_mae: 0.2480 - learning_rate: 1.2500e-05\n",
      "Epoch 317/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0475 - mae: 0.2811\n",
      "Epoch 317: val_loss did not improve from 0.99150\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0457 - mae: 0.2807 - val_loss: 0.9924 - val_mae: 0.2477 - learning_rate: 1.2500e-05\n",
      "Epoch 318/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0409 - mae: 0.2791\n",
      "Epoch 318: val_loss did not improve from 0.99150\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0411 - mae: 0.2791 - val_loss: 0.9927 - val_mae: 0.2475 - learning_rate: 1.2500e-05\n",
      "Epoch 319/500\n",
      "\u001b[1m 97/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0365 - mae: 0.2756\n",
      "Epoch 319: val_loss did not improve from 0.99150\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0368 - mae: 0.2758 - val_loss: 0.9947 - val_mae: 0.2481 - learning_rate: 1.2500e-05\n",
      "Epoch 320/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0127 - mae: 0.2777\n",
      "Epoch 320: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.99150\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0142 - mae: 0.2778 - val_loss: 0.9915 - val_mae: 0.2476 - learning_rate: 1.2500e-05\n",
      "Epoch 321/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0284 - mae: 0.2763\n",
      "Epoch 321: val_loss improved from 0.99150 to 0.99095, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0291 - mae: 0.2764 - val_loss: 0.9909 - val_mae: 0.2477 - learning_rate: 6.2500e-06\n",
      "Epoch 322/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0072 - mae: 0.2743\n",
      "Epoch 322: val_loss improved from 0.99095 to 0.98987, saving model to ./models/cell_type_gat_best_20250406-131525.keras\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0079 - mae: 0.2744 - val_loss: 0.9899 - val_mae: 0.2473 - learning_rate: 6.2500e-06\n",
      "Epoch 323/500\n",
      "\u001b[1m 93/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9359 - mae: 0.2716\n",
      "Epoch 323: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9462 - mae: 0.2722 - val_loss: 0.9910 - val_mae: 0.2474 - learning_rate: 6.2500e-06\n",
      "Epoch 324/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1302 - mae: 0.2834\n",
      "Epoch 324: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1249 - mae: 0.2831 - val_loss: 0.9901 - val_mae: 0.2477 - learning_rate: 6.2500e-06\n",
      "Epoch 325/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9914 - mae: 0.2759\n",
      "Epoch 325: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9930 - mae: 0.2759 - val_loss: 0.9909 - val_mae: 0.2472 - learning_rate: 6.2500e-06\n",
      "Epoch 326/500\n",
      "\u001b[1m 90/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0890 - mae: 0.2795\n",
      "Epoch 326: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0820 - mae: 0.2792 - val_loss: 0.9902 - val_mae: 0.2473 - learning_rate: 6.2500e-06\n",
      "Epoch 327/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9905 - mae: 0.2760\n",
      "Epoch 327: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9917 - mae: 0.2760 - val_loss: 0.9905 - val_mae: 0.2473 - learning_rate: 6.2500e-06\n",
      "Epoch 328/500\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0007 - mae: 0.2760\n",
      "Epoch 328: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0016 - mae: 0.2760 - val_loss: 0.9924 - val_mae: 0.2475 - learning_rate: 6.2500e-06\n",
      "Epoch 329/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0516 - mae: 0.2777\n",
      "Epoch 329: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0505 - mae: 0.2777 - val_loss: 0.9920 - val_mae: 0.2474 - learning_rate: 6.2500e-06\n",
      "Epoch 330/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0292 - mae: 0.2760\n",
      "Epoch 330: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0291 - mae: 0.2760 - val_loss: 0.9917 - val_mae: 0.2473 - learning_rate: 3.1250e-06\n",
      "Epoch 331/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9926 - mae: 0.2723\n",
      "Epoch 331: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9943 - mae: 0.2725 - val_loss: 0.9910 - val_mae: 0.2470 - learning_rate: 3.1250e-06\n",
      "Epoch 332/500\n",
      "\u001b[1m102/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0512 - mae: 0.2778\n",
      "Epoch 332: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0505 - mae: 0.2778 - val_loss: 0.9900 - val_mae: 0.2468 - learning_rate: 3.1250e-06\n",
      "Epoch 333/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9506 - mae: 0.2728\n",
      "Epoch 333: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9551 - mae: 0.2730 - val_loss: 0.9911 - val_mae: 0.2467 - learning_rate: 3.1250e-06\n",
      "Epoch 334/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0331 - mae: 0.2775\n",
      "Epoch 334: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0320 - mae: 0.2773 - val_loss: 0.9910 - val_mae: 0.2468 - learning_rate: 3.1250e-06\n",
      "Epoch 335/500\n",
      "\u001b[1m100/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9785 - mae: 0.2721\n",
      "Epoch 335: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9806 - mae: 0.2723 - val_loss: 0.9911 - val_mae: 0.2471 - learning_rate: 3.1250e-06\n",
      "Epoch 336/500\n",
      "\u001b[1m 98/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0461 - mae: 0.2787\n",
      "Epoch 336: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0459 - mae: 0.2786 - val_loss: 0.9913 - val_mae: 0.2473 - learning_rate: 3.1250e-06\n",
      "Epoch 337/500\n",
      "\u001b[1m 99/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0097 - mae: 0.2773\n",
      "Epoch 337: val_loss did not improve from 0.98987\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0103 - mae: 0.2772 - val_loss: 0.9921 - val_mae: 0.2475 - learning_rate: 1.5625e-06\n",
      "Epoch 337: early stopping\n",
      "Restoring model weights from the end of the best epoch: 322.\n",
      "Training complete. Best validation loss: 0.9899\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test slide: S_7\n",
      "Test data for slide S_7 loaded with 2088 samples and 14 node features.\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "Submission file 'submission_S_7.csv' created!\n",
      "Pipeline execution completed successfully!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage with the improved GNN workflow\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the dataset file\n",
    "    h5_file_path = \"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\"\n",
    "    \n",
    "    # Initialize GNN pipeline\n",
    "    pipeline = CellTypeGATModel(h5_file_path, random_state=42)\n",
    "    \n",
    "    # Load all training data\n",
    "    train_spot_tables = pipeline.load_train_data()\n",
    "    \n",
    "    # Prepare training set with graph features from all available slides\n",
    "    node_features, adjacency_lists, edge_features, y_train = pipeline.prepare_training_set(normalize=True)\n",
    "    \n",
    "    # Visualize the graph structure for a small subset\n",
    "    # Note: You need to implement the visualize_graph method if needed\n",
    "    sample_slide = list(pipeline.train_spot_tables.keys())[0]\n",
    "    sample_df = pipeline.train_spot_tables[sample_slide].head(100)\n",
    "    \n",
    "    # Uncomment this if you implement the visualize_graph method\n",
    "    # pipeline.visualize_graph(\n",
    "    #     sample_df,\n",
    "    #     adjacency_lists[:100],\n",
    "    #     edge_features[:100] if edge_features is not None else None,\n",
    "    #     cell_type_index=0  # Visualize for the first cell type\n",
    "    # )\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = pipeline.cross_validate(\n",
    "        node_features, \n",
    "        adjacency_lists, \n",
    "        y_train, \n",
    "        edge_features=edge_features,\n",
    "        n_splits=5, \n",
    "        batch_size=64,\n",
    "        epochs=500,\n",
    "        early_stopping=True,\n",
    "        patience=15\n",
    "    )\n",
    "    \n",
    "    # Analyze cross-validation results\n",
    "    print(f\"Cross-validation mean MSE: {np.mean(cv_results['mse_scores']):.4f}\")\n",
    "    print(f\"Cross-validation mean R²: {np.mean(cv_results['r2_scores']):.4f}\")\n",
    "    \n",
    "    # Optional: Plot cross-validation results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.boxplot(cv_results['mse_scores'])\n",
    "    plt.title('MSE Scores Distribution')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(cv_results['r2_scores'])\n",
    "    plt.title('R² Scores Distribution')\n",
    "    plt.ylabel('R² Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Train final model on full dataset with early stopping\n",
    "    pipeline.build_model(\n",
    "        input_dim=node_features.shape[1], \n",
    "        dropout_rate=0.5, \n",
    "        l2_reg=0.001,\n",
    "        learning_rate=0.0001,\n",
    "        n_heads=8\n",
    "    )\n",
    "    \n",
    "    final_model, history = pipeline.train(\n",
    "        node_features, \n",
    "        adjacency_lists, \n",
    "        y_train,\n",
    "        edge_features=edge_features,\n",
    "        validation_split=0.2,\n",
    "        batch_size=64,\n",
    "        epochs=500,\n",
    "        early_stopping=True,\n",
    "        patience=15,\n",
    "        reduce_lr=True\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('MAE over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate predictions for all test slides\n",
    "    test_slides = []\n",
    "    with h5py.File(h5_file_path, \"r\") as f:\n",
    "        test_spots = f[\"spots/Test\"]\n",
    "        test_slides = list(test_spots.keys())\n",
    "    \n",
    "    all_predictions = {}\n",
    "    all_submission_dfs = {}\n",
    "    \n",
    "    for slide_id in test_slides:\n",
    "        print(f\"Processing test slide: {slide_id}\")\n",
    "        \n",
    "        # Load test data with graph structure\n",
    "        test_df, test_node_features, test_adjacency_lists, test_edge_features = pipeline.load_test_data(\n",
    "            slide_id=slide_id, \n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = pipeline.predict(test_node_features)\n",
    "        all_predictions[slide_id] = predictions\n",
    "        \n",
    "        # Create submission for this slide\n",
    "        submission_df = pipeline.create_submission(\n",
    "            test_df, \n",
    "            predictions, \n",
    "            submission_filename=f\"submission_{slide_id}.csv\"\n",
    "        )\n",
    "        \n",
    "        all_submission_dfs[slide_id] = submission_df\n",
    "    \n",
    "    # Combine all submissions into one file if needed\n",
    "    combined_submission = pd.concat(list(all_submission_dfs.values()), axis=0)\n",
    "    combined_submission.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    print(\"Pipeline execution completed successfully!\")\n",
    "    \n",
    "    # Optional: Feature importance analysis\n",
    "    # If you want to analyze which features are most important\n",
    "    # This is a simple approach - for more advanced analysis you might \n",
    "    # need to modify the model architecture\n",
    "    \n",
    "    if hasattr(pipeline.model, 'layers') and len(pipeline.model.layers) > 1:\n",
    "        first_layer_weights = pipeline.model.layers[1].get_weights()[0]\n",
    "        feature_importance = np.mean(np.abs(first_layer_weights), axis=1)\n",
    "        \n",
    "        # Create feature names based on the node features created\n",
    "        # This is an approximation - actual feature names depend on create_graph_features implementation\n",
    "        base_features = ['x', 'y', 'r', 'theta', 'x^2', 'y^2', 'xy']\n",
    "        neighbor_features = ['mean_x', 'mean_y', 'std_x', 'std_y', 'mean_dist', 'min_dist', 'max_dist']\n",
    "        feature_names = base_features + neighbor_features\n",
    "        \n",
    "        # Plot feature importance if possible\n",
    "        if len(feature_names) == len(feature_importance):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(feature_names, feature_importance)\n",
    "            plt.title('Feature Importance')\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('Importance')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c9da3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:19:38.807819Z",
     "iopub.status.busy": "2025-04-06T13:19:38.807527Z",
     "iopub.status.idle": "2025-04-06T13:19:38.823726Z",
     "shell.execute_reply": "2025-04-06T13:19:38.823075Z"
    },
    "papermill": {
     "duration": 0.174386,
     "end_time": "2025-04-06T13:19:38.824837",
     "exception": false,
     "start_time": "2025-04-06T13:19:38.650451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Model Architecture:\n",
      "Layer 0: node_features\n",
      "  Type: InputLayer\n",
      "  Parameters: 0\n",
      "  Trainable: True\n",
      "\n",
      "Layer 1: dense_25\n",
      "  Type: Dense\n",
      "  Parameters: 1920\n",
      "  Trainable: True\n",
      "\n",
      "Layer 2: batch_normalization_20\n",
      "  Type: BatchNormalization\n",
      "  Parameters: 512\n",
      "  Trainable: True\n",
      "\n",
      "Layer 3: dropout_20\n",
      "  Type: Dropout\n",
      "  Parameters: 0\n",
      "  Trainable: True\n",
      "\n",
      "Layer 4: dense_26\n",
      "  Type: Dense\n",
      "  Parameters: 33024\n",
      "  Trainable: True\n",
      "\n",
      "Layer 5: batch_normalization_21\n",
      "  Type: BatchNormalization\n",
      "  Parameters: 1024\n",
      "  Trainable: True\n",
      "\n",
      "Layer 6: dropout_21\n",
      "  Type: Dropout\n",
      "  Parameters: 0\n",
      "  Trainable: True\n",
      "\n",
      "Layer 7: dense_27\n",
      "  Type: Dense\n",
      "  Parameters: 131584\n",
      "  Trainable: True\n",
      "\n",
      "Layer 8: batch_normalization_22\n",
      "  Type: BatchNormalization\n",
      "  Parameters: 2048\n",
      "  Trainable: True\n",
      "\n",
      "Layer 9: dropout_22\n",
      "  Type: Dropout\n",
      "  Parameters: 0\n",
      "  Trainable: True\n",
      "\n",
      "Layer 10: dense_28\n",
      "  Type: Dense\n",
      "  Parameters: 525312\n",
      "  Trainable: True\n",
      "\n",
      "Layer 11: batch_normalization_23\n",
      "  Type: BatchNormalization\n",
      "  Parameters: 4096\n",
      "  Trainable: True\n",
      "\n",
      "Layer 12: dropout_23\n",
      "  Type: Dropout\n",
      "  Parameters: 0\n",
      "  Trainable: True\n",
      "\n",
      "Layer 13: dense_29\n",
      "  Type: Dense\n",
      "  Parameters: 35875\n",
      "  Trainable: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed inspection of each layer\n",
    "print(\"\\nDetailed Model Architecture:\")\n",
    "for i, layer in enumerate(pipeline.model.layers):\n",
    "    print(f\"Layer {i}: {layer.name}\")\n",
    "    print(f\"  Type: {layer.__class__.__name__}\")\n",
    "   # print(f\"  Input shape: {layer.input_shape}\")\n",
    "#    print(f\"  Output shape: {layer.output_shape}\")\n",
    "    print(f\"  Parameters: {layer.count_params()}\")\n",
    "    print(f\"  Trainable: {layer.trainable}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e89421a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:19:39.138777Z",
     "iopub.status.busy": "2025-04-06T13:19:39.138448Z",
     "iopub.status.idle": "2025-04-06T13:19:39.161276Z",
     "shell.execute_reply": "2025-04-06T13:19:39.160439Z"
    },
    "papermill": {
     "duration": 0.183044,
     "end_time": "2025-04-06T13:19:39.162541",
     "exception": false,
     "start_time": "2025-04-06T13:19:38.979497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_20               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_21               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_22               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_23               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">35,875</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m1,920\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_20               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m33,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_21               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_22               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │         \u001b[38;5;34m525,312\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_23               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │           \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)                  │          \u001b[38;5;34m35,875\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,198,507</span> (8.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,198,507\u001b[0m (8.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">731,555</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m731,555\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> (15.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,840\u001b[0m (15.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,463,112</span> (5.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,463,112\u001b[0m (5.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model summary\n",
    "pipeline.model.summary()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11390004,
     "sourceId": 94147,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 718.146686,
   "end_time": "2025-04-06T13:19:42.342582",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-06T13:07:44.195896",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
