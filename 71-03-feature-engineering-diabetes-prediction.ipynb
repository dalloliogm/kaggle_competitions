{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d5561a",
   "metadata": {
    "papermill": {
     "duration": 0.007968,
     "end_time": "2025-12-19T14:20:49.296853",
     "exception": false,
     "start_time": "2025-12-19T14:20:49.288885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 00. Introduction\n",
    "The aim of this notebook is to add thorough, EDA-driven feature engineering on:\n",
    " - Original competition features\n",
    " - New features derived from raw data (general EDA rules)\n",
    " - New features derived from reconstructed columns (medical/logic rules)\n",
    " - Auto-generated meaningful features targeting ~20, ~50, ~100, and ~200 extras\n",
    "We evaluate each feature set with 5-fold CV (LightGBM GPU), log AUC, save OOF & submissions,\n",
    "and export per-set feature importances.\n",
    "\n",
    "Newly created features in version 2 saved in the dataset I created, reading those files here to run other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2b551",
   "metadata": {
    "papermill": {
     "duration": 0.0062,
     "end_time": "2025-12-19T14:20:49.310312",
     "exception": false,
     "start_time": "2025-12-19T14:20:49.304112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 01. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5518480e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-19T14:20:49.324774Z",
     "iopub.status.busy": "2025-12-19T14:20:49.324302Z",
     "iopub.status.idle": "2025-12-19T14:20:49.338870Z",
     "shell.execute_reply": "2025-12-19T14:20:49.337796Z"
    },
    "papermill": {
     "duration": 0.024322,
     "end_time": "2025-12-19T14:20:49.340889",
     "exception": false,
     "start_time": "2025-12-19T14:20:49.316567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running version: FE_012\n",
      "Models: ['hist_gbdt']\n",
      "Time limit (hours): 11.5\n"
     ]
    }
   ],
   "source": [
    "VERSION = \"FE_012\"\n",
    "TIME_LIMIT_HOURS = 11.5\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "\n",
    "# Select model(s) to run (keep your structure)\n",
    "MODELS_TO_RUN = [\n",
    "    # \"xgboost_gpu\",\n",
    "    # \"lightgbm_gpu\",\n",
    "    # \"catboost_gpu\",\n",
    "    # \"logreg\",\n",
    "    \"hist_gbdt\",\n",
    "]\n",
    "\n",
    "TRAIN_PATH = \"/kaggle/input/playground-series-s5e12/train.csv\"\n",
    "TEST_PATH  = \"/kaggle/input/playground-series-s5e12/test.csv\"\n",
    "SAMPLE_SUB_PATH = \"/kaggle/input/playground-series-s5e12/sample_submission.csv\"\n",
    "\n",
    "USE_PREBUILT_RECON = True\n",
    "RECON_TRAIN_PATH = \"/kaggle/input/s05e12-outputs-diabetes-prediction/train_reconstructed_vreconstruct_007.csv\"\n",
    "RECON_TEST_PATH  = \"/kaggle/input/s05e12-outputs-diabetes-prediction/test_reconstructed_vreconstruct_007.csv\"\n",
    "\n",
    "# Read engineered feature sets you created earlier\n",
    "USE_PREBUILT_ENGINEERED = True\n",
    "ENGINEERED_DIR = \"/kaggle/input/s05e12-outputs-diabetes-prediction\"\n",
    "ENGINEERED_VERSION = \"FE_002\"   # matches your saved engineered filenames\n",
    "\n",
    "TARGET = \"diagnosed_diabetes\"\n",
    "ID_COL = \"id\"\n",
    "SUB_TARGET_COL = \"diagnosed_diabetes\"\n",
    "\n",
    "# Paths to 6 feature-importance CSVs (LightGBM GPU)\n",
    "FI_PATHS = {\n",
    "    \"auto_100\": \"/kaggle/input/s05e12-outputs-diabetes-prediction/71_03-feat_importance_lightgbm_gpu_auto_100_vFE_008.csv\",\n",
    "    \"auto_200\": \"/kaggle/input/s05e12-outputs-diabetes-prediction/71_03-feat_importance_lightgbm_gpu_auto_200_vFE_008.csv\",\n",
    "    \"auto_20\":  \"/kaggle/input/s05e12-outputs-diabetes-prediction/71_03-feat_importance_lightgbm_gpu_auto_20_vFE_008.csv\",\n",
    "    \"auto_50\":  \"/kaggle/input/s05e12-outputs-diabetes-prediction/71_03-feat_importance_lightgbm_gpu_auto_50_vFE_008.csv\",\n",
    "    \"raw_eda\":  \"/kaggle/input/s05e12-outputs-diabetes-prediction/71_03-feat_importance_lightgbm_gpu_raw_eda_vFE_008.csv\",\n",
    "    \"recon_fe\": \"/kaggle/input/s05e12-outputs-diabetes-prediction/71_03-feat_importance_lightgbm_gpu_recon_fe_vFE_008.csv\",\n",
    "}\n",
    "\n",
    "TOPK_LIST = [5, 10, 15, 20, 25]\n",
    "WEIGHT_EPS = 1e-3  # to avoid zeroing features during weighting\n",
    "\n",
    "# Output\n",
    "import os\n",
    "OUTPUT_DIR = f\"model_outputs_v{VERSION}\"\n",
    "RESULTS_CSV = f\"{OUTPUT_DIR}/results_v{VERSION}.csv\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Running version:\", VERSION)\n",
    "print(\"Models:\", MODELS_TO_RUN)\n",
    "print(\"Time limit (hours):\", TIME_LIMIT_HOURS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad85be",
   "metadata": {
    "papermill": {
     "duration": 0.006233,
     "end_time": "2025-12-19T14:20:49.353880",
     "exception": false,
     "start_time": "2025-12-19T14:20:49.347647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 02. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf23ee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:20:49.369492Z",
     "iopub.status.busy": "2025-12-19T14:20:49.369104Z",
     "iopub.status.idle": "2025-12-19T14:21:02.046393Z",
     "shell.execute_reply": "2025-12-19T14:21:02.045427Z"
    },
    "papermill": {
     "duration": 12.68758,
     "end_time": "2025-12-19T14:21:02.048205",
     "exception": false,
     "start_time": "2025-12-19T14:20:49.360625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, time, gc, warnings, json, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# LightGBM / XGBoost / CatBoost\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Extra models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcbc02d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:02.062894Z",
     "iopub.status.busy": "2025-12-19T14:21:02.062204Z",
     "iopub.status.idle": "2025-12-19T14:21:02.070063Z",
     "shell.execute_reply": "2025-12-19T14:21:02.068769Z"
    },
    "papermill": {
     "duration": 0.017034,
     "end_time": "2025-12-19T14:21:02.071661",
     "exception": false,
     "start_time": "2025-12-19T14:21:02.054627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "START_TIME = time.time()\n",
    "def time_up():\n",
    "    return (time.time() - START_TIME) >= (TIME_LIMIT_HOURS * 3600)\n",
    "\n",
    "def seconds_to_str(s):\n",
    "    m, s = divmod(int(s), 60); h, m = divmod(m, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    import random\n",
    "    np.random.seed(seed); random.seed(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12009406",
   "metadata": {
    "papermill": {
     "duration": 0.006304,
     "end_time": "2025-12-19T14:21:02.084518",
     "exception": false,
     "start_time": "2025-12-19T14:21:02.078214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 03. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ef9e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:02.098448Z",
     "iopub.status.busy": "2025-12-19T14:21:02.098102Z",
     "iopub.status.idle": "2025-12-19T14:21:09.822782Z",
     "shell.execute_reply": "2025-12-19T14:21:09.821698Z"
    },
    "papermill": {
     "duration": 7.733805,
     "end_time": "2025-12-19T14:21:09.824562",
     "exception": false,
     "start_time": "2025-12-19T14:21:02.090757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reconstructed CSVs: train_recon (700000, 32) | test_recon (300000, 31)\n",
      "Files OK.\n",
      "Rows: train 700000 | test 300000\n",
      "Train features: 24\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "feature_cols_base = [c for c in train.columns if c not in [TARGET, ID_COL]]\n",
    "\n",
    "if USE_PREBUILT_RECON:\n",
    "    train_recon = pd.read_csv(RECON_TRAIN_PATH)\n",
    "    test_recon  = pd.read_csv(RECON_TEST_PATH)\n",
    "    if TARGET in train_recon.columns and TARGET in train.columns:\n",
    "        try: train_recon[TARGET] = train_recon[TARGET].astype(train[TARGET].dtype)\n",
    "        except: pass\n",
    "    for df, ref in [(train_recon, train), (test_recon, test)]:\n",
    "        if ID_COL in df.columns and ID_COL in ref.columns:\n",
    "            try: df[ID_COL] = df[ID_COL].astype(ref[ID_COL].dtype)\n",
    "            except: pass\n",
    "    print(\"Loaded reconstructed CSVs:\",\n",
    "          f\"train_recon {train_recon.shape} | test_recon {test_recon.shape}\")\n",
    "else:\n",
    "    train_recon, test_recon = train.copy(), test.copy()\n",
    "\n",
    "print(\"Files OK.\")\n",
    "print(\"Rows: train\", len(train), \"| test\", len(test))\n",
    "print(\"Train features:\", len(feature_cols_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bcf70a",
   "metadata": {
    "papermill": {
     "duration": 0.006962,
     "end_time": "2025-12-19T14:21:09.839041",
     "exception": false,
     "start_time": "2025-12-19T14:21:09.832079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 04. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c9c94d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:09.854731Z",
     "iopub.status.busy": "2025-12-19T14:21:09.853531Z",
     "iopub.status.idle": "2025-12-19T14:21:09.871799Z",
     "shell.execute_reply": "2025-12-19T14:21:09.869817Z"
    },
    "papermill": {
     "duration": 0.028897,
     "end_time": "2025-12-19T14:21:09.874547",
     "exception": false,
     "start_time": "2025-12-19T14:21:09.845650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_num_cat_cols(df, exclude=None):\n",
    "    exclude = set(exclude or [])\n",
    "    num_cols = [c for c in df.columns\n",
    "                if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols = [c for c in df.columns\n",
    "                if c not in exclude and c not in num_cols]\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def safe_div(a, b):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        out = np.where(b==0, np.nan, a/b)\n",
    "    return out\n",
    "\n",
    "def add_cols(df, new_cols: dict):\n",
    "    for k, v in new_cols.items():\n",
    "        df[k] = v\n",
    "    return df\n",
    "\n",
    "def cap_outliers(s: pd.Series, q_low=0.01, q_high=0.99):\n",
    "    lo, hi = s.quantile(q_low), s.quantile(q_high)\n",
    "    return s.clip(lo, hi)\n",
    "\n",
    "def log1p_if_positive(s: pd.Series):\n",
    "    if (s.dropna() >= 0).all():\n",
    "        return np.log1p(s)\n",
    "    return s\n",
    "\n",
    "def quantile_bucket(s: pd.Series, q=5):\n",
    "    try:\n",
    "        return pd.qcut(s, q, labels=False, duplicates=\"drop\")\n",
    "    except Exception:\n",
    "        return pd.Series(np.nan, index=s.index)\n",
    "\n",
    "def write_results_row(row_dict, results_csv=RESULTS_CSV):\n",
    "    df_row = pd.DataFrame([row_dict])\n",
    "    if os.path.exists(results_csv):\n",
    "        prev = pd.read_csv(results_csv)\n",
    "        out = pd.concat([prev, df_row], ignore_index=True)\n",
    "    else:\n",
    "        out = df_row\n",
    "    out.to_csv(results_csv, index=False)\n",
    "\n",
    "def save_oof_and_sub(set_name, model_name, oof, test_pred, ids_train, ids_test):\n",
    "    oof_path = f\"{OUTPUT_DIR}/oof_{model_name}_{set_name}_v{VERSION}.csv\"\n",
    "    sub_path = f\"{OUTPUT_DIR}/sub_{model_name}_{set_name}_v{VERSION}.csv\"\n",
    "    pd.DataFrame({ID_COL: ids_train, \"oof_pred\": oof}).to_csv(oof_path, index=False)\n",
    "    if test_pred is not None and ids_test is not None:\n",
    "        pd.DataFrame({ID_COL: ids_test, SUB_TARGET_COL: test_pred}).to_csv(sub_path, index=False)\n",
    "    else:\n",
    "        sub_path = None\n",
    "    return oof_path, sub_path\n",
    "\n",
    "def save_importance_csv(set_name, model_name, feat_names, fold_importances):\n",
    "    imp = (pd.DataFrame({\"feature\": feat_names, \"importance\": fold_importances})\n",
    "             .groupby(\"feature\", as_index=False)[\"importance\"].sum()\n",
    "             .sort_values(\"importance\", ascending=False))\n",
    "    imp_path = f\"{OUTPUT_DIR}/feat_importance_{model_name}_{set_name}_v{VERSION}.csv\"\n",
    "    imp.to_csv(imp_path, index=False)\n",
    "    return imp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14cb8b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:09.891832Z",
     "iopub.status.busy": "2025-12-19T14:21:09.891496Z",
     "iopub.status.idle": "2025-12-19T14:21:09.906771Z",
     "shell.execute_reply": "2025-12-19T14:21:09.905269Z"
    },
    "papermill": {
     "duration": 0.026757,
     "end_time": "2025-12-19T14:21:09.908854",
     "exception": false,
     "start_time": "2025-12-19T14:21:09.882097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_predict_lightgbm(est, Xtr, ytr, Xva, yva, Xte):\n",
    "    est.set_params(bagging_seed=SEED, feature_fraction_seed=SEED, data_random_seed=SEED, verbosity=-1)\n",
    "    est.fit(\n",
    "        Xtr, ytr,\n",
    "        eval_set=[(Xva, yva)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[early_stopping(200, verbose=False), log_evaluation(0)]\n",
    "    )\n",
    "    va = est.predict_proba(Xva)[:, 1]\n",
    "    te = est.predict_proba(Xte)[:, 1] if Xte is not None else None\n",
    "    return va, te, getattr(est, \"feature_importances_\", None)\n",
    "\n",
    "def fit_predict_xgb(est, Xtr, ytr, Xva, yva, Xte):\n",
    "    est.fit(Xtr, ytr, eval_set=[(Xva, yva)], verbose=False)\n",
    "    va = est.predict_proba(Xva)[:, 1]\n",
    "    te = est.predict_proba(Xte)[:, 1] if Xte is not None else None\n",
    "    return va, te, getattr(est, \"feature_importances_\", None)\n",
    "\n",
    "def fit_predict_cat(est, Xtr, ytr, Xva, yva, Xte):\n",
    "    trp = Pool(Xtr, ytr); vap = Pool(Xva, yva)\n",
    "    est.fit(trp, eval_set=vap, verbose=False, use_best_model=True)\n",
    "    va = est.predict_proba(Xva)[:, 1]\n",
    "    te = est.predict_proba(Xte)[:, 1] if Xte is not None else None\n",
    "    return va, te, getattr(est, \"feature_importances_\", None)\n",
    "\n",
    "def fit_predict_sklearn(est, Xtr, ytr, Xva, yva, Xte):\n",
    "    est_use = est\n",
    "    if not hasattr(est_use, \"predict_proba\"):\n",
    "        est_use = CalibratedClassifierCV(est_use, method=\"isotonic\", cv=3)\n",
    "    est_use.fit(Xtr, ytr)\n",
    "    va = est_use.predict_proba(Xva)[:, 1]\n",
    "    te = est_use.predict_proba(Xte)[:, 1] if Xte is not None else None\n",
    "    imp = getattr(est, \"feature_importances_\", None)\n",
    "    return va, te, imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752a627",
   "metadata": {
    "papermill": {
     "duration": 0.008415,
     "end_time": "2025-12-19T14:21:09.924021",
     "exception": false,
     "start_time": "2025-12-19T14:21:09.915606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 05. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce25b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:09.938614Z",
     "iopub.status.busy": "2025-12-19T14:21:09.938233Z",
     "iopub.status.idle": "2025-12-19T14:21:09.949582Z",
     "shell.execute_reply": "2025-12-19T14:21:09.948575Z"
    },
    "papermill": {
     "duration": 0.020504,
     "end_time": "2025-12-19T14:21:09.951248",
     "exception": false,
     "start_time": "2025-12-19T14:21:09.930744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_preprocessors(train_df, target_col=TARGET, id_col=ID_COL):\n",
    "    feature_cols = [c for c in train_df.columns if c not in [target_col, id_col]]\n",
    "    num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(train_df[c])]\n",
    "    cat_cols = [c for c in feature_cols if c not in num_cols]\n",
    "\n",
    "    preproc_ohe_sparse = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
    "            ]), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    preproc_ordscale = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([\n",
    "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"sc\", StandardScaler(with_mean=True))\n",
    "            ]), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "            ]), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    return feature_cols, num_cols, cat_cols, preproc_ohe_sparse, preproc_ordscale\n",
    "\n",
    "def preprocessor_for_model(model_name, train_df):\n",
    "    feature_cols, num_cols, cat_cols, preproc_ohe_sparse, preproc_ordscale = make_preprocessors(train_df)\n",
    "    if model_name in [\"xgboost_gpu\", \"lightgbm_gpu\", \"catboost_gpu\", \"hist_gbdt\"]:\n",
    "        return feature_cols, preproc_ohe_sparse\n",
    "    elif model_name in [\"logreg\"]:\n",
    "        return feature_cols, preproc_ordscale\n",
    "    else:\n",
    "        return feature_cols, preproc_ohe_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b13880",
   "metadata": {
    "papermill": {
     "duration": 0.007355,
     "end_time": "2025-12-19T14:21:09.966030",
     "exception": false,
     "start_time": "2025-12-19T14:21:09.958675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 06. Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8652918e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:09.982250Z",
     "iopub.status.busy": "2025-12-19T14:21:09.981907Z",
     "iopub.status.idle": "2025-12-19T14:21:09.991918Z",
     "shell.execute_reply": "2025-12-19T14:21:09.990734Z"
    },
    "papermill": {
     "duration": 0.02089,
     "end_time": "2025-12-19T14:21:09.993772",
     "exception": false,
     "start_time": "2025-12-19T14:21:09.972882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_xgboost_gpu():\n",
    "    return XGBClassifier(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=5,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.80,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        tree_method=\"gpu_hist\",\n",
    "        predictor=\"gpu_predictor\",\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "def make_lightgbm_gpu():\n",
    "    return lgb.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=3500,\n",
    "        learning_rate=0.025,\n",
    "        num_leaves=63,\n",
    "        subsample=0.90,\n",
    "        colsample_bytree=0.80,\n",
    "        min_data_in_leaf=25,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        device=\"gpu\",\n",
    "        random_state=SEED,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "def make_catboost_gpu():\n",
    "    return CatBoostClassifier(\n",
    "        iterations=3500,\n",
    "        learning_rate=0.025,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3.0,\n",
    "        loss_function=\"Logloss\",\n",
    "        eval_metric=\"AUC\",\n",
    "        task_type=\"GPU\",\n",
    "        random_seed=SEED,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "def make_logreg():\n",
    "    return LogisticRegression(\n",
    "        max_iter=4000,\n",
    "        solver=\"lbfgs\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "def make_hist_gbdt():\n",
    "    return HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=None,\n",
    "        max_bins=255,\n",
    "        early_stopping=True,\n",
    "        l2_regularization=0.0,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "FACTORIES = {\n",
    "    \"xgboost_gpu\": make_xgboost_gpu,\n",
    "    \"lightgbm_gpu\": make_lightgbm_gpu,\n",
    "    \"catboost_gpu\": make_catboost_gpu,\n",
    "    \"logreg\": make_logreg,\n",
    "    \"hist_gbdt\": make_hist_gbdt,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0eb11",
   "metadata": {
    "papermill": {
     "duration": 0.006709,
     "end_time": "2025-12-19T14:21:10.010534",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.003825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 07. EDA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84655820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:10.028846Z",
     "iopub.status.busy": "2025-12-19T14:21:10.028492Z",
     "iopub.status.idle": "2025-12-19T14:21:10.051764Z",
     "shell.execute_reply": "2025-12-19T14:21:10.050486Z"
    },
    "papermill": {
     "duration": 0.03601,
     "end_time": "2025-12-19T14:21:10.053695",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.017685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fe_raw_basic(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    tr = train_df.copy(); te = test_df.copy()\n",
    "    exclude = [TARGET, ID_COL]\n",
    "    num_cols = [c for c in tr.columns if c not in exclude and pd.api.types.is_numeric_dtype(tr[c])]\n",
    "    cat_cols = [c for c in tr.columns if c not in exclude and c not in num_cols]\n",
    "\n",
    "    # Numeric caps + log1p\n",
    "    for c in num_cols:\n",
    "        lo, hi = tr[c].quantile(0.01), tr[c].quantile(0.99)\n",
    "        tr[f\"{c}_cap\"] = tr[c].clip(lo, hi)\n",
    "        te[f\"{c}_cap\"] = te[c].clip(lo, hi)\n",
    "        if (tr[c].dropna() >= 0).all():\n",
    "            tr[f\"{c}_log1p\"] = np.log1p(tr[c])\n",
    "            te[f\"{c}_log1p\"] = np.log1p(te[c])\n",
    "\n",
    "    # Missingness ratios\n",
    "    tr[\"num_nan_ratio\"] = tr[num_cols].isna().mean(axis=1) if num_cols else 0.0\n",
    "    te[\"num_nan_ratio\"] = te[num_cols].isna().mean(axis=1) if num_cols else 0.0\n",
    "\n",
    "    # Simple interactions among top-variance numeric columns\n",
    "    if len(num_cols) >= 4:\n",
    "        var_rank = tr[num_cols].var().sort_values(ascending=False)\n",
    "        topk = list(var_rank.index[:6])\n",
    "        for i in range(len(topk)):\n",
    "            for j in range(i+1, len(topk)):\n",
    "                a, b = topk[i], topk[j]\n",
    "                tr[f\"{a}_plus_{b}\"]  = tr[a] + tr[b]\n",
    "                te[f\"{a}_plus_{b}\"]  = te[a] + te[b]\n",
    "                tr[f\"{a}_minus_{b}\"] = tr[a] - tr[b]\n",
    "                te[f\"{a}_minus_{b}\"] = te[a] - te[b]\n",
    "                tr[f\"{a}_ratio_{b}\"] = np.where(tr[b]==0, np.nan, tr[a]/tr[b])\n",
    "                te[f\"{a}_ratio_{b}\"] = np.where(te[b]==0, np.nan, te[a]/te[b])\n",
    "\n",
    "    # Quantile buckets for skewed numerics\n",
    "    for c in num_cols:\n",
    "        if tr[c].dropna().skew() > 1.0:\n",
    "            try:\n",
    "                tr[f\"{c}_q5\"] = pd.qcut(tr[c], 5, labels=False, duplicates=\"drop\")\n",
    "                te[f\"{c}_q5\"] = pd.qcut(te[c], 5, labels=False, duplicates=\"drop\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return tr, te\n",
    "\n",
    "\n",
    "def _consistent_stage_codes(tr_stage: pd.Series, te_stage: pd.Series):\n",
    "    both = pd.concat([tr_stage, te_stage], axis=0)\n",
    "    codes = pd.Categorical(both).codes\n",
    "    tr_codes = pd.Series(codes[:len(tr_stage)], index=tr_stage.index).astype(float)\n",
    "    te_codes = pd.Series(codes[len(tr_stage):], index=te_stage.index).astype(float)\n",
    "    return tr_codes, te_codes\n",
    "\n",
    "\n",
    "def fe_recon_basic(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    tr = train_df.copy(); te = test_df.copy()\n",
    "    cols = set(train_df.columns)\n",
    "\n",
    "    gf  = \"glucose_fasting\"\n",
    "    gpp = \"glucose_postprandial\"\n",
    "    ins = \"insulin_level\"\n",
    "    a1c = \"hba1c\"\n",
    "    rs  = \"diabetes_risk_score\"\n",
    "    stg = \"diabetes_stage\"\n",
    "\n",
    "    # Glucose deltas & ratios\n",
    "    if gf in cols and gpp in cols:\n",
    "        tr[\"glucose_delta\"] = tr[gpp] - tr[gf]\n",
    "        te[\"glucose_delta\"] = te[gpp] - te[gf]\n",
    "        tr[\"glucose_ratio\"] = safe_div(tr[gpp], tr[gf])\n",
    "        te[\"glucose_ratio\"] = safe_div(te[gpp], te[gf])\n",
    "\n",
    "    # Insulin resistance proxy\n",
    "    if gf in cols and ins in cols:\n",
    "        tr[\"insulin_resistance_proxy\"] = safe_div(tr[gf], (tr[ins] + 1e-3))\n",
    "        te[\"insulin_resistance_proxy\"] = safe_div(te[gf], (te[ins] + 1e-3))\n",
    "\n",
    "    # eAG from HbA1c (mg/dL)\n",
    "    if a1c in cols:\n",
    "        tr[\"a1c_eag\"] = 28.7 * tr[a1c] - 46.7\n",
    "        te[\"a1c_eag\"] = 28.7 * te[a1c] - 46.7\n",
    "\n",
    "    # Risk buckets & stage numeric encodings\n",
    "    if rs in cols:\n",
    "        tr[\"risk_bucket_q5\"] = quantile_bucket(tr[rs], q=5)\n",
    "        te[\"risk_bucket_q5\"] = quantile_bucket(te[rs], q=5)\n",
    "\n",
    "    if stg in cols:\n",
    "        if pd.api.types.is_numeric_dtype(tr[stg]):\n",
    "            tr[\"stage_code\"] = tr[stg].astype(float)\n",
    "            te[\"stage_code\"] = te[stg].astype(float)\n",
    "        else:\n",
    "            tr_codes, te_codes = _consistent_stage_codes(tr[stg], te[stg])\n",
    "            tr[\"stage_code\"] = tr_codes\n",
    "            te[\"stage_code\"] = te_codes\n",
    "\n",
    "    # Safety caps and logs\n",
    "    for c in [x for x in [gf, gpp, ins, a1c, rs] if x in cols]:\n",
    "        tr[f\"{c}_cap\"] = cap_outliers(tr[c])\n",
    "        te[f\"{c}_cap\"] = cap_outliers(te[c])\n",
    "        tr[f\"{c}_log1p\"] = log1p_if_positive(tr[c])\n",
    "        te[f\"{c}_log1p\"] = log1p_if_positive(te[c])\n",
    "\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd44d2c",
   "metadata": {
    "papermill": {
     "duration": 0.007412,
     "end_time": "2025-12-19T14:21:10.071287",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.063875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 08. Auto Generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "741e8714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:10.090136Z",
     "iopub.status.busy": "2025-12-19T14:21:10.089831Z",
     "iopub.status.idle": "2025-12-19T14:21:10.114117Z",
     "shell.execute_reply": "2025-12-19T14:21:10.112743Z"
    },
    "papermill": {
     "duration": 0.036832,
     "end_time": "2025-12-19T14:21:10.117247",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.080415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_generate_features(train_df: pd.DataFrame, test_df: pd.DataFrame, budget=20):\n",
    "    tr = train_df.copy(); te = test_df.copy()\n",
    "    exclude = [TARGET, ID_COL]\n",
    "    num_cols = [c for c in tr.columns if c not in exclude and pd.api.types.is_numeric_dtype(tr[c])]\n",
    "    cat_cols = [c for c in tr.columns if c not in exclude and c not in num_cols]\n",
    "\n",
    "    cand = {}\n",
    "\n",
    "    # A) z-scores and squares for top-variance numerics\n",
    "    var_rank = tr[num_cols].var().sort_values(ascending=False) if len(num_cols) else pd.Series(dtype=float)\n",
    "    topn = list(var_rank.index[:min(12, len(var_rank))])\n",
    "    for c in topn:\n",
    "        mu, sd = tr[c].mean(), tr[c].std(ddof=0) or 1.0\n",
    "        cand[f\"{c}_z\"] = (tr[c] - mu) / sd\n",
    "        cand[f\"{c}_2\"] = tr[c] * tr[c]\n",
    "\n",
    "    # B) pairwise interactions\n",
    "    for i in range(len(topn)):\n",
    "        for j in range(i+1, len(topn)):\n",
    "            a, b = topn[i], topn[j]\n",
    "            cand[f\"{a}_x_{b}\"] = tr[a] * tr[b]\n",
    "            cand[f\"{a}_r_{b}\"] = np.where(tr[b]==0, np.nan, tr[a]/tr[b])\n",
    "\n",
    "    # C) frequency encodings for low-card cats\n",
    "    for c in cat_cols:\n",
    "        if tr[c].nunique(dropna=True) <= 20:\n",
    "            freq = tr[c].value_counts(dropna=False) / len(tr)\n",
    "            cand[f\"{c}_freq\"] = tr[c].map(freq)\n",
    "\n",
    "    # D) quantile buckets on skewed numerics\n",
    "    skewed = [c for c in num_cols if tr[c].dropna().skew() > 1.0]\n",
    "    for c in skewed[:10]:\n",
    "        try:\n",
    "            cand[f\"{c}_q5auto\"] = pd.qcut(tr[c], 5, labels=False, duplicates=\"drop\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    C = pd.DataFrame(index=tr.index, data=cand)\n",
    "\n",
    "    # MI selection\n",
    "    MI_X = C.copy()\n",
    "    for col in MI_X.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(MI_X[col]):\n",
    "            MI_X[col] = pd.Categorical(MI_X[col]).codes\n",
    "    mi = mutual_info_classif(MI_X.fillna(-999), tr[TARGET].astype(int), random_state=SEED)\n",
    "    mi_series = pd.Series(mi, index=MI_X.columns).sort_values(ascending=False)\n",
    "    keep = list(mi_series.head(min(budget, len(mi_series))).index)\n",
    "\n",
    "    # Re-create kept features on test\n",
    "    for k in keep:\n",
    "        if k.endswith(\"_z\"):\n",
    "            base = k[:-2]; mu, sd = tr[base].mean(), tr[base].std(ddof=0) or 1.0\n",
    "            tr[k] = (tr[base] - mu) / sd\n",
    "            te[k] = (te[base] - mu) / sd\n",
    "        elif k.endswith(\"_2\"):\n",
    "            base = k[:-2]\n",
    "            tr[k] = tr[base] * tr[base]; te[k] = te[base] * te[base]\n",
    "        elif \"_x_\" in k:\n",
    "            a, b = k.split(\"_x_\")\n",
    "            tr[k] = tr[a] * tr[b]; te[k] = te[a] * te[b]\n",
    "        elif \"_r_\" in k:\n",
    "            a, b = k.split(\"_r_\")\n",
    "            tr[k] = np.where(tr[b]==0, np.nan, tr[a]/tr[b])\n",
    "            te[k] = np.where(te[b]==0, np.nan, te[a]/te[b])\n",
    "        elif k.endswith(\"_freq\"):\n",
    "            base = k[:-5]; freq = tr[base].value_counts(dropna=False) / len(tr)\n",
    "            tr[k] = tr[base].map(freq); te[k] = te[base].map(freq)\n",
    "        elif k.endswith(\"_q5auto\"):\n",
    "            base = k.replace(\"_q5auto\", \"\")\n",
    "            try:\n",
    "                tr[k] = pd.qcut(tr[base], 5, labels=False, duplicates=\"drop\")\n",
    "                te[k] = pd.qcut(te[base], 5, labels=False, duplicates=\"drop\")\n",
    "            except Exception:\n",
    "                tr[k] = np.nan; te[k] = np.nan\n",
    "        else:\n",
    "            tr[k] = C[k]; te[k] = np.nan\n",
    "\n",
    "    return tr, te, keep, mi_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccabab3",
   "metadata": {
    "papermill": {
     "duration": 0.006247,
     "end_time": "2025-12-19T14:21:10.130058",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.123811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 09. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5471a386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:10.145245Z",
     "iopub.status.busy": "2025-12-19T14:21:10.144885Z",
     "iopub.status.idle": "2025-12-19T14:21:10.330458Z",
     "shell.execute_reply": "2025-12-19T14:21:10.329307Z"
    },
    "papermill": {
     "duration": 0.19545,
     "end_time": "2025-12-19T14:21:10.332259",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.136809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_feature_set_for_model(model_name: str, set_name: str,\n",
    "                                   train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    feature_cols, preproc = preprocessor_for_model(model_name, train_df)\n",
    "\n",
    "    X = train_df[feature_cols].copy()\n",
    "    y = train_df[TARGET].astype(int)\n",
    "    X_te = test_df[feature_cols].copy() if test_df is not None else None\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    oof = np.zeros(len(train_df), dtype=np.float32)\n",
    "    test_preds, fold_aucs = [], []\n",
    "    fold_importance_sums = None\n",
    "\n",
    "    t0 = time.time()\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
    "        if time_up():\n",
    "            print(f\"[{set_name} | {model_name}] Time limit hit after fold {fold-1}.\")\n",
    "            break\n",
    "\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        fitted_pre = preproc.fit(X_tr)\n",
    "        Xtr_t = fitted_pre.transform(X_tr)\n",
    "        Xva_t = fitted_pre.transform(X_va)\n",
    "        Xte_t = fitted_pre.transform(X_te) if X_te is not None else None\n",
    "\n",
    "        est = FACTORIES[model_name]()\n",
    "        if model_name == \"lightgbm_gpu\":\n",
    "            va_pred, te_pred, imp = fit_predict_lightgbm(est, Xtr_t, y_tr, Xva_t, y_va, Xte_t)\n",
    "        elif model_name == \"xgboost_gpu\":\n",
    "            va_pred, te_pred, imp = fit_predict_xgb(est, Xtr_t, y_tr, Xva_t, y_va, Xte_t)\n",
    "        elif model_name == \"catboost_gpu\":\n",
    "            va_pred, te_pred, imp = fit_predict_cat(est, Xtr_t, y_tr, Xva_t, y_va, Xte_t)\n",
    "        else:\n",
    "            va_pred, te_pred, imp = fit_predict_sklearn(est, Xtr_t, y_tr, Xva_t, y_va, Xte_t)\n",
    "\n",
    "        oof[va_idx] = va_pred\n",
    "        if te_pred is not None:\n",
    "            test_preds.append(te_pred)\n",
    "\n",
    "        fold_aucs.append(roc_auc_score(y_va, va_pred))\n",
    "\n",
    "        try:\n",
    "            names = fitted_pre.get_feature_names_out()\n",
    "        except Exception:\n",
    "            names = np.array([f\"f_{i}\" for i in range(Xtr_t.shape[1])])\n",
    "        if imp is not None:\n",
    "            imp = np.asarray(imp, dtype=float)\n",
    "            if fold_importance_sums is None:\n",
    "                fold_importance_sums = pd.Series(0.0, index=names)\n",
    "            k = min(len(fold_importance_sums), len(imp))\n",
    "            fold_importance_sums.iloc[:k] += imp[:k]\n",
    "\n",
    "        del Xtr_t, Xva_t, Xte_t\n",
    "        gc.collect()\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    if not fold_aucs:\n",
    "        print(f\"[{set_name} | {model_name}] No folds completed.\")\n",
    "        return None\n",
    "\n",
    "    cv_mean, cv_std = float(np.mean(fold_aucs)), float(np.std(fold_aucs))\n",
    "    test_pred = np.mean(test_preds, axis=0) if test_preds else None\n",
    "\n",
    "    oof_path, sub_path = save_oof_and_sub(set_name, model_name, oof, test_pred,\n",
    "                                          train_df[ID_COL].values,\n",
    "                                          (test_df[ID_COL].values if (test_df is not None and test_pred is not None) else None))\n",
    "    imp_path = None\n",
    "    if fold_importance_sums is not None:\n",
    "        imp_path = save_importance_csv(set_name, model_name, fold_importance_sums.index, fold_importance_sums.values)\n",
    "\n",
    "    row = {\n",
    "        \"version\": VERSION,\n",
    "        \"feature_set\": set_name,\n",
    "        \"model\": model_name,\n",
    "        \"cv_auc_mean\": cv_mean,\n",
    "        \"cv_auc_std\": cv_std,\n",
    "        \"folds_completed\": len(fold_aucs),\n",
    "        \"train_time_sec\": round(elapsed, 2),\n",
    "        \"train_time_hms\": seconds_to_str(elapsed),\n",
    "        \"timestamp\": pd.Timestamp.utcnow().isoformat(),\n",
    "        \"oof_path\": oof_path,\n",
    "        \"sub_path\": sub_path,\n",
    "        \"importance_path\": imp_path\n",
    "    }\n",
    "    write_results_row(row)\n",
    "    print(f\"[{set_name} | {model_name}] CV AUC: {cv_mean:.6f} Â± {cv_std:.6f} | time {seconds_to_str(elapsed)}\")\n",
    "    print(f\"[{set_name} | {model_name}] OOF -> {oof_path}\")\n",
    "    if sub_path: print(f\"[{set_name} | {model_name}] SUB -> {sub_path}\")\n",
    "    if imp_path: print(f\"[{set_name} | {model_name}] IMP -> {imp_path}\")\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710f501",
   "metadata": {
    "papermill": {
     "duration": 0.007691,
     "end_time": "2025-12-19T14:21:10.346377",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.338686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Building Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99be5e05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:10.368184Z",
     "iopub.status.busy": "2025-12-19T14:21:10.367809Z",
     "iopub.status.idle": "2025-12-19T14:21:10.374321Z",
     "shell.execute_reply": "2025-12-19T14:21:10.372497Z"
    },
    "papermill": {
     "duration": 0.023901,
     "end_time": "2025-12-19T14:21:10.376745",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.352844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Ensure reconstructed frames exist (reuse prebuilt CSVs if configured earlier)\n",
    "# if 'train_recon' not in globals() or 'test_recon' not in globals():\n",
    "#     if USE_PREBUILT_RECON:\n",
    "#         train_recon = pd.read_csv(RECON_TRAIN_PATH)\n",
    "#         test_recon  = pd.read_csv(RECON_TEST_PATH)\n",
    "#         if TARGET in train_recon.columns and TARGET in train.columns:\n",
    "#             try: train_recon[TARGET] = train_recon[TARGET].astype(train[TARGET].dtype)\n",
    "#             except: pass\n",
    "#         for df, ref in [(train_recon, train), (test_recon, test)]:\n",
    "#             if ID_COL in df.columns and ID_COL in ref.columns:\n",
    "#                 try: df[ID_COL] = df[ID_COL].astype(ref[ID_COL].dtype)\n",
    "#                 except: pass\n",
    "#         print(\"Loaded reconstructed CSVs:\",\n",
    "#               f\"train_recon {train_recon.shape} | test_recon {test_recon.shape}\")\n",
    "#     else:\n",
    "#         train_recon, test_recon = train.copy(), test.copy()\n",
    "#         print(\"Using original train/test as fallback for reconstruction-dependent FE.\")\n",
    "\n",
    "# # B. Raw-EDA features on competition columns (ENGINEERED)\n",
    "# train_raw, test_raw = fe_raw_basic(train, test)\n",
    "\n",
    "# # C. Recon-aware features (ENGINEERED on reconstructed data)\n",
    "# train_recon_fe, test_recon_fe = fe_recon_basic(train_recon, test_recon)\n",
    "\n",
    "# # D. Auto-generated sets on reconstructed space (ENGINEERED; MI-selected)\n",
    "# train_auto20,  test_auto20,  keep20,  mi20  = auto_generate_features(train_recon_fe, test_recon_fe, budget=20)\n",
    "# train_auto50,  test_auto50,  keep50,  mi50  = auto_generate_features(train_recon_fe, test_recon_fe, budget=50)\n",
    "# train_auto100, test_auto100, keep100, mi100 = auto_generate_features(train_recon_fe, test_recon_fe, budget=100)\n",
    "# train_auto200, test_auto200, keep200, mi200 = auto_generate_features(train_recon_fe, test_recon_fe, budget=200)\n",
    "\n",
    "# print(\"Engineered feature sets ready:\")\n",
    "# for name, df in [\n",
    "#     (\"raw_eda\",   train_raw),\n",
    "#     (\"recon_fe\",  train_recon_fe),\n",
    "#     (\"auto_20\",   train_auto20),\n",
    "#     (\"auto_50\",   train_auto50),\n",
    "#     (\"auto_100\",  train_auto100),\n",
    "#     (\"auto_200\",  train_auto200),\n",
    "# ]:\n",
    "#     print(f\" - {name:10s} -> shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94eb2e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:10.395762Z",
     "iopub.status.busy": "2025-12-19T14:21:10.395286Z",
     "iopub.status.idle": "2025-12-19T14:21:10.402315Z",
     "shell.execute_reply": "2025-12-19T14:21:10.400256Z"
    },
    "papermill": {
     "duration": 0.017875,
     "end_time": "2025-12-19T14:21:10.405065",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.387190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to_save = {\n",
    "#     f\"{OUTPUT_DIR}/train_raw_eda_v{VERSION}.csv\":  train_raw,\n",
    "#     f\"{OUTPUT_DIR}/test_raw_eda_v{VERSION}.csv\":   test_raw,\n",
    "#     f\"{OUTPUT_DIR}/train_recon_fe_v{VERSION}.csv\": train_recon_fe,\n",
    "#     f\"{OUTPUT_DIR}/test_recon_fe_v{VERSION}.csv\":  test_recon_fe,\n",
    "#     f\"{OUTPUT_DIR}/train_auto20_v{VERSION}.csv\":   train_auto20,\n",
    "#     f\"{OUTPUT_DIR}/test_auto20_v{VERSION}.csv\":    test_auto20,\n",
    "#     f\"{OUTPUT_DIR}/train_auto50_v{VERSION}.csv\":   train_auto50,\n",
    "#     f\"{OUTPUT_DIR}/test_auto50_v{VERSION}.csv\":    test_auto50,\n",
    "#     f\"{OUTPUT_DIR}/train_auto100_v{VERSION}.csv\":  train_auto100,\n",
    "#     f\"{OUTPUT_DIR}/test_auto100_v{VERSION}.csv\":   test_auto100,\n",
    "#     f\"{OUTPUT_DIR}/train_auto200_v{VERSION}.csv\":  train_auto200,\n",
    "#     f\"{OUTPUT_DIR}/test_auto200_v{VERSION}.csv\":   test_auto200,\n",
    "# }\n",
    "# for path, df in to_save.items():\n",
    "#     df.to_csv(path, index=False)\n",
    "# print(\"Saved engineered datasets to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ac0e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:21:10.428468Z",
     "iopub.status.busy": "2025-12-19T14:21:10.428087Z",
     "iopub.status.idle": "2025-12-19T14:23:40.124919Z",
     "shell.execute_reply": "2025-12-19T14:23:40.123639Z"
    },
    "papermill": {
     "duration": 149.720203,
     "end_time": "2025-12-19T14:23:40.132379",
     "exception": false,
     "start_time": "2025-12-19T14:21:10.412176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prebuilt engineered feature sets from: /kaggle/input/s05e12-outputs-diabetes-prediction\n",
      "Engineered feature sets loaded:\n",
      " - raw_eda    -> shape (700000, 112)\n",
      " - recon_fe   -> shape (700000, 48)\n",
      " - auto20     -> shape (700000, 68)\n",
      " - auto50     -> shape (700000, 98)\n",
      " - auto100    -> shape (700000, 148)\n",
      " - auto200    -> shape (700000, 215)\n"
     ]
    }
   ],
   "source": [
    "def _load_engineered_pair(name: str, must_have_target: bool = True):\n",
    "    \"\"\"\n",
    "    Loads train_<name>_v{VERSION}.csv and test_<name>_v{VERSION}.csv\n",
    "    from ENGINEERED_DIR, aligns dtypes for ID and TARGET, and returns (train_df, test_df).\n",
    "    \"\"\"\n",
    "    tr_path = f\"{ENGINEERED_DIR}/train_{name}_v{ENGINEERED_VERSION}.csv\"\n",
    "    te_path = f\"{ENGINEERED_DIR}/test_{name}_v{ENGINEERED_VERSION}.csv\"\n",
    "\n",
    "    if not os.path.exists(tr_path):\n",
    "        raise FileNotFoundError(f\"Missing engineered TRAIN file: {tr_path}\")\n",
    "    if not os.path.exists(te_path):\n",
    "        raise FileNotFoundError(f\"Missing engineered TEST file: {te_path}\")\n",
    "\n",
    "    tr_df = pd.read_csv(tr_path)\n",
    "    te_df = pd.read_csv(te_path)\n",
    "\n",
    "    # Align ID dtype\n",
    "    if ID_COL in tr_df.columns:\n",
    "        try: tr_df[ID_COL] = tr_df[ID_COL].astype(train[ID_COL].dtype)\n",
    "        except: pass\n",
    "    if ID_COL in te_df.columns:\n",
    "        try: te_df[ID_COL] = te_df[ID_COL].astype(test[ID_COL].dtype)\n",
    "        except: pass\n",
    "\n",
    "    # Ensure target presence for train if required\n",
    "    if must_have_target and TARGET not in tr_df.columns:\n",
    "        # If the saved engineered file didn't include TARGET, merge it from original train\n",
    "        tr_df = tr_df.merge(train[[ID_COL, TARGET]], on=ID_COL, how=\"left\")\n",
    "\n",
    "    # Safety: ensure no duplicate columns sneaked in\n",
    "    tr_df = tr_df.loc[:, ~tr_df.columns.duplicated()]\n",
    "    te_df = te_df.loc[:, ~te_df.columns.duplicated()]\n",
    "\n",
    "    return tr_df, te_df\n",
    "\n",
    "\n",
    "# Only read engineered sets we need to evaluate now\n",
    "if USE_PREBUILT_ENGINEERED:\n",
    "    print(\"Reading prebuilt engineered feature sets from:\", ENGINEERED_DIR)\n",
    "\n",
    "    # B. Raw-EDA features on competition columns (ENGINEERED)\n",
    "    train_raw,    test_raw    = _load_engineered_pair(\"raw_eda\",    must_have_target=True)\n",
    "\n",
    "    # C. Recon-aware features (ENGINEERED)\n",
    "    train_recon_fe, test_recon_fe = _load_engineered_pair(\"recon_fe\", must_have_target=True)\n",
    "\n",
    "    # D. Auto-generated sets (ENGINEERED; MI-selected)\n",
    "    train_auto20,  test_auto20  = _load_engineered_pair(\"auto20\",   must_have_target=True)\n",
    "    train_auto50,  test_auto50  = _load_engineered_pair(\"auto50\",   must_have_target=True)\n",
    "    train_auto100, test_auto100 = _load_engineered_pair(\"auto100\",  must_have_target=True)\n",
    "    train_auto200, test_auto200 = _load_engineered_pair(\"auto200\",  must_have_target=True)\n",
    "\n",
    "    print(\"Engineered feature sets loaded:\")\n",
    "    for name, df in [\n",
    "        (\"raw_eda\",   train_raw),\n",
    "        (\"recon_fe\",  train_recon_fe),\n",
    "        (\"auto20\",    train_auto20),\n",
    "        (\"auto50\",    train_auto50),\n",
    "        (\"auto100\",   train_auto100),\n",
    "        (\"auto200\",   train_auto200),\n",
    "    ]:\n",
    "        print(f\" - {name:10s} -> shape {df.shape}\")\n",
    "else:\n",
    "    raise RuntimeError(\"USE_PREBUILT_ENGINEERED is False. Set it True to read prebuilt datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eaccf8",
   "metadata": {
    "papermill": {
     "duration": 0.006515,
     "end_time": "2025-12-19T14:23:40.145929",
     "exception": false,
     "start_time": "2025-12-19T14:23:40.139414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81bbb8da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:23:40.161635Z",
     "iopub.status.busy": "2025-12-19T14:23:40.161166Z",
     "iopub.status.idle": "2025-12-19T14:23:40.257533Z",
     "shell.execute_reply": "2025-12-19T14:23:40.256121Z"
    },
    "papermill": {
     "duration": 0.106773,
     "end_time": "2025-12-19T14:23:40.259247",
     "exception": false,
     "start_time": "2025-12-19T14:23:40.152474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded feature-importance tables:\n",
      "auto_100 -> (166, 2) | top3: ['num__physical_activity_minutes_per_week_z', 'num__triglycerides', 'num__physical_activity_minutes_per_week_2']\n",
      "auto_200 -> (233, 2) | top3: ['num__physical_activity_minutes_per_week_z', 'num__physical_activity_minutes_per_week', 'num__physical_activity_minutes_per_week_2']\n",
      "auto_20 -> (86, 2) | top3: ['num__physical_activity_minutes_per_week', 'num__triglycerides', 'num__cholesterol_total']\n",
      "auto_50 -> (116, 2) | top3: ['num__physical_activity_minutes_per_week', 'num__triglycerides', 'num__screen_time_hours_per_day']\n",
      "raw_eda -> (128, 2) | top3: ['num__physical_activity_minutes_per_week', 'num__physical_activity_minutes_per_week_cap', 'num__physical_activity_minutes_per_week_log1p']\n",
      "recon_fe -> (66, 2) | top3: ['num__physical_activity_minutes_per_week', 'num__triglycerides', 'num__cholesterol_total']\n"
     ]
    }
   ],
   "source": [
    "def _read_fi_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Normalize importance >= 0, handle column names\n",
    "    if \"feature\" not in df.columns:\n",
    "        # try to infer\n",
    "        maybe = [c for c in df.columns if c.lower().startswith(\"feat\")]\n",
    "        if maybe:\n",
    "            df = df.rename(columns={maybe[0]: \"feature\"})\n",
    "    if \"importance\" not in df.columns:\n",
    "        maybe = [c for c in df.columns if c.lower().startswith(\"import\")]\n",
    "        if maybe:\n",
    "            df = df.rename(columns={maybe[0]: \"importance\"})\n",
    "    df = df[[\"feature\",\"importance\"]].copy()\n",
    "    df[\"importance\"] = df[\"importance\"].fillna(0).astype(float).clip(lower=0)\n",
    "    # Combine duplicates by sum (just in case)\n",
    "    df = df.groupby(\"feature\", as_index=False)[\"importance\"].sum().sort_values(\"importance\", ascending=False)\n",
    "    return df\n",
    "\n",
    "fi_tables = {k: _read_fi_csv(v) for k, v in FI_PATHS.items()}\n",
    "\n",
    "print(\"Loaded feature-importance tables:\")\n",
    "for k, v in fi_tables.items():\n",
    "    print(k, \"->\", v.shape, \"| top3:\", list(v.head(3)[\"feature\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd1b52c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:23:40.275114Z",
     "iopub.status.busy": "2025-12-19T14:23:40.274411Z",
     "iopub.status.idle": "2025-12-19T14:23:40.286552Z",
     "shell.execute_reply": "2025-12-19T14:23:40.285566Z"
    },
    "papermill": {
     "duration": 0.022111,
     "end_time": "2025-12-19T14:23:40.288188",
     "exception": false,
     "start_time": "2025-12-19T14:23:40.266077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _weights_from_fi(fi_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Return dict feature -> normalized weight in [0,1], with epsilon floor.\"\"\"\n",
    "    if fi_df.empty:\n",
    "        return {}\n",
    "    imp = fi_df[\"importance\"].astype(float).values\n",
    "    m = imp.max() if imp.size else 0.0\n",
    "    if m <= 0:\n",
    "        w = np.zeros_like(imp)\n",
    "    else:\n",
    "        w = imp / m\n",
    "    w = np.maximum(w, 0.0)\n",
    "    weights = dict(zip(fi_df[\"feature\"], w))\n",
    "    return weights\n",
    "\n",
    "def _apply_feature_weights(df: pd.DataFrame, weights: dict) -> pd.DataFrame:\n",
    "    \"\"\"Multiply NUMERIC columns by (weight + WEIGHT_EPS) if column in weights.\"\"\"\n",
    "    if not weights:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for col, w in weights.items():\n",
    "        if col in out.columns and pd.api.types.is_numeric_dtype(out[col]):\n",
    "            out[col] = out[col] * (float(w) + WEIGHT_EPS)\n",
    "    return out\n",
    "\n",
    "def _subset_by_topk(train_df: pd.DataFrame, test_df: pd.DataFrame, fi_df: pd.DataFrame, k: int):\n",
    "    \"\"\"Keep only ID, TARGET and Top-K features that exist in df.\"\"\"\n",
    "    top_feats = list(fi_df.head(k)[\"feature\"])\n",
    "    keep_feats = [c for c in top_feats if c in train_df.columns]\n",
    "    cols = [ID_COL] + ([TARGET] if TARGET in train_df.columns else []) + keep_feats\n",
    "    tr_k = train_df[cols].copy()\n",
    "    te_cols = [c for c in cols if c != TARGET]\n",
    "    te_k = test_df[te_cols].copy()\n",
    "    return tr_k, te_k, keep_feats\n",
    "\n",
    "def _signature_from_cols(cols: list) -> str:\n",
    "    \"\"\"Create a stable signature (for dedup) from a list of feature columns.\"\"\"\n",
    "    return \"|\".join(sorted(cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f1d6a",
   "metadata": {
    "papermill": {
     "duration": 0.008251,
     "end_time": "2025-12-19T14:23:40.303410",
     "exception": false,
     "start_time": "2025-12-19T14:23:40.295159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 12. Run List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e545661d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:23:40.318770Z",
     "iopub.status.busy": "2025-12-19T14:23:40.318405Z",
     "iopub.status.idle": "2025-12-19T14:23:50.524960Z",
     "shell.execute_reply": "2025-12-19T14:23:50.523866Z"
    },
    "papermill": {
     "duration": 10.216636,
     "end_time": "2025-12-19T14:23:50.526666",
     "exception": false,
     "start_time": "2025-12-19T14:23:40.310030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run bundles prepared: 14\n",
      "  -> raw_eda_full_unweighted\n",
      "  -> raw_eda_full_weighted\n",
      "  -> recon_fe_full_unweighted\n",
      "  -> recon_fe_full_weighted\n",
      "  -> auto_20_full_unweighted\n",
      "  -> auto_20_full_weighted\n"
     ]
    }
   ],
   "source": [
    "# Map feature set names to loaded dataframes\n",
    "feature_sets = {\n",
    "    \"raw_eda\":   (train_raw,     test_raw),\n",
    "    \"recon_fe\":  (train_recon_fe, test_recon_fe),\n",
    "    \"auto_20\":   (train_auto20,  test_auto20),\n",
    "    \"auto_50\":   (train_auto50,  test_auto50),\n",
    "    \"auto_100\":  (train_auto100, test_auto100),\n",
    "    \"auto_200\":  (train_auto200, test_auto200),\n",
    "}\n",
    "\n",
    "# Safety: align TARGET presence in train and ID presence in test\n",
    "for name, (tr_df, te_df) in feature_sets.items():\n",
    "    if TARGET not in tr_df.columns:\n",
    "        tr_df = tr_df.merge(train[[ID_COL, TARGET]], on=ID_COL, how=\"left\")\n",
    "        feature_sets[name] = (tr_df, te_df)\n",
    "\n",
    "# 12A. Build full-set (unweighted) and full-set (weighted) tasks\n",
    "run_bundles = []  # list of tuples: (set_name, train_df, test_df)\n",
    "\n",
    "# Full sets\n",
    "for set_name, (tr_df, te_df) in feature_sets.items():\n",
    "    # Unweighted\n",
    "    run_bundles.append((f\"{set_name}_full_unweighted\", tr_df, te_df))\n",
    "    # Weighted\n",
    "    fi_df = fi_tables.get(set_name.replace(\"auto_\", \"auto_\"), pd.DataFrame(columns=[\"feature\",\"importance\"]))\n",
    "    w = _weights_from_fi(fi_df)\n",
    "    tr_w = _apply_feature_weights(tr_df.drop(columns=[TARGET], errors=\"ignore\"), w)\n",
    "    if TARGET in tr_df.columns:\n",
    "        tr_w = tr_w.join(tr_df[[ID_COL, TARGET]].set_index(ID_COL), on=ID_COL)\n",
    "    te_w = _apply_feature_weights(te_df, w)\n",
    "    run_bundles.append((f\"{set_name}_full_weighted\", tr_w, te_w))\n",
    "\n",
    "# 12B. Build Top-K subsets across ALL sets, deduplicate by feature-signature\n",
    "seen_sigs = set()\n",
    "topk_pairs = []  # (set_name, k, weighted_flag, tr_df, te_df)\n",
    "\n",
    "for set_name, (tr_df, te_df) in feature_sets.items():\n",
    "    fi_df = fi_tables[set_name]  # guaranteed by mapping above\n",
    "    for k in TOPK_LIST:\n",
    "        # Unweighted subset\n",
    "        tr_k, te_k, keep_feats = _subset_by_topk(tr_df, te_df, fi_df, k)\n",
    "        sig = _signature_from_cols(keep_feats)\n",
    "        if sig not in seen_sigs:\n",
    "            seen_sigs.add(sig)\n",
    "            topk_pairs.append((f\"{set_name}_top{k}_unweighted\", tr_k, te_k))\n",
    "        # Weighted subset (weights from fi of the SAME set)\n",
    "        w = _weights_from_fi(fi_df)\n",
    "        tr_kw = _apply_feature_weights(tr_k.drop(columns=[TARGET], errors=\"ignore\"), w)\n",
    "        if TARGET in tr_k.columns:\n",
    "            tr_kw = tr_kw.join(tr_k[[ID_COL, TARGET]].set_index(ID_COL), on=ID_COL)\n",
    "        te_kw = _apply_feature_weights(te_k, w)\n",
    "        sig_w = _signature_from_cols(keep_feats) + \"_w\"\n",
    "        if sig_w not in seen_sigs:\n",
    "            seen_sigs.add(sig_w)\n",
    "            topk_pairs.append((f\"{set_name}_top{k}_weighted\", tr_kw, te_kw))\n",
    "\n",
    "# Merge topk tasks into run_bundles\n",
    "run_bundles.extend(topk_pairs)\n",
    "\n",
    "print(f\"Total run bundles prepared: {len(run_bundles)}\")\n",
    "for name, _, _ in run_bundles[:6]:\n",
    "    print(\"  ->\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a206d",
   "metadata": {
    "papermill": {
     "duration": 0.006611,
     "end_time": "2025-12-19T14:23:50.540222",
     "exception": false,
     "start_time": "2025-12-19T14:23:50.533611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. The RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d734ec22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:23:50.556409Z",
     "iopub.status.busy": "2025-12-19T14:23:50.556041Z",
     "iopub.status.idle": "2025-12-19T15:24:00.273270Z",
     "shell.execute_reply": "2025-12-19T15:24:00.272152Z"
    },
    "papermill": {
     "duration": 3609.729518,
     "end_time": "2025-12-19T15:24:00.276766",
     "exception": false,
     "start_time": "2025-12-19T14:23:50.547248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw_eda_full_unweighted | hist_gbdt] CV AUC: 0.714577 Â± 0.001078 | time 00:04:42\n",
      "[raw_eda_full_unweighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_raw_eda_full_unweighted_vFE_012.csv\n",
      "[raw_eda_full_unweighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_raw_eda_full_unweighted_vFE_012.csv\n",
      "[raw_eda_full_weighted | hist_gbdt] CV AUC: 0.714577 Â± 0.001078 | time 00:04:40\n",
      "[raw_eda_full_weighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_raw_eda_full_weighted_vFE_012.csv\n",
      "[raw_eda_full_weighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_raw_eda_full_weighted_vFE_012.csv\n",
      "[recon_fe_full_unweighted | hist_gbdt] CV AUC: 0.717598 Â± 0.000832 | time 00:02:45\n",
      "[recon_fe_full_unweighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_recon_fe_full_unweighted_vFE_012.csv\n",
      "[recon_fe_full_unweighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_recon_fe_full_unweighted_vFE_012.csv\n",
      "[recon_fe_full_weighted | hist_gbdt] CV AUC: 0.717598 Â± 0.000832 | time 00:02:40\n",
      "[recon_fe_full_weighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_recon_fe_full_weighted_vFE_012.csv\n",
      "[recon_fe_full_weighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_recon_fe_full_weighted_vFE_012.csv\n",
      "[auto_20_full_unweighted | hist_gbdt] CV AUC: 0.717798 Â± 0.000806 | time 00:03:24\n",
      "[auto_20_full_unweighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_auto_20_full_unweighted_vFE_012.csv\n",
      "[auto_20_full_unweighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_auto_20_full_unweighted_vFE_012.csv\n",
      "[auto_20_full_weighted | hist_gbdt] CV AUC: 0.717798 Â± 0.000806 | time 00:03:22\n",
      "[auto_20_full_weighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_auto_20_full_weighted_vFE_012.csv\n",
      "[auto_20_full_weighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_auto_20_full_weighted_vFE_012.csv\n",
      "[auto_50_full_unweighted | hist_gbdt] CV AUC: 0.717785 Â± 0.000689 | time 00:04:34\n",
      "[auto_50_full_unweighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_auto_50_full_unweighted_vFE_012.csv\n",
      "[auto_50_full_unweighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_auto_50_full_unweighted_vFE_012.csv\n",
      "[auto_50_full_weighted | hist_gbdt] CV AUC: 0.717785 Â± 0.000689 | time 00:04:31\n",
      "[auto_50_full_weighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_auto_50_full_weighted_vFE_012.csv\n",
      "[auto_50_full_weighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_auto_50_full_weighted_vFE_012.csv\n",
      "[auto_100_full_unweighted | hist_gbdt] CV AUC: 0.717249 Â± 0.000797 | time 00:06:14\n",
      "[auto_100_full_unweighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_auto_100_full_unweighted_vFE_012.csv\n",
      "[auto_100_full_unweighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_auto_100_full_unweighted_vFE_012.csv\n",
      "[auto_100_full_weighted | hist_gbdt] CV AUC: 0.717249 Â± 0.000797 | time 00:06:14\n",
      "[auto_100_full_weighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_auto_100_full_weighted_vFE_012.csv\n",
      "[auto_100_full_weighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_auto_100_full_weighted_vFE_012.csv\n",
      "[auto_200_full_unweighted | hist_gbdt] CV AUC: 0.717227 Â± 0.000776 | time 00:08:13\n",
      "[auto_200_full_unweighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_auto_200_full_unweighted_vFE_012.csv\n",
      "[auto_200_full_unweighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_auto_200_full_unweighted_vFE_012.csv\n",
      "[auto_200_full_weighted | hist_gbdt] CV AUC: 0.717227 Â± 0.000776 | time 00:08:08\n",
      "[auto_200_full_weighted | hist_gbdt] OOF -> model_outputs_vFE_012/oof_hist_gbdt_auto_200_full_weighted_vFE_012.csv\n",
      "[auto_200_full_weighted | hist_gbdt] SUB -> model_outputs_vFE_012/sub_hist_gbdt_auto_200_full_weighted_vFE_012.csv\n",
      "[raw_eda_top5_unweighted | hist_gbdt] ERROR: Found array with 0 feature(s) (shape=(560000, 0)) while a minimum of 1 is required by HistGradientBoostingClassifier.\n",
      "[raw_eda_top5_weighted | hist_gbdt] ERROR: Found array with 0 feature(s) (shape=(560000, 0)) while a minimum of 1 is required by HistGradientBoostingClassifier.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>model</th>\n",
       "      <th>cv_auc_mean</th>\n",
       "      <th>cv_auc_std</th>\n",
       "      <th>folds_completed</th>\n",
       "      <th>train_time_sec</th>\n",
       "      <th>train_time_hms</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>oof_path</th>\n",
       "      <th>sub_path</th>\n",
       "      <th>importance_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>auto_20_full_unweighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717798</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>5</td>\n",
       "      <td>204.48</td>\n",
       "      <td>00:03:24</td>\n",
       "      <td>2025-12-19T14:42:16.793854+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_auto_20_fu...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_auto_20_fu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>auto_20_full_weighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717798</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>5</td>\n",
       "      <td>202.38</td>\n",
       "      <td>00:03:22</td>\n",
       "      <td>2025-12-19T14:45:41.388445+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_auto_20_fu...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_auto_20_fu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>auto_50_full_unweighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717785</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>5</td>\n",
       "      <td>274.64</td>\n",
       "      <td>00:04:34</td>\n",
       "      <td>2025-12-19T14:50:19.257864+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_auto_50_fu...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_auto_50_fu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>auto_50_full_weighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717785</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>5</td>\n",
       "      <td>271.08</td>\n",
       "      <td>00:04:31</td>\n",
       "      <td>2025-12-19T14:54:52.705737+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_auto_50_fu...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_auto_50_fu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>recon_fe_full_unweighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717598</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>5</td>\n",
       "      <td>165.96</td>\n",
       "      <td>00:02:45</td>\n",
       "      <td>2025-12-19T14:36:07.012460+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_recon_fe_f...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_recon_fe_f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>recon_fe_full_weighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717598</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>5</td>\n",
       "      <td>160.94</td>\n",
       "      <td>00:02:40</td>\n",
       "      <td>2025-12-19T14:38:49.958431+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_recon_fe_f...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_recon_fe_f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>auto_100_full_unweighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717249</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>5</td>\n",
       "      <td>374.32</td>\n",
       "      <td>00:06:14</td>\n",
       "      <td>2025-12-19T15:01:10.555943+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_auto_100_f...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_auto_100_f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>auto_100_full_weighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717249</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>5</td>\n",
       "      <td>374.57</td>\n",
       "      <td>00:06:14</td>\n",
       "      <td>2025-12-19T15:07:27.733844+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_auto_100_f...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_auto_100_f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>auto_200_full_unweighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717227</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>5</td>\n",
       "      <td>493.03</td>\n",
       "      <td>00:08:13</td>\n",
       "      <td>2025-12-19T15:15:45.439608+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_auto_200_f...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_auto_200_f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>auto_200_full_weighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.717227</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>5</td>\n",
       "      <td>488.75</td>\n",
       "      <td>00:08:08</td>\n",
       "      <td>2025-12-19T15:23:59.911100+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_auto_200_f...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_auto_200_f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>raw_eda_full_unweighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.714577</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>5</td>\n",
       "      <td>282.72</td>\n",
       "      <td>00:04:42</td>\n",
       "      <td>2025-12-19T14:28:35.677839+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_raw_eda_fu...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_raw_eda_fu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FE_012</td>\n",
       "      <td>raw_eda_full_weighted</td>\n",
       "      <td>hist_gbdt</td>\n",
       "      <td>0.714577</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>5</td>\n",
       "      <td>280.78</td>\n",
       "      <td>00:04:40</td>\n",
       "      <td>2025-12-19T14:33:18.906118+00:00</td>\n",
       "      <td>model_outputs_vFE_012/oof_hist_gbdt_raw_eda_fu...</td>\n",
       "      <td>model_outputs_vFE_012/sub_hist_gbdt_raw_eda_fu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               feature_set      model  cv_auc_mean  cv_auc_std  \\\n",
       "4   FE_012   auto_20_full_unweighted  hist_gbdt     0.717798    0.000806   \n",
       "5   FE_012     auto_20_full_weighted  hist_gbdt     0.717798    0.000806   \n",
       "6   FE_012   auto_50_full_unweighted  hist_gbdt     0.717785    0.000689   \n",
       "7   FE_012     auto_50_full_weighted  hist_gbdt     0.717785    0.000689   \n",
       "2   FE_012  recon_fe_full_unweighted  hist_gbdt     0.717598    0.000832   \n",
       "3   FE_012    recon_fe_full_weighted  hist_gbdt     0.717598    0.000832   \n",
       "8   FE_012  auto_100_full_unweighted  hist_gbdt     0.717249    0.000797   \n",
       "9   FE_012    auto_100_full_weighted  hist_gbdt     0.717249    0.000797   \n",
       "10  FE_012  auto_200_full_unweighted  hist_gbdt     0.717227    0.000776   \n",
       "11  FE_012    auto_200_full_weighted  hist_gbdt     0.717227    0.000776   \n",
       "0   FE_012   raw_eda_full_unweighted  hist_gbdt     0.714577    0.001078   \n",
       "1   FE_012     raw_eda_full_weighted  hist_gbdt     0.714577    0.001078   \n",
       "\n",
       "    folds_completed  train_time_sec train_time_hms  \\\n",
       "4                 5          204.48       00:03:24   \n",
       "5                 5          202.38       00:03:22   \n",
       "6                 5          274.64       00:04:34   \n",
       "7                 5          271.08       00:04:31   \n",
       "2                 5          165.96       00:02:45   \n",
       "3                 5          160.94       00:02:40   \n",
       "8                 5          374.32       00:06:14   \n",
       "9                 5          374.57       00:06:14   \n",
       "10                5          493.03       00:08:13   \n",
       "11                5          488.75       00:08:08   \n",
       "0                 5          282.72       00:04:42   \n",
       "1                 5          280.78       00:04:40   \n",
       "\n",
       "                           timestamp  \\\n",
       "4   2025-12-19T14:42:16.793854+00:00   \n",
       "5   2025-12-19T14:45:41.388445+00:00   \n",
       "6   2025-12-19T14:50:19.257864+00:00   \n",
       "7   2025-12-19T14:54:52.705737+00:00   \n",
       "2   2025-12-19T14:36:07.012460+00:00   \n",
       "3   2025-12-19T14:38:49.958431+00:00   \n",
       "8   2025-12-19T15:01:10.555943+00:00   \n",
       "9   2025-12-19T15:07:27.733844+00:00   \n",
       "10  2025-12-19T15:15:45.439608+00:00   \n",
       "11  2025-12-19T15:23:59.911100+00:00   \n",
       "0   2025-12-19T14:28:35.677839+00:00   \n",
       "1   2025-12-19T14:33:18.906118+00:00   \n",
       "\n",
       "                                             oof_path  \\\n",
       "4   model_outputs_vFE_012/oof_hist_gbdt_auto_20_fu...   \n",
       "5   model_outputs_vFE_012/oof_hist_gbdt_auto_20_fu...   \n",
       "6   model_outputs_vFE_012/oof_hist_gbdt_auto_50_fu...   \n",
       "7   model_outputs_vFE_012/oof_hist_gbdt_auto_50_fu...   \n",
       "2   model_outputs_vFE_012/oof_hist_gbdt_recon_fe_f...   \n",
       "3   model_outputs_vFE_012/oof_hist_gbdt_recon_fe_f...   \n",
       "8   model_outputs_vFE_012/oof_hist_gbdt_auto_100_f...   \n",
       "9   model_outputs_vFE_012/oof_hist_gbdt_auto_100_f...   \n",
       "10  model_outputs_vFE_012/oof_hist_gbdt_auto_200_f...   \n",
       "11  model_outputs_vFE_012/oof_hist_gbdt_auto_200_f...   \n",
       "0   model_outputs_vFE_012/oof_hist_gbdt_raw_eda_fu...   \n",
       "1   model_outputs_vFE_012/oof_hist_gbdt_raw_eda_fu...   \n",
       "\n",
       "                                             sub_path  importance_path  \n",
       "4   model_outputs_vFE_012/sub_hist_gbdt_auto_20_fu...              NaN  \n",
       "5   model_outputs_vFE_012/sub_hist_gbdt_auto_20_fu...              NaN  \n",
       "6   model_outputs_vFE_012/sub_hist_gbdt_auto_50_fu...              NaN  \n",
       "7   model_outputs_vFE_012/sub_hist_gbdt_auto_50_fu...              NaN  \n",
       "2   model_outputs_vFE_012/sub_hist_gbdt_recon_fe_f...              NaN  \n",
       "3   model_outputs_vFE_012/sub_hist_gbdt_recon_fe_f...              NaN  \n",
       "8   model_outputs_vFE_012/sub_hist_gbdt_auto_100_f...              NaN  \n",
       "9   model_outputs_vFE_012/sub_hist_gbdt_auto_100_f...              NaN  \n",
       "10  model_outputs_vFE_012/sub_hist_gbdt_auto_200_f...              NaN  \n",
       "11  model_outputs_vFE_012/sub_hist_gbdt_auto_200_f...              NaN  \n",
       "0   model_outputs_vFE_012/sub_hist_gbdt_raw_eda_fu...              NaN  \n",
       "1   model_outputs_vFE_012/sub_hist_gbdt_raw_eda_fu...              NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for set_name, tr_df, te_df in run_bundles:\n",
    "    for model_name in MODELS_TO_RUN:\n",
    "        if time_up():\n",
    "            print(\"\\n=== Global time limit reached; stopping. ===\")\n",
    "            break\n",
    "        try:\n",
    "            row = evaluate_feature_set_for_model(model_name, set_name, tr_df, te_df)\n",
    "            if row is not None:\n",
    "                results.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"[{set_name} | {model_name}] ERROR:\", e)\n",
    "            continue\n",
    "\n",
    "# Summary table\n",
    "if os.path.exists(RESULTS_CSV):\n",
    "    results_df = pd.read_csv(RESULTS_CSV).sort_values(\n",
    "        [\"cv_auc_mean\",\"feature_set\",\"model\"], ascending=[False, True, True]\n",
    "    )\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results CSV produced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccbae4",
   "metadata": {
    "papermill": {
     "duration": 0.008945,
     "end_time": "2025-12-19T15:24:00.295681",
     "exception": false,
     "start_time": "2025-12-19T15:24:00.286736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    },
    {
     "datasetId": 8316713,
     "sourceId": 13128284,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9004147,
     "sourceId": 14220090,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3799.30315,
   "end_time": "2025-12-19T15:24:02.540463",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-19T14:20:43.237313",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
