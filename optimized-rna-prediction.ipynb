{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abe43b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T20:08:25.962600Z",
     "iopub.status.busy": "2026-02-08T20:08:25.962237Z",
     "iopub.status.idle": "2026-02-08T20:11:32.050038Z",
     "shell.execute_reply": "2026-02-08T20:11:32.048516Z"
    },
    "papermill": {
     "duration": 186.094837,
     "end_time": "2026-02-08T20:11:32.052587",
     "exception": false,
     "start_time": "2026-02-08T20:08:25.957750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/biopython-cp312/biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython==1.86) (2.0.2)\r\n",
      "Installing collected packages: biopython\r\n",
      "Successfully installed biopython-1.86\r\n",
      "Processing 0 | 0.0s\n",
      "Processing 10 | 136.0s\n",
      "Processing 20 | 139.4s\n",
      "submission.csv! saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROVEN 0.370 VERSION\n",
    "====================\n",
    "Alternative baseline that scored 0.370\n",
    "Slightly different from 0.371 version\n",
    "\"\"\"\n",
    "\n",
    "!pip install --no-index /kaggle/input/biopython-cp312/biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
    "\n",
    "# ============================================================\n",
    "# Stanford RNA 3D Folding Part 2 — Deterministic Template Baseline\n",
    "# CPU-only, no external data, no ML training\n",
    "# Output: submission.csv (5 coordinate sets per residue)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import os, sys\n",
    "\n",
    "# Determinism hygiene\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# A) Data ingestion\n",
    "# -----------------------------\n",
    "DATA_PATH = \"/kaggle/input/stanford-rna-3d-folding-2/\"\n",
    "train_seqs   = pd.read_csv(DATA_PATH + \"train_sequences.csv\")\n",
    "test_seqs    = pd.read_csv(DATA_PATH + \"test_sequences.csv\")\n",
    "train_labels = pd.read_csv(DATA_PATH + \"train_labels.csv\")\n",
    "\n",
    "sys.path.append(os.path.join(DATA_PATH, \"extra\"))\n",
    "\n",
    "# --- Robust FASTA parser: prefer dataset extra/parse_fasta_py.py; fallback if import fails ---\n",
    "try:\n",
    "    import typing as _typing\n",
    "    import builtins as _builtins\n",
    "\n",
    "    _builtins.Dict  = getattr(_typing, \"Dict\")\n",
    "    _builtins.Tuple = getattr(_typing, \"Tuple\")\n",
    "    _builtins.List  = getattr(_typing, \"List\")\n",
    "\n",
    "    from parse_fasta_py import parse_fasta as _parse_fasta_raw\n",
    "\n",
    "    # Normalize output to: {chain_id: sequence_string}\n",
    "    def parse_fasta(fasta_content: str):\n",
    "        d = _parse_fasta_raw(fasta_content)\n",
    "        out = {}\n",
    "        for k, v in d.items():\n",
    "            out[k] = v[0] if isinstance(v, tuple) else v\n",
    "        return out\n",
    "\n",
    "except Exception:\n",
    "    # Fallback FASTA parser: {chain_id: sequence_string}\n",
    "    def parse_fasta(fasta_content: str):\n",
    "        out = {}\n",
    "        cur = None\n",
    "        seq_parts = []\n",
    "        for line in str(fasta_content).splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                if cur is not None:\n",
    "                    out[cur] = \"\".join(seq_parts)\n",
    "                header = line[1:]\n",
    "                cur = header.split()[0]  # first token as chain id\n",
    "                seq_parts = []\n",
    "            else:\n",
    "                seq_parts.append(line.replace(\" \", \"\"))\n",
    "        if cur is not None:\n",
    "            out[cur] = \"\".join(seq_parts)\n",
    "        return out\n",
    "\n",
    "def parse_stoichiometry(stoich: str):\n",
    "    if pd.isna(stoich) or str(stoich).strip() == \"\":\n",
    "        return []\n",
    "    out = []\n",
    "    for part in str(stoich).split(\";\"):\n",
    "        ch, cnt = part.split(\":\")\n",
    "        out.append((ch.strip(), int(cnt)))\n",
    "    return out\n",
    "\n",
    "def get_chain_segments(row):\n",
    "    \"\"\"\n",
    "    Returns list of (start,end) segments in row['sequence'] corresponding to chain copies\n",
    "    in stoichiometry order. If parsing fails or mismatch: fallback single segment (0,L).\n",
    "    \"\"\"\n",
    "    seq = row[\"sequence\"]\n",
    "    stoich = row.get(\"stoichiometry\", \"\")\n",
    "    all_seq = row.get(\"all_sequences\", \"\")\n",
    "\n",
    "    if pd.isna(stoich) or pd.isna(all_seq) or str(stoich).strip() == \"\" or str(all_seq).strip() == \"\":\n",
    "        return [(0, len(seq))]\n",
    "\n",
    "    try:\n",
    "        chain_dict = parse_fasta(all_seq)  # chain_id -> sequence\n",
    "        order = parse_stoichiometry(stoich)\n",
    "\n",
    "        segs = []\n",
    "        pos = 0\n",
    "        for ch, cnt in order:\n",
    "            base = chain_dict.get(ch)\n",
    "            if base is None:\n",
    "                return [(0, len(seq))]\n",
    "            for _ in range(cnt):\n",
    "                L = len(base)\n",
    "                segs.append((pos, pos + L))\n",
    "                pos += L\n",
    "\n",
    "        if pos != len(seq):\n",
    "            return [(0, len(seq))]\n",
    "        return segs\n",
    "    except Exception:\n",
    "        return [(0, len(seq))]\n",
    "\n",
    "def build_segments_map(df):\n",
    "    seg_map = {}\n",
    "    stoich_map = {}\n",
    "    for _, r in df.iterrows():\n",
    "        tid = r[\"target_id\"]\n",
    "        seg_map[tid] = get_chain_segments(r)\n",
    "        stoich_map[tid] = str(r.get(\"stoichiometry\", \"\") if not pd.isna(r.get(\"stoichiometry\", \"\")) else \"\")\n",
    "    return seg_map, stoich_map\n",
    "\n",
    "train_segs_map, train_stoich_map = build_segments_map(train_seqs)\n",
    "test_segs_map,  test_stoich_map  = build_segments_map(test_seqs)\n",
    "\n",
    "# -----------------------------\n",
    "# B) Labels to templates\n",
    "# -----------------------------\n",
    "def process_labels(labels_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    train_coords_dict: {target_id: (L,3) coords from x_1,y_1,z_1}\n",
    "    Group key = labels_df['ID'] split at last underscore\n",
    "    \"\"\"\n",
    "    coords_dict = {}\n",
    "    prefixes = labels_df[\"ID\"].str.rsplit(\"_\", n=1).str[0]\n",
    "    for id_prefix, group in labels_df.groupby(prefixes, sort=False):\n",
    "        coords_dict[id_prefix] = group.sort_values(\"resid\")[[\"x_1\", \"y_1\", \"z_1\"]].values\n",
    "    return coords_dict\n",
    "\n",
    "train_coords_dict = process_labels(train_labels)\n",
    "\n",
    "# -----------------------------\n",
    "# C) Similarity search (PairwiseAligner strict global w/ strong gaps)\n",
    "# -----------------------------\n",
    "from Bio.Align import PairwiseAligner\n",
    "\n",
    "aligner = PairwiseAligner()\n",
    "aligner.mode = \"global\"\n",
    "aligner.match_score = 2\n",
    "aligner.mismatch_score = -1.5\n",
    "aligner.open_gap_score = -8\n",
    "aligner.extend_gap_score = -0.4\n",
    "\n",
    "# Explicit terminal gap penalties (avoid end-gap sliding)\n",
    "aligner.query_left_open_gap_score = -8\n",
    "aligner.query_left_extend_gap_score = -0.4\n",
    "aligner.query_right_open_gap_score = -8\n",
    "aligner.query_right_extend_gap_score = -0.4\n",
    "aligner.target_left_open_gap_score = -8\n",
    "aligner.target_left_extend_gap_score = -0.4\n",
    "aligner.target_right_open_gap_score = -8\n",
    "aligner.target_right_extend_gap_score = -0.4\n",
    "\n",
    "def find_similar_sequences(query_seq: str, train_seqs_df: pd.DataFrame, train_coords_dict: dict, top_n: int = 5):\n",
    "    similar_seqs = []\n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        target_id = row[\"target_id\"]\n",
    "        train_seq = row[\"sequence\"]\n",
    "        if target_id not in train_coords_dict:\n",
    "            continue\n",
    "\n",
    "        if abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq)) > 0.3:\n",
    "            continue\n",
    "\n",
    "        raw_score = aligner.score(query_seq, train_seq)\n",
    "        normalized_score = raw_score / (2 * min(len(query_seq), len(train_seq)))\n",
    "        similar_seqs.append((target_id, train_seq, normalized_score, train_coords_dict[target_id]))\n",
    "\n",
    "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return similar_seqs[:top_n]\n",
    "\n",
    "# -----------------------------\n",
    "# D) Template transfer (alignment.aligned block mapping + interpolation fill)\n",
    "# -----------------------------\n",
    "def adapt_template_to_query(query_seq: str, template_seq: str, template_coords: np.ndarray):\n",
    "    alignment = next(iter(aligner.align(query_seq, template_seq)))\n",
    "    new_coords = np.full((len(query_seq), 3), np.nan, dtype=float)\n",
    "\n",
    "    # Vectorized chunk mapping via aligned blocks\n",
    "    for (q_start, q_end), (t_start, t_end) in zip(*alignment.aligned):\n",
    "        t_chunk = template_coords[t_start:t_end]\n",
    "        if len(t_chunk) == (q_end - q_start):\n",
    "            new_coords[q_start:q_end] = t_chunk\n",
    "\n",
    "    # Fill unmatched residues by interpolation / edge-fill / fallback line\n",
    "    for i in range(len(new_coords)):\n",
    "        if np.isnan(new_coords[i, 0]):\n",
    "            prev_v = next((j for j in range(i - 1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            next_v = next((j for j in range(i + 1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n",
    "\n",
    "            if prev_v >= 0 and next_v >= 0:\n",
    "                w = (i - prev_v) / (next_v - prev_v)\n",
    "                new_coords[i] = (1 - w) * new_coords[prev_v] + w * new_coords[next_v]\n",
    "            elif prev_v >= 0:\n",
    "                new_coords[i] = new_coords[prev_v] + [3, 0, 0]\n",
    "            elif next_v >= 0:\n",
    "                new_coords[i] = new_coords[next_v] + [3, 0, 0]\n",
    "            else:\n",
    "                new_coords[i] = [i * 3, 0, 0]\n",
    "\n",
    "    return np.nan_to_num(new_coords)\n",
    "\n",
    "# -----------------------------\n",
    "# E) Segment-aware local refinement (US-align compatible)\n",
    "# -----------------------------\n",
    "def adaptive_rna_constraints(coordinates: np.ndarray, target_id: str, confidence: float = 1.0, passes: int = 2):\n",
    "    \"\"\"\n",
    "    Apply within each chain segment only (no smoothing across chain breaks).\n",
    "    - i,i+1 bond target ~5.95 Å (symmetric)\n",
    "    - i,i+2 target ~10.2 Å (symmetric)\n",
    "    - small Laplacian smoothing\n",
    "    - light self-avoidance on subsampled points\n",
    "    Strength increases when confidence is lower:\n",
    "      strength = max(0.02, 0.75*(1-min(conf,0.90)))\n",
    "    \"\"\"\n",
    "    coords = coordinates.copy()\n",
    "    segments = test_segs_map.get(target_id, [(0, len(coords))])\n",
    "\n",
    "    strength = 0.75 * (1.0 - min(confidence, 0.90))\n",
    "    strength = max(strength, 0.02)\n",
    "\n",
    "    for _ in range(passes):\n",
    "        for (s, e) in segments:\n",
    "            X = coords[s:e]\n",
    "            L = e - s\n",
    "            if L < 3:\n",
    "                coords[s:e] = X\n",
    "                continue\n",
    "\n",
    "            # (1) bond i,i+1 to ~5.95Å\n",
    "            d = X[1:] - X[:-1]\n",
    "            dist = np.linalg.norm(d, axis=1) + 1e-6\n",
    "            target = 5.95\n",
    "            scale = (target - dist) / dist\n",
    "            adj = (d * scale[:, None]) * (0.22 * strength)\n",
    "            X[:-1] -= adj\n",
    "            X[1:]  += adj\n",
    "\n",
    "            # (2) soft i,i+2 to ~10.2Å\n",
    "            d2 = X[2:] - X[:-2]\n",
    "            dist2 = np.linalg.norm(d2, axis=1) + 1e-6\n",
    "            target2 = 10.2\n",
    "            scale2 = (target2 - dist2) / dist2\n",
    "            adj2 = (d2 * scale2[:, None]) * (0.10 * strength)\n",
    "            X[:-2] -= adj2\n",
    "            X[2:]  += adj2\n",
    "\n",
    "            # (3) Laplacian smoothing\n",
    "            lap = 0.5 * (X[:-2] + X[2:]) - X[1:-1]\n",
    "            X[1:-1] += (0.06 * strength) * lap\n",
    "\n",
    "            # (4) light self-avoidance (subsample)\n",
    "            if L >= 25:\n",
    "                k = min(L, 160) if L > 220 else L\n",
    "                idx = np.linspace(0, L - 1, k).astype(int) if k < L else np.arange(L)\n",
    "\n",
    "                P = X[idx]\n",
    "                diff = P[:, None, :] - P[None, :, :]\n",
    "                distm = np.linalg.norm(diff, axis=2) + 1e-6\n",
    "                sep = np.abs(idx[:, None] - idx[None, :])\n",
    "\n",
    "                mask = (sep > 2) & (distm < 3.2)\n",
    "                if np.any(mask):\n",
    "                    force = (3.2 - distm) / distm\n",
    "                    vec = (diff * force[:, :, None] * mask[:, :, None]).sum(axis=1)\n",
    "                    X[idx] += (0.015 * strength) * vec\n",
    "\n",
    "            coords[s:e] = X\n",
    "\n",
    "    return coords\n",
    "\n",
    "# -----------------------------\n",
    "# F) Best-of-5 predictions (deterministic seed)\n",
    "# -----------------------------\n",
    "def _rotmat(axis, ang):\n",
    "    axis = np.asarray(axis, float)\n",
    "    axis = axis / (np.linalg.norm(axis) + 1e-12)\n",
    "    x, y, z = axis\n",
    "    c, s = np.cos(ang), np.sin(ang)\n",
    "    C = 1.0 - c\n",
    "    return np.array(\n",
    "        [\n",
    "            [c + x * x * C,     x * y * C - z * s, x * z * C + y * s],\n",
    "            [y * x * C + z * s, c + y * y * C,     y * z * C - x * s],\n",
    "            [z * x * C - y * s, z * y * C + x * s, c + z * z * C],\n",
    "        ],\n",
    "        dtype=float,\n",
    "    )\n",
    "\n",
    "def apply_hinge(coords, seg, rng, max_angle_deg=25):\n",
    "    s, e = seg\n",
    "    L = e - s\n",
    "    if L < 30:\n",
    "        return coords\n",
    "    pivot = s + int(rng.integers(10, L - 10))\n",
    "    axis = rng.normal(size=3)\n",
    "    ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n",
    "    R = _rotmat(axis, ang)\n",
    "    X = coords.copy()\n",
    "    p0 = X[pivot].copy()\n",
    "    X[pivot + 1 : e] = (X[pivot + 1 : e] - p0) @ R.T + p0\n",
    "    return X\n",
    "\n",
    "def jitter_chains(coords, segments, rng, max_angle_deg=12, max_trans=1.5):\n",
    "    X = coords.copy()\n",
    "    global_center = X.mean(axis=0, keepdims=True)\n",
    "    for (s, e) in segments:\n",
    "        axis = rng.normal(size=3)\n",
    "        ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n",
    "        R = _rotmat(axis, ang)\n",
    "        shift = rng.normal(size=3)\n",
    "        shift = shift / (np.linalg.norm(shift) + 1e-12) * float(rng.uniform(0.0, max_trans))\n",
    "        c = X[s:e].mean(axis=0, keepdims=True)\n",
    "        X[s:e] = (X[s:e] - c) @ R.T + c + shift\n",
    "    X -= X.mean(axis=0, keepdims=True) - global_center\n",
    "    return X\n",
    "\n",
    "def smooth_wiggle(coords, segments, rng, amp=0.8):\n",
    "    X = coords.copy()\n",
    "    for (s, e) in segments:\n",
    "        L = e - s\n",
    "        if L < 20:\n",
    "            continue\n",
    "        n_ctrl = 6\n",
    "        ctrl_x = np.linspace(0, L - 1, n_ctrl)\n",
    "        ctrl_disp = rng.normal(0, amp, size=(n_ctrl, 3))\n",
    "        t = np.arange(L)\n",
    "        disp = np.vstack([np.interp(t, ctrl_x, ctrl_disp[:, k]) for k in range(3)]).T\n",
    "        X[s:e] += disp\n",
    "    return X\n",
    "\n",
    "def predict_rna_structures(row, train_seqs_df, train_coords_dict, n_predictions=5):\n",
    "    tid = row[\"target_id\"]\n",
    "    seq = row[\"sequence\"]\n",
    "\n",
    "    # Canonical A/C/G/U only (do not remap)\n",
    "    assert set(seq).issubset(set(\"ACGU\")), f\"Non-ACGU in {tid}; do not remap here.\"\n",
    "\n",
    "    segments = test_segs_map.get(tid, [(0, len(seq))])\n",
    "\n",
    "    # Candidate pool top_n=30 then sample for diversity\n",
    "    cands = find_similar_sequences(\n",
    "        query_seq=seq, train_seqs_df=train_seqs_df, train_coords_dict=train_coords_dict, top_n=30\n",
    "    )\n",
    "    assert all(len(c[3]) == len(c[1]) for c in cands), \"Template coords/seq length mismatch\"\n",
    "\n",
    "    predictions = []\n",
    "    used = set()\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        seed = (abs(hash(tid)) + i * 10007) % (2**32)\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        if not cands:\n",
    "            # Hard fallback: straight line per chain segment\n",
    "            coords = np.zeros((len(seq), 3), dtype=float)\n",
    "            for (s, e) in segments:\n",
    "                for j in range(s + 1, e):\n",
    "                    coords[j] = coords[j - 1] + [5.95, 0, 0]\n",
    "            predictions.append(coords)\n",
    "            continue\n",
    "\n",
    "        # Template choice\n",
    "        if i == 0:\n",
    "            t_id, t_seq, sim, t_coords = cands[0]\n",
    "        else:\n",
    "            K = min(12, len(cands))\n",
    "            sims = np.array([cands[k][2] for k in range(K)], float)\n",
    "            w = np.exp((sims - sims.max()) / 0.08)\n",
    "            for k in range(K):\n",
    "                if cands[k][0] in used:\n",
    "                    w[k] *= 0.10\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            k = int(rng.choice(np.arange(K), p=w))\n",
    "            t_id, t_seq, sim, t_coords = cands[k]\n",
    "\n",
    "        used.add(t_id)\n",
    "\n",
    "        # Transfer coords (no sliding)\n",
    "        adapted = adapt_template_to_query(query_seq=seq, template_seq=t_seq, template_coords=t_coords)\n",
    "\n",
    "        # Diversity transforms, then refinement\n",
    "        if i == 0:\n",
    "            X = adapted\n",
    "        elif i == 1:\n",
    "            X = adapted + rng.normal(0, max(0.01, (0.40 - sim) * 0.06), adapted.shape)\n",
    "        elif i == 2:\n",
    "            longest = max(segments, key=lambda se: se[1] - se[0])\n",
    "            X = apply_hinge(adapted, longest, rng, max_angle_deg=22)\n",
    "        elif i == 3:\n",
    "            X = jitter_chains(adapted, segments, rng, max_angle_deg=10, max_trans=1.0)\n",
    "        else:\n",
    "            X = smooth_wiggle(adapted, segments, rng, amp=0.7)\n",
    "\n",
    "        refined = adaptive_rna_constraints(X, tid, confidence=sim, passes=2)\n",
    "        predictions.append(refined)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# -----------------------------\n",
    "# G) Submission writer (exact schema)\n",
    "# -----------------------------\n",
    "all_predictions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in test_seqs.iterrows():\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Processing {idx} | {time.time() - start_time:.1f}s\")\n",
    "    tid = row[\"target_id\"]\n",
    "    seq = row[\"sequence\"]\n",
    "\n",
    "    preds = predict_rna_structures(row, train_seqs, train_coords_dict, n_predictions=5)\n",
    "\n",
    "    # Safety: each prediction must be (L,3)\n",
    "    L = len(seq)\n",
    "    for p in preds:\n",
    "        assert isinstance(p, np.ndarray) and p.shape == (L, 3), f\"Bad pred shape for {tid}: {getattr(p,'shape',None)}\"\n",
    "        assert np.isfinite(p).all(), f\"Non-finite coords in {tid}\"\n",
    "\n",
    "    for j in range(L):\n",
    "        res = {\"ID\": f\"{tid}_{j+1}\", \"resname\": seq[j], \"resid\": j + 1}\n",
    "        for i in range(5):\n",
    "            res[f\"x_{i+1}\"], res[f\"y_{i+1}\"], res[f\"z_{i+1}\"] = preds[i][j]\n",
    "        all_predictions.append(res)\n",
    "\n",
    "sub = pd.DataFrame(all_predictions)\n",
    "\n",
    "cols = [\"ID\", \"resname\", \"resid\"] + [f\"{c}_{i}\" for i in range(1, 6) for c in [\"x\", \"y\", \"z\"]]\n",
    "\n",
    "# Clip explicitly (competition clips coords; prevent explosions)\n",
    "coord_cols = [c for c in cols if c.startswith((\"x_\", \"y_\", \"z_\"))]\n",
    "sub[coord_cols] = sub[coord_cols].clip(-999.999, 9999.999)\n",
    "\n",
    "sub[cols].to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv! saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c07c6a",
   "metadata": {
    "papermill": {
     "duration": 0.001865,
     "end_time": "2026-02-08T20:11:32.056593",
     "exception": false,
     "start_time": "2026-02-08T20:11:32.054728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15231210,
     "sourceId": 118765,
     "sourceType": "competition"
    },
    {
     "datasetId": 9328538,
     "sourceId": 14604295,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 191.0359,
   "end_time": "2026-02-08T20:11:32.880002",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-08T20:08:21.844102",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
