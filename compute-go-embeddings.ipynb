{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Protein Embedding Generator for CAFA6\n\nThis notebook generates embeddings for protein sequences using ESM2 or ESM3 models.\n\nOutputs: `protein_embeddings_train.npy`, `protein_embeddings_test.npy`, and ID lists.","metadata":{}},{"cell_type":"markdown","source":"## Model Selection","metadata":{}},{"cell_type":"code","source":"# Choose model: 'esm2' or 'esm3'\nMODEL_TYPE = 'esm2'  # Change to 'esm3' for ESM3\n\nif MODEL_TYPE == 'esm2':\n    model_name = 'facebook/esm2_t6_8M_UR50D'\nelif MODEL_TYPE == 'esm3':\n    # ESM3 support (may require installation)\n    # !pip install esm\n    import esm\n    model_name = 'esm3_sm_open_v0'  # Example, adjust as needed\nelse:\n    raise ValueError(\"MODEL_TYPE must be 'esm2' or 'esm3'\")\n\nprint(f'Selected model: {MODEL_TYPE} - {model_name}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup and Load Data","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom Bio import SeqIO\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Data paths\ndata_dir = '/kaggle/input/cafa-6-protein-function-prediction/Train/'\ntrain_seq_file = os.path.join(data_dir, 'train_sequences.fasta')\ntest_seq_file = os.path.join(data_dir, 'testsuperset.fasta')\n\n# Load sequences\ntrain_sequences = {record.id: str(record.seq) for record in SeqIO.parse(train_seq_file, 'fasta')}\ntest_sequences = {record.id: str(record.seq) for record in SeqIO.parse(test_seq_file, 'fasta')}\nprint(f'Loaded {len(train_sequences)} train and {len(test_sequences)} test sequences')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Model and Generate Embeddings","metadata":{}},{"cell_type":"code","source":"if MODEL_TYPE == 'esm2':\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name)\n    model.to(device)\n    model.eval()\n    \n    def get_embedding(sequence):\n        inputs = tokenizer(sequence, return_tensors='pt', truncation=True, max_length=1024).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n        \nelif MODEL_TYPE == 'esm3':\n    # Placeholder for ESM3 (adjust based on actual API)\n    model = esm.pretrained.load_model_and_alphabet(model_name)[0]\n    model.to(device)\n    \n    def get_embedding(sequence):\n        # Implement ESM3 embedding logic\n        pass\n\nprint(f'Model loaded: {model_name}')\n\n# Batch processing\nbatch_size = 10\n\n# Train embeddings\ntrain_ids = list(train_sequences.keys())\ntrain_embeddings = []\nfor i in range(0, len(train_ids), batch_size):\n    batch_ids = train_ids[i:i+batch_size]\n    batch_seqs = [train_sequences[pid] for pid in batch_ids]\n    \n    if MODEL_TYPE == 'esm2':\n        inputs = tokenizer(batch_seqs, return_tensors='pt', truncation=True, max_length=1024, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n    \n    train_embeddings.extend(embeddings)\n    if (i // batch_size) % 10 == 0:\n        print(f'Processed {i + len(batch_ids)} / {len(train_ids)} train proteins')\n\n# Test embeddings\ntest_ids = list(test_sequences.keys())\ntest_embeddings = []\nfor i in range(0, len(test_ids), batch_size):\n    batch_ids = test_ids[i:i+batch_size]\n    batch_seqs = [test_sequences[pid] for pid in batch_ids]\n    \n    if MODEL_TYPE == 'esm2':\n        inputs = tokenizer(batch_seqs, return_tensors='pt', truncation=True, max_length=1024, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n    \n    test_embeddings.extend(embeddings)\n    if (i // batch_size) % 10 == 0:\n        print(f'Processed {i + len(batch_ids)} / {len(test_ids)} test proteins')\n\nprint(f'Embeddings generated. Train: {len(train_embeddings)}, Test: {len(test_embeddings)}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Embeddings","metadata":{}},{"cell_type":"code","source":"# Save to numpy files\nnp.save('/kaggle/working/protein_embeddings_train.npy', np.array(train_embeddings))\nnp.save('/kaggle/working/protein_embeddings_test.npy', np.array(test_embeddings))\nnp.save('/kaggle/working/train_ids.npy', np.array(train_ids))\nnp.save('/kaggle/working/test_ids.npy', np.array(test_ids))\n\nprint('Embeddings saved to /kaggle/working/')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}