{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "967dcd4d",
   "metadata": {
    "papermill": {
     "duration": 0.005306,
     "end_time": "2025-05-08T11:02:47.976493",
     "exception": false,
     "start_time": "2025-05-08T11:02:47.971187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Only Submission(LoadLocalTrainModel)\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c6c42",
   "metadata": {
    "papermill": {
     "duration": 0.004239,
     "end_time": "2025-05-08T11:02:47.985535",
     "exception": false,
     "start_time": "2025-05-08T11:02:47.981296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "credit goes to this notebook and author  https://www.kaggle.com/code/hideyukizushi/bird25-onlyinf-v2-s-focallossbce-cv-962-lb-829"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c8f17",
   "metadata": {
    "papermill": {
     "duration": 0.00431,
     "end_time": "2025-05-08T11:02:47.994780",
     "exception": false,
     "start_time": "2025-05-08T11:02:47.990470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "just changed configuration in cfg class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e9e06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.004237,
     "end_time": "2025-05-08T11:02:48.003535",
     "exception": false,
     "start_time": "2025-05-08T11:02:47.999298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **ℹ️INFO**\n",
    "* This notebook is an inference notebook.\n",
    "* that performed a unique LocalTrain based on the great Train/Inference published by the Kadircan İdrisoğlu.\n",
    "    * [PP] https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25\n",
    "    * [TRAIN] https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25\n",
    "    * [INF] https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-inference-birdclef-25\n",
    "\n",
    "### **ℹ️2025/04/28 MyLocalTrainResult**\n",
    "* trained using FocalLossBCE, which was used in the previous competition, BirdCLEF 2024 8th place solution. The results were good.\n",
    "    ```\n",
    "    0.9652\n",
    "    0.9605\n",
    "    0.9607\n",
    "    0.9626\n",
    "    [OOF]0.9622\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd7de64",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-05-08T11:02:48.014554Z",
     "iopub.status.busy": "2025-05-08T11:02:48.014206Z",
     "iopub.status.idle": "2025-05-08T11:03:04.095105Z",
     "shell.execute_reply": "2025-05-08T11:03:04.093977Z"
    },
    "papermill": {
     "duration": 16.089085,
     "end_time": "2025-05-08T11:03:04.097150",
     "exception": false,
     "start_time": "2025-05-08T11:02:48.008065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d024c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.108095Z",
     "iopub.status.busy": "2025-05-08T11:03:04.107716Z",
     "iopub.status.idle": "2025-05-08T11:03:04.114653Z",
     "shell.execute_reply": "2025-05-08T11:03:04.113760Z"
    },
    "papermill": {
     "duration": 0.014338,
     "end_time": "2025-05-08T11:03:04.116475",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.102137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    FocalLossBCE Use Example\n",
    "\"\"\"\n",
    "class FocalLossBCE(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha: float = 0.25,\n",
    "            gamma: float = 2,\n",
    "            reduction: str = \"mean\",\n",
    "            bce_weight: float = 0.6,\n",
    "            focal_weight: float = 1.4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        self.bce_weight = bce_weight\n",
    "        self.focal_weight = focal_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        focall_loss = torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "            inputs=logits,\n",
    "            targets=targets,\n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction,\n",
    "        )\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "        return self.bce_weight * bce_loss + self.focal_weight * focall_loss\n",
    "\n",
    "def get_criterion(cfg):\n",
    "    return FocalLossBCE()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c542682",
   "metadata": {
    "papermill": {
     "duration": 0.004321,
     "end_time": "2025-05-08T11:03:04.125688",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.121367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Inference Pipeline\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d780",
   "metadata": {
    "papermill": {
     "duration": 0.004295,
     "end_time": "2025-05-08T11:03:04.134572",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.130277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **》》》Env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02c2690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.144976Z",
     "iopub.status.busy": "2025-05-08T11:03:04.144610Z",
     "iopub.status.idle": "2025-05-08T11:03:04.150612Z",
     "shell.execute_reply": "2025-05-08T11:03:04.149465Z"
    },
    "papermill": {
     "duration": 0.013531,
     "end_time": "2025-05-08T11:03:04.152641",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.139110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # ------------------------------------------- #\n",
    "    # [IMPORTANT]\n",
    "    # * Melspectrogram & Audio Params\n",
    "    # ------------------------------------------- #\n",
    "    N_FFT = 1034\n",
    "    HOP_LENGTH = 64\n",
    "    N_MELS = 136\n",
    "    FMIN = 20\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "    FS = 32000  \n",
    "    WINDOW_SIZE = 5\n",
    "\n",
    "    # ------------------------------------------- #\n",
    "    # * Model def\n",
    "    # ------------------------------------------- #\n",
    "    model_path = '/kaggle/input/pub-bird25-b-422-ppv15-v2-s-focallossbce'\n",
    "    model_name = 'tf_efficientnetv2_s.in21k_ft_in1k'\n",
    "    use_specific_folds = False\n",
    "    folds = [0,1,2,3]\n",
    "    in_channels = 1\n",
    "    device = 'cpu'  \n",
    "\n",
    "    # datasets\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 16\n",
    "    use_tta = False  \n",
    "    tta_count = 3\n",
    "    threshold = 0.7\n",
    "\n",
    "    # util\n",
    "    debug = False\n",
    "    debug_count = 3\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8d253c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.163631Z",
     "iopub.status.busy": "2025-05-08T11:03:04.163289Z",
     "iopub.status.idle": "2025-05-08T11:03:04.189531Z",
     "shell.execute_reply": "2025-05-08T11:03:04.188123Z"
    },
    "papermill": {
     "duration": 0.033864,
     "end_time": "2025-05-08T11:03:04.191367",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.157503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff2837",
   "metadata": {
    "papermill": {
     "duration": 0.004473,
     "end_time": "2025-05-08T11:03:04.200780",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.196307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **》》》Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28c3433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.211991Z",
     "iopub.status.busy": "2025-05-08T11:03:04.211660Z",
     "iopub.status.idle": "2025-05-08T11:03:04.218477Z",
     "shell.execute_reply": "2025-05-08T11:03:04.217289Z"
    },
    "papermill": {
     "duration": 0.014987,
     "end_time": "2025-05-08T11:03:04.220774",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.205787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=False,  \n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.0,    \n",
    "            drop_path_rate=0.0\n",
    "        )\n",
    "        \n",
    "        backbone_out = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feat_dim = backbone_out\n",
    "        self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626af13e",
   "metadata": {
    "papermill": {
     "duration": 0.0045,
     "end_time": "2025-05-08T11:03:04.230338",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.225838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **》》》Melspectrogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2bdb185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.240913Z",
     "iopub.status.busy": "2025-05-08T11:03:04.240576Z",
     "iopub.status.idle": "2025-05-08T11:03:04.248560Z",
     "shell.execute_reply": "2025-05-08T11:03:04.247447Z"
    },
    "papermill": {
     "duration": 0.015555,
     "end_time": "2025-05-08T11:03:04.250563",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.235008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0,\n",
    "        pad_mode=\"reflect\",\n",
    "        norm='slaney',\n",
    "        htk=True,\n",
    "        center=True,\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4584654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.261904Z",
     "iopub.status.busy": "2025-05-08T11:03:04.261556Z",
     "iopub.status.idle": "2025-05-08T11:03:04.278338Z",
     "shell.execute_reply": "2025-05-08T11:03:04.277361Z"
    },
    "papermill": {
     "duration": 0.024453,
     "end_time": "2025-05-08T11:03:04.280034",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.255581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    model_files = find_model_files(cfg)\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    if cfg.use_specific_folds:\n",
    "        filtered_files = []\n",
    "        for fold in cfg.folds:\n",
    "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "            filtered_files.extend(fold_files)\n",
    "        model_files = filtered_files\n",
    "        print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
    "    \n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n",
    "            \n",
    "            model = BirdCLEFModel(cfg, num_classes)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "        for segment_idx in range(total_segments):\n",
    "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "            segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "            if cfg.use_tta:\n",
    "                all_preds = []\n",
    "                \n",
    "                for tta_idx in range(cfg.tta_count):\n",
    "                    mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                    mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                    mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "                    if len(models) == 1:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = models[0](mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            all_preds.append(probs)\n",
    "                    else:\n",
    "                        segment_preds = []\n",
    "                        for model in models:\n",
    "                            with torch.no_grad():\n",
    "                                outputs = model(mel_spec)\n",
    "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                                segment_preds.append(probs)\n",
    "                        \n",
    "                        avg_preds = np.mean(segment_preds, axis=0)\n",
    "                        all_preds.append(avg_preds)\n",
    "\n",
    "                final_preds = np.mean(all_preds, axis=0)\n",
    "            else:\n",
    "                mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "                if len(models) == 1:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = models[0](mel_spec)\n",
    "                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    segment_preds = []\n",
    "                    for model in models:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            segment_preds.append(probs)\n",
    "\n",
    "                    final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "            predictions.append(final_preds)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41cb46b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.290988Z",
     "iopub.status.busy": "2025-05-08T11:03:04.290639Z",
     "iopub.status.idle": "2025-05-08T11:03:04.299496Z",
     "shell.execute_reply": "2025-05-08T11:03:04.298593Z"
    },
    "papermill": {
     "duration": 0.016129,
     "end_time": "2025-05-08T11:03:04.301005",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.284876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"Apply test-time augmentation\"\"\"\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53660050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.311771Z",
     "iopub.status.busy": "2025-05-08T11:03:04.311444Z",
     "iopub.status.idle": "2025-05-08T11:03:04.317617Z",
     "shell.execute_reply": "2025-05-08T11:03:04.316249Z"
    },
    "papermill": {
     "duration": 0.013586,
     "end_time": "2025-05-08T11:03:04.319418",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.305832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "    submission_path = 'submission.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6d180",
   "metadata": {
    "papermill": {
     "duration": 0.004505,
     "end_time": "2025-05-08T11:03:04.328942",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.324437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Create Submission\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e65f21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:04.339828Z",
     "iopub.status.busy": "2025-05-08T11:03:04.339504Z",
     "iopub.status.idle": "2025-05-08T11:03:15.666570Z",
     "shell.execute_reply": "2025-05-08T11:03:15.665164Z"
    },
    "papermill": {
     "duration": 11.334768,
     "end_time": "2025-05-08T11:03:15.668685",
     "exception": false,
     "start_time": "2025-05-08T11:03:04.333917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "TTA enabled: False (variations: 0)\n",
      "Found a total of 4 model files.\n",
      "Loading model: /kaggle/input/pub-bird25-b-422-ppv15-v2-s-focallossbce/model_0.9626.pth\n",
      "Loading model: /kaggle/input/pub-bird25-b-422-ppv15-v2-s-focallossbce/model_0.9652.pth\n",
      "Loading model: /kaggle/input/pub-bird25-b-422-ppv15-v2-s-focallossbce/model_0.9607.pth\n",
      "Loading model: /kaggle/input/pub-bird25-b-422-ppv15-v2-s-focallossbce/model_0.9605.pth\n",
      "Model usage: Ensemble of 4 models\n",
      "Found 0 test soundscapes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2472607e5949679d21c5b2f16b9b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission dataframe...\n",
      "Submission saved to submission.csv\n",
      "Inference completed in 0.19 minutes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65fdc35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:03:15.682438Z",
     "iopub.status.busy": "2025-05-08T11:03:15.682043Z",
     "iopub.status.idle": "2025-05-08T11:03:15.713311Z",
     "shell.execute_reply": "2025-05-08T11:03:15.712138Z"
    },
    "papermill": {
     "duration": 0.041283,
     "end_time": "2025-05-08T11:03:15.715535",
     "exception": false,
     "start_time": "2025-05-08T11:03:15.674252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6b2e2",
   "metadata": {
    "papermill": {
     "duration": 0.00498,
     "end_time": "2025-05-08T11:03:15.726421",
     "exception": false,
     "start_time": "2025-05-08T11:03:15.721441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7267557,
     "sourceId": 11590046,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.19152,
   "end_time": "2025-05-08T11:03:18.555589",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-08T11:02:44.364069",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1cb19b3cd8ee4c8dad8375aec73a4f6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6273c8d6cafa4efb8bd16d8bd71c9d26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78011d88cf66418da43af87191e8741c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d273711a9b2e43989c6d8e114ba40119",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b20c7b185b4e4be88ff35cc36e9cf0dd",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0
      }
     },
     "8d2472607e5949679d21c5b2f16b9b54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dbfe45df19e64b1c9517bc24c83a6393",
        "IPY_MODEL_78011d88cf66418da43af87191e8741c",
        "IPY_MODEL_bfe3050faa6c4294a20ef2896642f7c0"
       ],
       "layout": "IPY_MODEL_ac2bef9b18c6401dbd54cd72cf79bf72",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ac2bef9b18c6401dbd54cd72cf79bf72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b20c7b185b4e4be88ff35cc36e9cf0dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bfe3050faa6c4294a20ef2896642f7c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d783c400c60d4bcc8767993284ed2bb5",
       "placeholder": "​",
       "style": "IPY_MODEL_1cb19b3cd8ee4c8dad8375aec73a4f6f",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "d273711a9b2e43989c6d8e114ba40119": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "d783c400c60d4bcc8767993284ed2bb5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dbfe45df19e64b1c9517bc24c83a6393": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6273c8d6cafa4efb8bd16d8bd71c9d26",
       "placeholder": "​",
       "style": "IPY_MODEL_e4d3820b95aa443f849ec540ca265bdb",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "e4d3820b95aa443f849ec540ca265bdb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
