{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q pytorch-tabnet lightgbm catboost xgboost\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\n# RMSLE Metric\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, np.maximum(0, y_pred)))\n\n# Dataset\ndf = pd.read_csv(\"path/to/data.csv\")  # Replace with correct path\nTARGET = \"Calories\"\nNUM_FEATURES = [\"Age\", \"Height\", \"Weight\", \"Duration\", \"Heart_Rate\", \"Body_Temp\"]\nCAT_FEATURES = [\"Sex\"]\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(transformers=[\n    ('num', StandardScaler(), NUM_FEATURES),\n    ('cat', OneHotEncoder(handle_unknown=\"ignore\"), CAT_FEATURES)\n])\n\nX = df[NUM_FEATURES + CAT_FEATURES]\ny = df[TARGET].values\nX_proc = preprocessor.fit_transform(X)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = []\nmodels = []\n\n# Blend 5 models: LGBM, CatBoost, XGBoost, TabNet, PyTorch MLP\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_proc)):\n    print(f\"\\nFold {fold+1}\")\n    X_train, X_val = X_proc[train_idx], X_proc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    preds_fold = []\n\n    # LightGBM\n    model_lgb = lgb.LGBMRegressor(n_estimators=1000)\n    model_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n    preds_fold.append(model_lgb.predict(X_val))\n    \n    # CatBoost\n    model_cat = CatBoostRegressor(verbose=0, iterations=1000, early_stopping_rounds=50)\n    model_cat.fit(X_train, y_train, eval_set=(X_val, y_val))\n    preds_fold.append(model_cat.predict(X_val))\n    \n    # XGBoost\n    model_xgb = xgb.XGBRegressor(n_estimators=1000)\n    model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n    preds_fold.append(model_xgb.predict(X_val))\n\n    # TabNet\n    tabnet = TabNetRegressor()\n    tabnet.fit(\n        X_train=X_train, y_train=y_train.reshape(-1, 1),\n        eval_set=[(X_val, y_val.reshape(-1, 1))],\n        eval_metric=['rmse'], max_epochs=200,\n        patience=20, verbose=0\n    )\n    preds_fold.append(tabnet.predict(X_val).ravel())\n\n    # MLP on TPU\n    class MLP(nn.Module):\n        def __init__(self, input_dim):\n            super().__init__()\n            self.model = nn.Sequential(\n                nn.Linear(input_dim, 128),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(128, 64),\n                nn.ReLU(),\n                nn.Linear(64, 1)\n            )\n\n        def forward(self, x):\n            return self.model(x).squeeze(1)\n\n    device = xm.xla_device()\n    model_mlp = MLP(X_train.shape[1]).to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.Adam(model_mlp.parameters(), lr=1e-3)\n\n    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n\n    for epoch in range(20):  # Short training for demonstration\n        model_mlp.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model_mlp(xb)\n            loss = loss_fn(preds, yb)\n            loss.backward()\n            xm.optimizer_step(optimizer)\n\n    model_mlp.eval()\n    with torch.no_grad():\n        val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n        preds_mlp = model_mlp(val_tensor).cpu().numpy()\n    preds_fold.append(preds_mlp)\n\n    # Blend predictions by mean\n    blended = np.mean(preds_fold, axis=0)\n    score = rmsle(y_val, blended)\n    print(f\"Fold RMSLE: {score:.4f}\")\n    oof_preds.extend(blended)\n\nfinal_score = rmsle(y, oof_preds)\nprint(f\"\\nOverall OOF RMSLE: {final_score:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}