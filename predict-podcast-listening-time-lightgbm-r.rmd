# %% [code]
---
title: "Predict Podcast Listening Time | Lightgbm| R"
date: "2025-Apr-03"
output:
  html_document:
    toc: yes
    toc_depth: 6
    code_folding: show
    theme: cosmo
    highlight: tango
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 7,
                      fig.height = 5)

```


## About the Data set:

**Dataset Description:**

The dataset for this competition (both train and test) was generated from a deep learning model trained on the Podcast Listening Time Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.    

**Files**
train.csv - the training dataset; Listening_Time_minutes is the target     
test.csv - the test dataset; your objective is to predict the Listening_Time_minutes for each row         
sample_submission.csv - a sample submission file in the correct format.    


**Goal:** To predict listening time of a podcast episode.       

**Evaluation:** Submissions are scored on the root mean squared error. RMSE  


## Import libraries

```{r import-libraries}


library(tidyverse)
library(janitor)
library(skimr)
library(scales)
library(ggthemes)
library(kableExtra)
library(flextable)
library(paletteer)
library(patchwork)
library(ggcorrplot)
library(GGally)

# Models

library(tidymodels)
library(finetune)
library(stacks)
library(bonsai)
library(vip)


theme_set(theme_light())


```


## Import data

```{r import-data}



train_df <- read_csv("/kaggle/input/playground-series-s5e4/train.csv")  |> 
  clean_names() |> 
  select(-id)


test_df <- read_csv("/kaggle/input/playground-series-s5e4/test.csv") |> 
  clean_names() |>
  select(-id)

sample_submission <- read_csv("/kaggle/input/playground-series-s5e4/sample_submission.csv")



all_Variables <-train_df |>
  colnames()

num_Variables <- train_df |>
  select(where(is.numeric)) |>
  colnames()


num_Variables_no_target <- train_df |>
  select(where(is.numeric), -listening_time_minutes ) |>
  colnames()

```


## EDA

### Basic Summary statistics

```{r}

train_df |>
  skim() |>
  kbl(format = "html",
      caption = "Variables Dignosis | Train",
      digits = 2) |>
  kable_classic(full_width = F)


test_df |>
    skim() |>
  kbl(format = "html",
      caption = "variables dignosis | Test",
      digits = 2) |>
  kable_classic(full_width = F)

```

**Observations:**     

Observations:   

Train_df:   
• The dataset, without the Original dataset, contains 539048 entries with various attributes such as podcast_name, episode_title, host_popularity_percentage, number_of_ads, and listening_time_minutes  
• “episode_sentiment” Three(3) unique values.    
• “publication_time” Four(4) unique values.   
• “publication_day” Seven(7) unique values.   
• “genre” 10 unique values.    
• “episode_title” 100 unique values.   
• “podcast_name” 48 unique values.   

Missing data train_df:
• “episode_length_minutes” 87093 Missing values.  
• “guest_popularity_percentage” 146030 Missing values.   
• “number_of_ads” One(1) Missing value. 

NO Missing data In train df!

Test_df:   
• The dataset contains 250000 entries with various attributes such as podcast_name, episode_title, host_popularity_percentage, number_of_ads, and episode_length_minutes. 

• “episode_length_minutes” 28736  Missing values.  
• “guest_popularity_percentage” 48832  Missing values.  


Features:

• **id:**	Unique identifier for each podcast episode.
• **Podcast_Name:**	Name of the podcast series.
• **Episode_Title:**	Title of the specific episode.
• **Episode_Length_minutes:**	Duration of the episode in minutes.
• **Genre	The category:** of the podcast (e.g., Education, Health).
• **Host_Popularity_percentage:**	Popularity of the podcast host (0–100%).
• **Publication_Day:**	Day of the week the episode was published.
• **Publication_Time:**	Time of day the episode was published.
• **Guest_Popularity_percentage:**	Popularity of the guest (if any) (0–100%).
• **Number_of_Ads:**	Count of advertisements in the episode.
• **Episode_Sentiment:**	Sentiment of the episode (Positive, Negative, etc.).
• **Listening_Time_minutes:**	Target Variable - Actual listening time.


### Target variable Counts and Percentage

```{r}

train_df |>
  tabyl(listening_time_minutes) |>
  arrange(-n) |>  
  head(20) |> 
  kbl(format = "html",
      caption = "Target variable Counts and Percentage",
      digits = 4) |>
  kable_classic(full_width = F)


```


### Target variable distribution 


```{r, fig.width = 7, fig.height = 4}


target_plt <- train_df |> 
  select(listening_time_minutes) |>
  ggplot(aes(listening_time_minutes)) +
  geom_histogram(
    aes(listening_time_minutes),
    alpha = 0.6,
    position = "identity",
    bins = 40,
    color = "black",
    fill = "gray80"
  ) +
  geom_rug(aes(color = listening_time_minutes), show.legend = FALSE) +
  labs(title = "Target variable Distridution",
       caption = "Data source: Kaggle.com, Predict Podcast Listening Time",
       x = "Target variable",
       y = "Counts",

       ) 

target_plt


```


### Distridution of Target variable by Genre

```{r, fig.width = 10, fig.height = 5}

train_df |> 
  select(listening_time_minutes, genre) |>
  ggplot(aes(listening_time_minutes)) +
  geom_boxplot(
    aes(y = genre, x= listening_time_minutes, fill = genre),
    alpha = 0.4,
    position = "identity",
  ) +
  scale_fill_paletteer_d("MoMAColors::Doughton") +
  labs(title = "Distridution of Target variable by Genre",
       caption = "Data source: Kaggle.com, Predict Podcast Listening Time",
       x = "Target variable",
       y = NULL,
       fill = "Genre") 

```


### Distridution of Target variable by Publication day and Genre

```{r, fig.width = 9, fig.height = 8}

train_df |> 
  select(listening_time_minutes, publication_day, genre) |>
  ggplot(aes(listening_time_minutes)) +
  geom_boxplot(
    aes(y = publication_day, x= listening_time_minutes, fill = genre),
    alpha = 0.4,
    position = "identity",
    show.legend = FALSE
  ) +
  facet_wrap(vars(genre), ncol = 3, scale ="free_x")+
  scale_fill_paletteer_d("MoMAColors::Doughton") +
    theme(
    strip.background = element_rect(fill = "white"),
    strip.background.x = element_rect(colour = "white"),
    strip.background.y = element_rect(colour = "white"),
    strip.text = element_text(
      color = "black",
      face = "bold",
      size = 10
    )
  ) +
  labs(title = "Distridution of Target variable by Publication day and Genre",
       caption = "Data source: Kaggle.com, Predict Podcast Listening Time",
       x = "Target variable",
       y = NULL,
       fill = "Publication day") 

```



### Numerical variables by Target variable | Train data

```{r}

train_df |>
  select(all_of(num_Variables_no_target)) |>
  pivot_longer(cols = everything()) |>
  ggplot(aes(
    x = value,
  )) +
  geom_histogram(alpha = 0.5, fill = "darkgreen", color = "#000000") +
  facet_wrap(vars(name), scales = "free") +
  theme(
    strip.background = element_rect(fill = "white", color = "white"),
    strip.text = element_text(colour = 'black', face = "bold")
  ) +
  labs(
    title = "Numerical variables by Target variable",
    caption = "Data source: Kaggle.com, Predict Podcast Listening Time",
    x = "Value",
    y = "Density"
  )


```


### Missing Data rate using skimr package | Train & Test DF

```{r, fig.width = 12, fig.height = 8}

miss_nan <- function(df){
  
  df |> 
  skim() |>
  filter(n_missing != 0) |>
  as_tibble() |>
  select(skim_variable, n_missing, complete_rate) |>
  mutate(missing_rate = round(abs(complete_rate - 1) * 100, 1)) |>
  ggplot(aes(
    x = fct_reorder(skim_variable, n_missing),
    y = missing_rate,
    fill = skim_variable,
    label = paste0(missing_rate, "%")
  )) +
  geom_col() +
  geom_text(
    size = 3,
    hjust = -0.1,
    vjust = 0.25,
    col = "black"
  ) +
  coord_flip() +
  scale_fill_paletteer_d("tvthemes::simpsons")+
  scale_y_continuous(label = label_percent(scale = 1))+
  theme(legend.position = "none") 
  
}


(miss_nan(train_df) +
  labs(
    title = "Missing Data rate using skimr package | Train",
    subtitle = "Plot, Missing Data distribution",
    x = NULL,
    y = NULL
  )) /

(miss_nan(test_df) +
  labs(
    title = "Missing Data rate using skimr package | Test",
    caption = "Data source: Kaggle.com | Predict Podcast Listening Time",
    x = NULL,
    y = NULL
  ))

  

```


### Episode Length | Minutes

```{r, fig.width = 10, fig.height = 5}


train_df |>
  select(episode_length_minutes) |> 
  drop_na() |> 
  ggplot(aes(x = episode_length_minutes)) +
  geom_histogram(
    linewidth = 0.7,
    binwidth = 20,
    bins = 5,
    show.legend = FALSE,
    fill = "gray",
    color = "white"
  ) +
  geom_text(stat = "bin", aes(y = after_stat(count), label = after_stat(count)),
            binwidth = 20, vjust = -0.2) +
  theme(
    axis.text.x = element_text(angle = 90),
    strip.background = element_rect(fill = "white", color = "white"),
    strip.text = element_text(colour = 'black')
  ) +
  labs(
    title = "Episode Length | Minutes",
    caption = "Data source: Kaggle.com | Predict Podcast Listening Time",
    x = NULL,
    y = "Counts"
  )


```


### Podcast Name VS Average Listening Time by group \nCompered to the Average Listening Time  | Minutes

```{r, fig.width = 6, fig.height = 8}

train_df |>
  group_by(podcast_name) |>
  summarize(avg_listen = mean(listening_time_minutes)) |>
  ungroup() |>
  ggplot(aes(y = reorder(podcast_name, avg_listen), x = avg_listen)) +
  geom_point(
    aes(fill = podcast_name),
    show.legend = FALSE,
    color = "steelblue",
    size = 2.5
  )  +
  geom_vline(
    xintercept = mean(train_df$listening_time_minutes),
    color = "darkred",
    linewidth = 0.8,
    linetype = 2
  ) +
  labs(
    title = "Podcast Name VS Average Listening Time by group \nCompered to the Average Listening Time  | Minutes",
    caption = "Data source: Kaggle.com | Predict Podcast Listening Time",
    x = "Average Listening Time by Podcast Name",
    y = "Podcast Name"
  )

```

### Episode Length Minutes VS Listening Time  | Minutes

```{r, fig.width = 9, fig.height = 6}

train_df |>
  ggplot(aes(x = episode_length_minutes, y = listening_time_minutes)) +
  geom_point(
    show.legend = FALSE,
    color = "steelblue",
    size = 1.5,
    shape = 21,
    fill = "white"
  ) +
  geom_smooth(color = "darkgreen") +
  labs(
    title = "Episode Length Minutes VS Listening Time  | Minutes",
    caption = "Data source: Kaggle.com | Predict Podcast Listening Time",
    x = "Episode Length | Minutes",
    y = " Listening Time | Minutes"
  )




```


### Episode Length Minutes VS Listening Time | replace 300 with 30 | Minutes

```{r, fig.width = 9, fig.height = 6}
train_df |>
  mutate(episode_length_minutes = if_else(episode_length_minutes >= 300, 30, episode_length_minutes)) |> 
  ggplot(aes(x = episode_length_minutes, y = listening_time_minutes)) +
  geom_point(
    show.legend = FALSE,
    color = "steelblue",
    size = 1.5,
    shape = 21,
    fill = "white"
  ) +
  geom_smooth(color = "darkgreen") +
  labs(
    title = "Episode Length Minutes VS Listening Time replace 300 with 30 | Minutes",
    caption = "Data source: Kaggle.com | Predict Podcast Listening Time",
    x = "Episode Length | Minutes",
    y = " Listening Time | Minutes"
  )


```


                                                             
### Is Listening Time Longer or Shorter than  Episode Length ?

```{r, fig.width = 8, fig.height = 3}


train_df |>
  mutate(el_lt = if_else(listening_time_minutes > episode_length_minutes, "longer", "shorter")) |> 
  select(el_lt) |> 
  ggplot(aes(y = el_lt)) +
  geom_bar(fill = "black",
           color = "blue") +
  labs(title = "Is Listening Time Longer or Shorter than  Episode Length ?",
       y = NULL,
       x = "Counts")


```



### Listening Time by Episode Sentiment and Publication time!

```{r, fig.width = 9, fig.height = 8}

train_df |>
  ggplot(aes(y = episode_sentiment, listening_time_minutes, publication_time)) +
  geom_boxplot(
    fill = "black",
    color = "darkgreen",
    linewidth = 0.8
  ) +
  facet_wrap(vars(publication_time)) +
  theme(
    strip.background = element_rect(fill = "white"),
    strip.background.x = element_rect(colour = "white"),
    strip.background.y = element_rect(colour = "white"),
    strip.text = element_text(
      color = "black",
      face = "bold",
      size = 10
    )
  ) +
  labs(title = "Listening Time by Episode Sentiment and Publication time!", 
       y = NULL, 
       x = "Listening Time")


```



### Pair Plot, scatter plot, Histogram and Correlation coefficient | train_df

```{r, fig.width=10, fig.height= 10, fig.align="center"}


train_df |> 
  group_by(listening_time_minutes) |> 
  slice_sample(prop = 0.03) |>  
  ungroup() |>  
  select(all_of(num_Variables)) |> 
  drop_na() |>
ggpairs(
  lower = list(continuous = wrap(
    "smooth",
    alpha = 0.7,
    size = 0.8,
    color = "steelblue"
  )),
  diag = list(continuous = wrap("barDiag", fill = "steelblue", color = "gray80")),
  upper = list(continuous = wrap("cor", size = 3))
) +
  theme(
    axis.text = element_text(size = 8),
    panel.background = element_rect(fill = "white"),
    strip.background = element_rect(fill = "white"),
    strip.background.x = element_rect(colour = "black"),
    strip.background.y = element_rect(colour = "black"),
    strip.text = element_text(color = "black", face = "bold", size = 8)
  ) +
  labs(
    title = "Pair plot | train_df",
    subtitle = "Pair Plot, scatter plot, Histogram and Correlation coefficient",
    caption = "Data source: Kaggle.com, Predict Podcast Listening Time",
    x = NULL,
    y = NULL
  )


```

## Model

### Pre-proccesing 


```{r}

# let's fix the strange values in the test data "episode_length_minutes" and "number_of_ads"


test_df <- test_df |> 
  mutate(episode_length_minutes = if_else(episode_length_minutes > 121, 76, episode_length_minutes),
         number_of_ads = if_else(number_of_ads > 3 , 3, number_of_ads),
         )


preproc_fun <- function(df) {
  df <- df |>
    mutate(ads_per_minute = number_of_ads / (episode_length_minutes + 1),
           number_of_ads = as.integer(number_of_ads),
           epi_guest = guest_popularity_percentage /(episode_length_minutes + 1),
           epi_host = host_popularity_percentage /(episode_length_minutes + 1),
           ads_per_guest = number_of_ads / (guest_popularity_percentage + 1),
           ads_per_host = number_of_ads / (host_popularity_percentage + 1),
           is_weekend = if_else(publication_day %in% c("Saturday", "Sunday"), 1, 0),
           across(.cols = where(is.character), as.factor)
           
           )
  
  return(df)
  
}

train_df_wset <- preproc_fun(train_df) |>
  mutate(episode_length_minutes = if_else(episode_length_minutes >= 300, 30, episode_length_minutes)) |> 
  group_by(listening_time_minutes) |> 
  slice_sample(prop = 1) |>  
  ungroup()

test_df_wset <- preproc_fun(test_df)



num_Variables_wset <- train_df_wset |>
  select(where(is.numeric)) |>
  colnames()


glimpse(train_df_wset)

sapply(test_df_wset, function(x) sum(is.na(x)))


```

### Basic Summary statistics after pre-processing 

```{r}

train_df_wset |>
  skim() |>
  kbl(format = "html",
      caption = "Variables Dignosis | Train after pre-processing ",
      digits = 2) |>
  kable_classic(full_width = F)


test_df_wset |>
    skim() |>
  kbl(format = "html",
      caption = "variables dignosis | Test after pre-processing ",
      digits = 2) |>
  kable_classic(full_width = F)

```

So then, we can split the dataset, although it may need advanced cleaning procedure!

### Data split

```{r Data-Split}

### Data Split
SEED = 8
set.seed(SEED)

train_data_split <-
  initial_split(train_df_wset, prop = 0.9, strata = "listening_time_minutes")

train_data <- training(train_data_split)
test_data  <- testing(train_data_split)



### Generate CV samples 5 folds

set.seed(SEED)
cv_fold <- vfold_cv (train_data, v = 5, strata = "listening_time_minutes") #3

```


### Lightgbm model | Recipy | Model specification | Workflow | Parameters


```{r lightgbm }

lgbm_rec <-
  recipe(listening_time_minutes ~ ., data = train_data)|>
  step_impute_linear(all_numeric_predictors()) |> 
  step_zv(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors())


lgbm_spec <-
  boost_tree(
    trees = 1500,
    tree_depth = tune(),
    learn_rate = tune(),
    mtry = tune(),
    min_n = tune(),
    loss_reduction = tune()
  ) |>
  set_engine(engine = "lightgbm",
             num_leaves = tune()
             
) |>
  set_mode(mode = "regression")



#### Create a workflow

lgbm_wf <- workflow() |>
  add_recipe(lgbm_rec) |>
  add_model(lgbm_spec)

lgbm_wf


set.seed(SEED)

lgbm_ctrl <- control_grid(verbose = TRUE,
                        save_pred = TRUE,
                        save_workflow = TRUE)



params <- lgbm_wf |>
  extract_parameter_set_dials() |>
  update(
         mtry = mtry(range = c(3, 15)),
         min_n = min_n(range = c(20, 60)),
         tree_depth = tree_depth(range = c(5, 20)),
         learn_rate = learn_rate(range = c(-2.1,-1.1)),
         num_leaves = num_leaves(range = c(50, 300)),
         loss_reduction = loss_reduction(range = c(-10, 0))
         ) |>
  finalize(train_data)
```


#### Lightgbm tune grid

```{r lightgbm-tune-grid }

lgbm_res <- tune_grid(
  lgbm_wf,
  resamples = cv_fold,
  grid = 15,
  control = lgbm_ctrl,
  metrics = metric_set(rmse),
  param_info = params
)


autoplot(lgbm_res) +
  theme(
    legend.position = "top",
    strip.background = element_rect(fill = "white"),
    strip.background.x = element_rect(colour = "white"),
    strip.background.y = element_rect(colour = "white"),
    strip.text = element_text(
      color = "black",
      face = "bold",
      size = 7
    )
  ) +
  labs(title = "Autoplot-Grid | Tune Grid",
    y = "rmse")


show_best(lgbm_res, metric = "rmse") |> 
  kbl(format = "html", caption = "Lightgbm Best parameters | Tune Grid") |>
  kable_classic(full_width = F)


```

#### Lightgbm fine-tune

```{r lightgbm-fine-tune}

set.seed(SEED)

lgbm_sim_anneal <- tune_sim_anneal(
  lgbm_wf,
  resamples = cv_fold,
  iter = 5,
  initial = lgbm_res,
  control = control_sim_anneal(
    verbose = TRUE,
    verbose_iter = TRUE,
    save_pred = TRUE,
    save_workflow = TRUE
  ),
  metrics = metric_set(rmse),
  param_info = params
)

autoplot(lgbm_sim_anneal) +
  theme(
    legend.position = "top",
    strip.background = element_rect(fill = "white"),
    strip.background.x = element_rect(colour = "white"),
    strip.background.y = element_rect(colour = "white"),
    strip.text = element_text(
      color = "black",
      face = "bold",
      size = 7
    )
  ) +
  labs(title = "Autoplot-Grid | Fine Tune",
    y = "rmse")



show_best(lgbm_sim_anneal, metric = "rmse") |> 
  kbl(format = "html", caption = "Lightgbm Best parameters | Fine Tune") |>
  kable_classic(full_width = F)


```

#### Evaluate the Trained Model

##### Final fit on training set then evaluated on the test set.

```{r, fig.height = 5, fig.width = 7}

set.seed(SEED)

last_fit_train_test <-
  lgbm_wf |>
  finalize_workflow(select_best(lgbm_sim_anneal, metric = "rmse")) |>
  last_fit(metrics = metric_set(rmse), train_data_split)


eval_tbl <- tibble(last_fit_train_test$.predictions[[1]][3],
                  last_fit_train_test$.predictions[[1]][1])

eval_tbl |> 
  ggplot(aes(x = listening_time_minutes, y =  .pred))+
  geom_point(size = 0.3, color = "tomato") +
  geom_smooth(method = "lm", color = "steelblue", se = FALSE) +
  labs(title = "Predictions",
       x = "Actual",
       y = "Predicted")

```



### Variable Importance

```{r, fig.height=5, fig.width=8}

last_fit_train_test |> 
  extract_fit_engine() |> 
  vip(num_features = ncol(train_data),
    geom = "col",
    aesthetics = list(
      width = 0.7)) +
  theme(
        plot.background = element_rect(fill = "white"),
        plot.title.position = "panel",
        plot.title = element_text(size = 18, hjust = 0.5),
        plot.caption.position = "plot",
        plot.caption = element_text(size = 8, color = "grey"),
        panel.grid = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14)
      ) +
  labs(
    title = "Variable Importance",
    x = NULL,
    y = "Importance"
  )


```


### Target Variable VS Predictions | distribution | Table

```{r}

skim(eval_tbl)

```

### Histogram | Target Variable VS Predictions | distribution

```{r, fig.height = 4, fig.width = 8}

(eval_tbl |> 
  ggplot() +
  geom_histogram(aes(x = listening_time_minutes), bins = 40,
                 fill = "white",
                 color = "gray60") +
  geom_vline(
    aes(xintercept = mean(listening_time_minutes)),
    linewidth = 1,
    linetype = "dotdash",
    color = "darkred"
  ) +
  scale_y_continuous(labels = comma)+
  scale_x_continuous(labels = comma)+
  theme_classic() +
  labs(
    title = "Target Variable distribution",
    x = "Listening time | minutes",
    y = NULL
  ))


(eval_tbl |> 
  ggplot() +
  geom_histogram(aes(x = .pred), bins = 40,
                 fill = "white",
                 color = "gray60") +
  geom_vline(
    aes(xintercept = mean(.pred)),
    linewidth = 1,
    linetype = "dotdash",
    color = "darkred"
  ) +
  scale_y_continuous(labels = comma)+
  scale_x_continuous(labels = comma)+
  theme_classic() +
  labs(
    title = "Predictions distribution",
    caption = "Data source: Kaggle.com | Predict Podcast Listening Time",
    x = "Predictions",
    y = NULL
  ))


```



### Ensamble Stacking

```{r ensamble-stacking}

ensamble_stacking <- stacks() |>  
  stacks::add_candidates(lgbm_sim_anneal) |> 
  stacks::blend_predictions(
    metric = metric_set(rmse),
    control = tune::control_grid(allow_par = TRUE)
  )

```



#### Fit Members

```{r fit-members}

ensemble <- fit_members(ensamble_stacking)

autoplot(ensemble) +
  theme(
    legend.position = "top",
    strip.background = element_rect(fill = "white"),
    strip.background.x = element_rect(colour = "white"),
    strip.background.y = element_rect(colour = "white"),
    strip.text = element_text(
      color = "black",
      face = "bold",
      size = 7
    )
  ) +
  labs(title = "Autoplot - Ensemble",
    y = "Mean")

```


### Predictions

```{r Predictions}


submit <- stacks::augment(ensemble, test_df_wset) 

```

## Submission

```{r Submission}

sample_submission |>
  mutate(
    Listening_Time_minutes  = submit$.pred
  ) |>
  write_csv("submission.csv")

```


## NEXT

I will consider potential improvements, such as:     

• More Data understanding.     
• More data transformation strategy.     
• Models Enhancement.     
• Performing feature selection or engineering to enhance the model’s performance.    
• More...    


Stay Tuned! Your support is highly appreciated!    