{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea5d4c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:21:13.208426Z",
     "iopub.status.busy": "2025-04-27T09:21:13.207874Z",
     "iopub.status.idle": "2025-04-27T09:21:14.036680Z",
     "shell.execute_reply": "2025-04-27T09:21:14.035911Z"
    },
    "papermill": {
     "duration": 0.83663,
     "end_time": "2025-04-27T09:21:14.038305",
     "exception": false,
     "start_time": "2025-04-27T09:21:13.201675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/input/yolo-model'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip download -d ./packages ultralytics\n",
    "# !tar cfvz archive.tar.gz ./packages\n",
    "import kagglehub\n",
    "kagglehub.dataset_download('rachiteagles/yolo-pkg')\n",
    "kagglehub.dataset_download('rachiteagles/yolo-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573c2ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:21:14.048855Z",
     "iopub.status.busy": "2025-04-27T09:21:14.048628Z",
     "iopub.status.idle": "2025-04-27T09:21:16.822860Z",
     "shell.execute_reply": "2025-04-27T09:21:16.821908Z"
    },
    "papermill": {
     "duration": 2.780529,
     "end_time": "2025-04-27T09:21:16.824309",
     "exception": false,
     "start_time": "2025-04-27T09:21:14.043780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/yolo-pkg/yolo/ultralytics-8.3.112-py3-none-any.whl\r\n",
      "Installing collected packages: ultralytics\r\n",
      "Successfully installed ultralytics-8.3.112\r\n"
     ]
    }
   ],
   "source": [
    "# !tar xfvz archive.tar.gz\n",
    "!pip install --no-index --no-deps /kaggle/input/yolo-pkg/yolo/ultralytics-8.3.112-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3358fa32",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-27T09:21:16.834226Z",
     "iopub.status.busy": "2025-04-27T09:21:16.833989Z",
     "iopub.status.idle": "2025-04-27T09:21:29.179911Z",
     "shell.execute_reply": "2025-04-27T09:21:29.179314Z"
    },
    "papermill": {
     "duration": 12.352172,
     "end_time": "2025-04-27T09:21:29.181046",
     "exception": false,
     "start_time": "2025-04-27T09:21:16.828874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79d8c839ced0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, cv2, torch, yaml, threading, time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from ultralytics import YOLO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0b8334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:21:29.191485Z",
     "iopub.status.busy": "2025-04-27T09:21:29.190737Z",
     "iopub.status.idle": "2025-04-27T09:21:29.195480Z",
     "shell.execute_reply": "2025-04-27T09:21:29.194952Z"
    },
    "papermill": {
     "duration": 0.010722,
     "end_time": "2025-04-27T09:21:29.196497",
     "exception": false,
     "start_time": "2025-04-27T09:21:29.185775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define YOLO dataset structure\n",
    "yolo_dataset_dir = \"/kaggle/working/yolo_dataset\"\n",
    "yolo_images_train = os.path.join(yolo_dataset_dir, \"images\", \"train\")\n",
    "yolo_images_val = os.path.join(yolo_dataset_dir, \"images\", \"val\")\n",
    "yolo_labels_train = os.path.join(yolo_dataset_dir, \"labels\", \"train\")\n",
    "yolo_labels_val = os.path.join(yolo_dataset_dir, \"labels\", \"val\")\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [yolo_images_train, yolo_images_val, yolo_labels_train, yolo_labels_val]:\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06294319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:21:29.205894Z",
     "iopub.status.busy": "2025-04-27T09:21:29.205688Z",
     "iopub.status.idle": "2025-04-27T09:21:29.268987Z",
     "shell.execute_reply": "2025-04-27T09:21:29.268213Z"
    },
    "papermill": {
     "duration": 0.069346,
     "end_time": "2025-04-27T09:21:29.270155",
     "exception": false,
     "start_time": "2025-04-27T09:21:29.200809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define global constants for dataset directories\n",
    "DATA_DIR = '/kaggle/input/byu-locating-bacterial-flagellar-motors-2025'\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train_labels.csv')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "MODEL_PATH = \"/kaggle/working/yolo_weights/motor_detector/weights/best.pt\"\n",
    "OUTPUT_DIR = './'\n",
    "MODEL_DIR = './models'\n",
    "TRUST = 4\n",
    "TRAIN_SPLIT = 0.8\n",
    "BOX_SIZE = 24\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 8  \n",
    "SUBMISSION_PATH = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "# Detection parameters\n",
    "CONFIDENCE_THRESHOLD = 0.45  # Lower threshold to catch more potential motors\n",
    "MAX_DETECTIONS_PER_TOMO = 3  # Keep track of top N detections per tomogram\n",
    "NMS_IOU_THRESHOLD = 0.2  # Non-maximum suppression threshold for 3D clustering\n",
    "CONCENTRATION = 1 # ONLY PROCESS 1/20 slices for fast submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf6813a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:21:29.280421Z",
     "iopub.status.busy": "2025-04-27T09:21:29.279928Z",
     "iopub.status.idle": "2025-04-27T09:21:29.285022Z",
     "shell.execute_reply": "2025-04-27T09:21:29.284319Z"
    },
    "papermill": {
     "duration": 0.011261,
     "end_time": "2025-04-27T09:21:29.286146",
     "exception": false,
     "start_time": "2025-04-27T09:21:29.274885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rotate_image_and_coords(image, angle, x, y, img_width, img_height):\n",
    "    \"\"\"Rotate the image and calculate the new coordinates of the motor based on the angle\"\"\"\n",
    "    if angle == 0:\n",
    "        rotated_image = image.copy()\n",
    "        new_x, new_y = x, y\n",
    "    elif angle == 90:\n",
    "        rotated_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "        new_x, new_y = img_width - y, x  # Rotation formula for 90 degrees\n",
    "    elif angle == 180:\n",
    "        rotated_image = cv2.rotate(image, cv2.ROTATE_180)\n",
    "        new_x, new_y = img_width - x, img_height - y  # Rotation formula for 180 degrees\n",
    "    elif angle == 270:\n",
    "        rotated_image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        new_x, new_y = y, img_width - x  # Rotation formula for 270 degrees\n",
    "\n",
    "    return rotated_image, new_x, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef82638f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:21:29.295895Z",
     "iopub.status.busy": "2025-04-27T09:21:29.295689Z",
     "iopub.status.idle": "2025-04-27T09:21:29.310752Z",
     "shell.execute_reply": "2025-04-27T09:21:29.310061Z"
    },
    "papermill": {
     "duration": 0.021331,
     "end_time": "2025-04-27T09:21:29.311879",
     "exception": false,
     "start_time": "2025-04-27T09:21:29.290548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image processing functions\n",
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    Normalize slice data using 2nd and 98th percentiles\n",
    "    \"\"\"\n",
    "    # Calculate percentiles\n",
    "    p2 = np.percentile(slice_data, 2)\n",
    "    p98 = np.percentile(slice_data, 98)\n",
    "    \n",
    "    # Clip the data to the percentile range\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    \n",
    "    # Normalize to [0, 255] range\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    \n",
    "    return np.uint8(normalized)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_yolo_dataset(trust=TRUST, train_split=TRAIN_SPLIT):\n",
    "    \"\"\"\n",
    "    Extract slices containing motors from tomograms and save to YOLO structure with annotations\n",
    "    \"\"\"\n",
    "    # Load the labels CSV\n",
    "    labels_df = pd.read_csv(TRAIN_CSV)\n",
    "    \n",
    "    # Count total number of motors\n",
    "    total_motors = labels_df['Number of motors'].sum()\n",
    "    print(f\"Total number of motors in the dataset: {total_motors}\")\n",
    "    \n",
    "    # Get unique tomograms that have motors\n",
    "    tomo_df = labels_df[labels_df['Number of motors'] > 0].copy()\n",
    "    unique_tomos = tomo_df['tomo_id'].unique()\n",
    "    \n",
    "    print(f\"Found {len(unique_tomos)} unique tomograms with motors\")\n",
    "    \n",
    "    # Perform the train-val split at the tomogram level (not motor level)\n",
    "    # This ensures all slices from a single tomogram go to either train or val\n",
    "    np.random.shuffle(unique_tomos)  # Shuffle the tomograms\n",
    "    split_idx = int(len(unique_tomos) * train_split)\n",
    "    train_tomos = unique_tomos[:split_idx]\n",
    "    val_tomos = unique_tomos[split_idx:]\n",
    "    \n",
    "    print(f\"Split: {len(train_tomos)} tomograms for training, {len(val_tomos)} tomograms for validation\")\n",
    "    \n",
    "    # Function to process a set of tomograms\n",
    "    def process_tomogram_set(tomogram_ids, images_dir, labels_dir, set_name):\n",
    "        motor_counts = []\n",
    "        for tomo_id in tomogram_ids:\n",
    "            # Get all motors for this tomogram\n",
    "            tomo_motors = labels_df[labels_df['tomo_id'] == tomo_id]\n",
    "            for _, motor in tomo_motors.iterrows():\n",
    "                if pd.isna(motor['Motor axis 0']):\n",
    "                    continue\n",
    "                motor_counts.append(\n",
    "                    (tomo_id, \n",
    "                     int(motor['Motor axis 0']), \n",
    "                     int(motor['Motor axis 1']), \n",
    "                     int(motor['Motor axis 2']),\n",
    "                     int(motor['Array shape (axis 0)']))\n",
    "                )\n",
    "        \n",
    "        print(f\"Will process approximately {len(motor_counts) * (2 * trust + 1)} slices for {set_name}\")\n",
    "        \n",
    "        # Process each motor\n",
    "        processed_slices = 0\n",
    "        \n",
    "        for tomo_id, z_center, y_center, x_center, z_max in tqdm(motor_counts, desc=f\"Processing {set_name} motors\"):\n",
    "            # Calculate range of slices to include\n",
    "            z_min = max(0, z_center - trust)\n",
    "            z_max = min(z_max - 1, z_center + trust)\n",
    "            \n",
    "            # Process each slice in the range\n",
    "            for z in range(z_min, z_max + 1):\n",
    "                # Create slice filename\n",
    "                slice_filename = f\"slice_{z:04d}.jpg\"\n",
    "                \n",
    "                # Source path for the slice\n",
    "                src_path = os.path.join(TRAIN_DIR, tomo_id, slice_filename)\n",
    "                \n",
    "                if not os.path.exists(src_path):\n",
    "                    print(f\"Warning: {src_path} does not exist, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load and normalize the slice\n",
    "                img = cv2.imread(src_path)\n",
    "\n",
    "                # Get image dimensions\n",
    "                img_height,img_width,_ = img.shape\n",
    "\n",
    "                for angle in [0, 90, 180, 270]:\n",
    "                    rotated_img, new_x, new_y = rotate_image_and_coords(\n",
    "                            img, angle, x_center, y_center, img_width, img_height\n",
    "                        )\n",
    "                    img_array = np.array(rotated_img)\n",
    "                    \n",
    "                    # Normalize the image\n",
    "                    normalized_img = normalize_slice(img_array)\n",
    "                    \n",
    "                    # Create destination filename (with unique identifier)\n",
    "                    dest_filename = f\"{tomo_id}_z{z:04d}_y{new_x:04d}_x{new_x:04d}_rot{angle:04d}.jpg\"\n",
    "                    dest_path = os.path.join(images_dir, dest_filename)\n",
    "                    \n",
    "                    # Save the normalized image\n",
    "                    Image.fromarray(normalized_img).save(dest_path)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Create YOLO format label\n",
    "                    # YOLO format: <class> <x_center> <y_center> <width> <height>\n",
    "                    # Values are normalized to [0, 1]\n",
    "                    x_center_norm = new_x / img_width\n",
    "                    y_center_norm = new_y / img_height\n",
    "                    box_width_norm = BOX_SIZE / img_width\n",
    "                    box_height_norm = BOX_SIZE / img_height\n",
    "                    \n",
    "                    # Write label file\n",
    "                    label_path = os.path.join(labels_dir, dest_filename.replace('.jpg', '.txt'))\n",
    "                    with open(label_path, 'w') as f:\n",
    "                        f.write(f\"0 {x_center_norm} {y_center_norm} {box_width_norm} {box_height_norm}\\n\")\n",
    "                    \n",
    "                    processed_slices += 1\n",
    "        \n",
    "        return processed_slices, len(motor_counts)\n",
    "    \n",
    "    # Process training tomograms\n",
    "    train_slices, train_motors = process_tomogram_set(train_tomos, yolo_images_train, yolo_labels_train, \"training\")\n",
    "    \n",
    "    # Process validation tomograms\n",
    "    val_slices, val_motors = process_tomogram_set(val_tomos, yolo_images_val, yolo_labels_val, \"validation\")\n",
    "    \n",
    "    # Create YAML configuration file for YOLO\n",
    "    yaml_content = {\n",
    "        'path': yolo_dataset_dir,\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': {0: 'motor'}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(yolo_dataset_dir, 'dataset.yaml'), 'w') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"- Train set: {len(train_tomos)} tomograms, {train_motors} motors, {train_slices} slices\")\n",
    "    print(f\"- Validation set: {len(val_tomos)} tomograms, {val_motors} motors, {val_slices} slices\")\n",
    "    print(f\"- Total: {len(train_tomos) + len(val_tomos)} tomograms, {train_motors + val_motors} motors, {train_slices + val_slices} slices\")\n",
    "    \n",
    "    # Return summary info\n",
    "    return {\n",
    "        \"dataset_dir\": yolo_dataset_dir,\n",
    "        \"yaml_path\": os.path.join(yolo_dataset_dir, 'dataset.yaml'),\n",
    "        \"train_tomograms\": len(train_tomos),\n",
    "        \"val_tomograms\": len(val_tomos),\n",
    "        \"train_motors\": train_motors,\n",
    "        \"val_motors\": val_motors,\n",
    "        \"train_slices\": train_slices,\n",
    "        \"val_slices\": val_slices\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35fb0c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:21:29.321349Z",
     "iopub.status.busy": "2025-04-27T09:21:29.321115Z",
     "iopub.status.idle": "2025-04-27T09:40:36.228836Z",
     "shell.execute_reply": "2025-04-27T09:40:36.228112Z"
    },
    "papermill": {
     "duration": 1146.934897,
     "end_time": "2025-04-27T09:40:36.251147",
     "exception": false,
     "start_time": "2025-04-27T09:21:29.316250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motors in the dataset: 831\n",
      "Found 362 unique tomograms with motors\n",
      "Split: 289 tomograms for training, 73 tomograms for validation\n",
      "Will process approximately 3267 slices for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training motors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 363/363 [15:22<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process approximately 792 slices for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation motors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [03:44<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "- Train set: 289 tomograms, 363 motors, 13048 slices\n",
      "- Validation set: 73 tomograms, 88 motors, 3168 slices\n",
      "- Total: 362 tomograms, 451 motors, 16216 slices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summary = prepare_yolo_dataset(TRUST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33fc6e67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:40:36.294509Z",
     "iopub.status.busy": "2025-04-27T09:40:36.294220Z",
     "iopub.status.idle": "2025-04-27T09:40:36.298838Z",
     "shell.execute_reply": "2025-04-27T09:40:36.298309Z"
    },
    "papermill": {
     "duration": 0.02728,
     "end_time": "2025-04-27T09:40:36.299927",
     "exception": false,
     "start_time": "2025-04-27T09:40:36.272647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_dir': '/kaggle/working/yolo_dataset',\n",
       " 'yaml_path': '/kaggle/working/yolo_dataset/dataset.yaml',\n",
       " 'train_tomograms': 289,\n",
       " 'val_tomograms': 73,\n",
       " 'train_motors': 363,\n",
       " 'val_motors': 88,\n",
       " 'train_slices': 13048,\n",
       " 'val_slices': 3168}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95202325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:40:36.343823Z",
     "iopub.status.busy": "2025-04-27T09:40:36.343629Z",
     "iopub.status.idle": "2025-04-27T09:40:36.350877Z",
     "shell.execute_reply": "2025-04-27T09:40:36.350318Z"
    },
    "papermill": {
     "duration": 0.031054,
     "end_time": "2025-04-27T09:40:36.351929",
     "exception": false,
     "start_time": "2025-04-27T09:40:36.320875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_dfl_loss_curve(run_dir):\n",
    "    \"\"\"\n",
    "    Plot the DFL loss curves for train and validation, marking the best model\n",
    "    \n",
    "    Args:\n",
    "        run_dir (str): Directory where the training results are stored\n",
    "    \"\"\"\n",
    "    # Path to the results CSV file\n",
    "    results_csv = os.path.join(run_dir, 'results.csv')\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        print(f\"Results file not found at {results_csv}\")\n",
    "        return\n",
    "    \n",
    "    # Read results CSV\n",
    "    results_df = pd.read_csv(results_csv)\n",
    "    \n",
    "    # Check if DFL loss columns exist\n",
    "    train_dfl_col = [col for col in results_df.columns if 'train/dfl_loss' in col]\n",
    "    val_dfl_col = [col for col in results_df.columns if 'val/dfl_loss' in col]\n",
    "    \n",
    "    if not train_dfl_col or not val_dfl_col:\n",
    "        print(\"DFL loss columns not found in results CSV\")\n",
    "        print(f\"Available columns: {results_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    train_dfl_col = train_dfl_col[0]\n",
    "    val_dfl_col = val_dfl_col[0]\n",
    "    \n",
    "    # Find the epoch with the best validation loss\n",
    "    best_epoch = results_df[val_dfl_col].idxmin()\n",
    "    best_val_loss = results_df.loc[best_epoch, val_dfl_col]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.plot(results_df['epoch'], results_df[train_dfl_col], label='Train DFL Loss')\n",
    "    plt.plot(results_df['epoch'], results_df[val_dfl_col], label='Validation DFL Loss')\n",
    "    \n",
    "    # Mark the best model with a vertical line\n",
    "    plt.axvline(x=results_df.loc[best_epoch, 'epoch'], color='r', linestyle='--', \n",
    "                label=f'Best Model (Epoch {int(results_df.loc[best_epoch, \"epoch\"])}, Val Loss: {best_val_loss:.4f})')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('DFL Loss')\n",
    "    plt.title('Training and Validation DFL Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the plot in the same directory as weights\n",
    "    plot_path = os.path.join(run_dir, 'dfl_loss_curve.png')\n",
    "    plt.savefig(plot_path)\n",
    "    \n",
    "    # Also save it to the working directory for easier access\n",
    "    plt.savefig(os.path.join('/kaggle/working', 'dfl_loss_curve.png'))\n",
    "    \n",
    "    print(f\"Loss curve saved to {plot_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Return the best epoch info\n",
    "    return best_epoch, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f79aabf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:40:36.397110Z",
     "iopub.status.busy": "2025-04-27T09:40:36.396584Z",
     "iopub.status.idle": "2025-04-27T09:40:36.401606Z",
     "shell.execute_reply": "2025-04-27T09:40:36.401063Z"
    },
    "papermill": {
     "duration": 0.027915,
     "end_time": "2025-04-27T09:40:36.402678",
     "exception": false,
     "start_time": "2025-04-27T09:40:36.374763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_yolo_model(yaml_path, pretrained_weights_path, epochs=30, batch_size=16, img_size=640):\n",
    "    \"\"\"\n",
    "    Train a YOLO model on the prepared dataset\n",
    "    \n",
    "    Args:\n",
    "        yaml_path (str): Path to the dataset YAML file\n",
    "        pretrained_weights_path (str): Path to pre-downloaded weights file\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        img_size (int): Image size for training\n",
    "    \"\"\"\n",
    "    print(f\"Loading pre-trained weights from: {pretrained_weights_path}\")\n",
    "    \n",
    "    # Load a pre-trained YOLOv8 model\n",
    "    model = YOLO(pretrained_weights_path)\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        project=yolo_weights_dir,\n",
    "        name='motor_detector',\n",
    "        exist_ok=True,\n",
    "        patience=5,              # Early stopping if no improvement for 5 epochs\n",
    "        save_period=5,           # Save checkpoints every 5 epochs\n",
    "        val=True,                # Ensure validation is performed\n",
    "        verbose=True             # Show detailed output during training\n",
    "    )\n",
    "    \n",
    "    # Get the path to the run directory\n",
    "    run_dir = os.path.join(yolo_weights_dir, 'motor_detector')\n",
    "    \n",
    "    # Plot and save the loss curve\n",
    "    best_epoch_info = plot_dfl_loss_curve(run_dir)\n",
    "    \n",
    "    if best_epoch_info:\n",
    "        best_epoch, best_val_loss = best_epoch_info\n",
    "        print(f\"\\nBest model found at epoch {best_epoch} with validation DFL loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf22e67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:40:36.447020Z",
     "iopub.status.busy": "2025-04-27T09:40:36.446372Z",
     "iopub.status.idle": "2025-04-27T09:40:36.450260Z",
     "shell.execute_reply": "2025-04-27T09:40:36.449727Z"
    },
    "papermill": {
     "duration": 0.027226,
     "end_time": "2025-04-27T09:40:36.451333",
     "exception": false,
     "start_time": "2025-04-27T09:40:36.424107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "yaml_path = '/kaggle/working/yolo_dataset/dataset.yaml'\n",
    "yolo_pretrained_weights = '/kaggle/input/yolo-model/yolov8n.pt'\n",
    "yolo_weights_dir = \"/kaggle/working/yolo_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "916307ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:40:36.499515Z",
     "iopub.status.busy": "2025-04-27T09:40:36.498958Z",
     "iopub.status.idle": "2025-04-27T10:24:04.400700Z",
     "shell.execute_reply": "2025-04-27T10:24:04.399729Z"
    },
    "papermill": {
     "duration": 2607.926705,
     "end_time": "2025-04-27T10:24:04.402027",
     "exception": false,
     "start_time": "2025-04-27T09:40:36.475322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights from: /kaggle/input/yolo-model/yolov8n.pt\n",
      "Ultralytics 8.3.112 ðŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/input/yolo-model/yolov8n.pt, data=/kaggle/working/yolo_dataset/dataset.yaml, epochs=30, time=None, patience=5, batch=16, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=/kaggle/working/yolo_weights, name=motor_detector, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=/kaggle/working/yolo_weights/motor_detector\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
      "WARNING âš ï¸ \u001b[34m\u001b[1mAMP: \u001b[0mchecks skipped. Offline and unable to download YOLO11n for AMP checks. Setting 'amp=True'. If you experience zero-mAP or NaN losses you can disable AMP with amp=False.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2914.6Â±400.3 MB/s, size: 352.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/yolo_dataset/labels/train... 13048 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13048/13048 [00:08<00:00, 1582.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/yolo_dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1619.5Â±1449.5 MB/s, size: 371.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/yolo_dataset/labels/val... 3168 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3168/3168 [00:02<00:00, 1390.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/yolo_dataset/labels/val.cache\n",
      "Plotting labels to /kaggle/working/yolo_weights/motor_detector/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/kaggle/working/yolo_weights/motor_detector\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      2.27G       3.25      7.402      1.223          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:42<00:00,  5.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:20<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.122      0.122     0.0404    0.00785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      2.76G      2.722       2.63      1.051         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:35<00:00,  5.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168       0.14      0.111     0.0441    0.00935\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      2.76G      2.559      2.326      1.022          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:33<00:00,  5.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:20<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168       0.38      0.283      0.229     0.0784\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      2.76G      2.429      2.129     0.9975          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168       0.56      0.341      0.357      0.123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      2.76G      2.342      1.994     0.9787         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:20<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.462      0.251      0.254     0.0545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      2.76G      2.231      1.849     0.9664         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.637      0.339      0.388      0.118\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      2.76G       2.16      1.766     0.9501         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.657      0.357       0.41      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      2.76G      2.113       1.72     0.9429          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:31<00:00,  5.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168        0.7      0.354      0.417      0.112\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      2.76G      2.024       1.63     0.9296          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:20<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.749      0.419      0.521      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      2.76G       1.99      1.574     0.9218         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.735      0.394      0.489      0.172\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      2.76G      1.935       1.51     0.9165          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.676      0.374      0.441      0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      2.76G      1.896      1.475      0.908          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:20<00:00,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.811      0.413       0.53      0.181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      2.76G      1.854      1.443     0.9037         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:32<00:00,  5.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.735      0.406      0.482      0.147\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      2.76G      1.823      1.393     0.8988          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [02:33<00:00,  5.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:19<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.713      0.443      0.548      0.181\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 5 epochs. Best results observed at epoch 9, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=5) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "14 epochs completed in 0.676 hours.\n",
      "Optimizer stripped from /kaggle/working/yolo_weights/motor_detector/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /kaggle/working/yolo_weights/motor_detector/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /kaggle/working/yolo_weights/motor_detector/weights/best.pt...\n",
      "Ultralytics 8.3.112 ðŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:20<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3168       3168      0.749      0.419      0.521      0.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n",
      "/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.2ms preprocess, 1.8ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/kaggle/working/yolo_weights/motor_detector\u001b[0m\n",
      "Loss curve saved to /kaggle/working/yolo_weights/motor_detector/dfl_loss_curve.png\n",
      "\n",
      "Best model found at epoch 8 with validation DFL loss: 0.9475\n"
     ]
    }
   ],
   "source": [
    "model, results = train_yolo_model(\n",
    "        yaml_path,\n",
    "        pretrained_weights_path=yolo_pretrained_weights,\n",
    "        epochs=30  # Using 30 epochs instead of 100 for faster training\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52981841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T10:24:06.618094Z",
     "iopub.status.busy": "2025-04-27T10:24:06.617668Z",
     "iopub.status.idle": "2025-04-27T10:24:06.626294Z",
     "shell.execute_reply": "2025-04-27T10:24:06.625478Z"
    },
    "papermill": {
     "duration": 1.103291,
     "end_time": "2025-04-27T10:24:06.627545",
     "exception": false,
     "start_time": "2025-04-27T10:24:05.524254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def debug_image_loading(tomo_id):\n",
    "    \"\"\"\n",
    "    Debug function to check image loading\n",
    "    \"\"\"\n",
    "    tomo_dir = os.path.join(TEST_DIR, tomo_id)\n",
    "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    if not slice_files:\n",
    "        print(f\"No image files found in {tomo_dir}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(slice_files)} image files in {tomo_dir}\")\n",
    "    sample_file = slice_files[len(slice_files)//2]  # Middle slice\n",
    "    img_path = os.path.join(tomo_dir, sample_file)\n",
    "    \n",
    "    # Try different loading methods\n",
    "    try:\n",
    "        # Method 1: PIL\n",
    "        img_pil = Image.open(img_path)\n",
    "        img_array_pil = np.array(img_pil)\n",
    "        print(f\"PIL Image shape: {img_array_pil.shape}, dtype: {img_array_pil.dtype}\")\n",
    "        \n",
    "        # Method 2: OpenCV\n",
    "        img_cv2 = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        print(f\"OpenCV Image shape: {img_cv2.shape}, dtype: {img_cv2.dtype}\")\n",
    "        \n",
    "        # Method 3: Convert to RGB\n",
    "        img_rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        print(f\"OpenCV RGB Image shape: {img_rgb.shape}, dtype: {img_rgb.dtype}\")\n",
    "        \n",
    "        print(\"Image loading successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        \n",
    "    # Also test with YOLO's built-in loader\n",
    "    try:\n",
    "        test_model = YOLO(MODEL_PATH)\n",
    "        test_results = test_model([img_path], verbose=False)\n",
    "        print(\"YOLO model successfully processed the test image\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with YOLO processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6271e785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T10:24:08.834724Z",
     "iopub.status.busy": "2025-04-27T10:24:08.834445Z",
     "iopub.status.idle": "2025-04-27T10:24:08.838419Z",
     "shell.execute_reply": "2025-04-27T10:24:08.837855Z"
    },
    "papermill": {
     "duration": 1.112756,
     "end_time": "2025-04-27T10:24:08.839511",
     "exception": false,
     "start_time": "2025-04-27T10:24:07.726755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preload_image_batch(file_paths):\n",
    "    \"\"\"Preload a batch of images to CPU memory\"\"\"\n",
    "    images = []\n",
    "    for path in file_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            # Try with PIL as fallback\n",
    "            img = np.array(Image.open(path))\n",
    "        images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d516130d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T10:24:11.057310Z",
     "iopub.status.busy": "2025-04-27T10:24:11.057017Z",
     "iopub.status.idle": "2025-04-27T10:24:11.061613Z",
     "shell.execute_reply": "2025-04-27T10:24:11.061058Z"
    },
    "papermill": {
     "duration": 1.108121,
     "end_time": "2025-04-27T10:24:11.062777",
     "exception": false,
     "start_time": "2025-04-27T10:24:09.954656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU profiling context manager\n",
    "class GPUProfiler:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.start_time = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac6e034f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T10:24:13.370173Z",
     "iopub.status.busy": "2025-04-27T10:24:13.369572Z",
     "iopub.status.idle": "2025-04-27T10:24:13.375893Z",
     "shell.execute_reply": "2025-04-27T10:24:13.375333Z"
    },
    "papermill": {
     "duration": 1.138725,
     "end_time": "2025-04-27T10:24:13.376984",
     "exception": false,
     "start_time": "2025-04-27T10:24:12.238259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_3d_nms(detections, iou_threshold):\n",
    "    \"\"\"\n",
    "    Perform 3D Non-Maximum Suppression on detections to merge nearby motors\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "    \n",
    "    # Sort by confidence (highest first)\n",
    "    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # List to store final detections after NMS\n",
    "    final_detections = []\n",
    "    \n",
    "    # Define 3D distance function\n",
    "    def distance_3d(d1, d2):\n",
    "        return np.sqrt((d1['z'] - d2['z'])**2 + \n",
    "                       (d1['y'] - d2['y'])**2 + \n",
    "                       (d1['x'] - d2['x'])**2)\n",
    "    \n",
    "    # Maximum distance threshold (based on box size and slice gap)\n",
    "    box_size = 24  # Same as annotation box size\n",
    "    distance_threshold = box_size * iou_threshold\n",
    "    \n",
    "    # Process each detection\n",
    "    while detections:\n",
    "        # Take the detection with highest confidence\n",
    "        best_detection = detections.pop(0)\n",
    "        final_detections.append(best_detection)\n",
    "        \n",
    "        # Filter out detections that are too close to the best detection\n",
    "        detections = [d for d in detections if distance_3d(d, best_detection) > distance_threshold]\n",
    "    \n",
    "    return final_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8022174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T10:24:15.558181Z",
     "iopub.status.busy": "2025-04-27T10:24:15.557903Z",
     "iopub.status.idle": "2025-04-27T10:24:15.570513Z",
     "shell.execute_reply": "2025-04-27T10:24:15.569997Z"
    },
    "papermill": {
     "duration": 1.069957,
     "end_time": "2025-04-27T10:24:15.571632",
     "exception": false,
     "start_time": "2025-04-27T10:24:14.501675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_tomogram(tomo_id, model, index=0, total=1):\n",
    "    \"\"\"\n",
    "    Process a single tomogram and return the most confident motor detection\n",
    "    \"\"\"\n",
    "    print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n",
    "    \n",
    "    # Get all slice files for this tomogram\n",
    "    tomo_dir = os.path.join(TEST_DIR, tomo_id)\n",
    "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    # Apply CONCENTRATION to reduce the number of slices processed\n",
    "    # This will process approximately CONCENTRATION fraction of all slices\n",
    "    selected_indices = np.linspace(0, len(slice_files)-1, int(len(slice_files) * CONCENTRATION))\n",
    "    selected_indices = np.round(selected_indices).astype(int)\n",
    "    slice_files = [slice_files[i] for i in selected_indices]\n",
    "    \n",
    "    print(f\"Processing {len(slice_files)} out of {len(os.listdir(tomo_dir))} slices based on CONCENTRATION={CONCENTRATION}\")\n",
    "    \n",
    "    # Create a list to store all detections\n",
    "    all_detections = []\n",
    "    \n",
    "    # Create CUDA streams for parallel processing if using GPU\n",
    "    if DEVICE.startswith('cuda'):\n",
    "        streams = [torch.cuda.Stream() for _ in range(min(4, BATCH_SIZE))]\n",
    "    else:\n",
    "        streams = [None]\n",
    "    \n",
    "    # Variables for preloading\n",
    "    next_batch_thread = None\n",
    "    next_batch_images = None\n",
    "    \n",
    "    # Process slices in batches\n",
    "    for batch_start in range(0, len(slice_files), BATCH_SIZE):\n",
    "        # Wait for previous preload thread if it exists\n",
    "        if next_batch_thread is not None:\n",
    "            next_batch_thread.join()\n",
    "            next_batch_images = None\n",
    "            \n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(slice_files))\n",
    "        batch_files = slice_files[batch_start:batch_end]\n",
    "        \n",
    "        # Start preloading next batch\n",
    "        next_batch_start = batch_end\n",
    "        next_batch_end = min(next_batch_start + BATCH_SIZE, len(slice_files))\n",
    "        next_batch_files = slice_files[next_batch_start:next_batch_end] if next_batch_start < len(slice_files) else []\n",
    "        \n",
    "        if next_batch_files:\n",
    "            next_batch_paths = [os.path.join(tomo_dir, f) for f in next_batch_files]\n",
    "            next_batch_thread = threading.Thread(target=preload_image_batch, args=(next_batch_paths,))\n",
    "            next_batch_thread.start()\n",
    "        else:\n",
    "            next_batch_thread = None\n",
    "        \n",
    "        # Split batch across streams for parallel processing\n",
    "        sub_batches = np.array_split(batch_files, len(streams))\n",
    "        sub_batch_results = []\n",
    "        \n",
    "        for i, sub_batch in enumerate(sub_batches):\n",
    "            if len(sub_batch) == 0:\n",
    "                continue\n",
    "                \n",
    "            stream = streams[i % len(streams)]\n",
    "            with torch.cuda.stream(stream) if stream and DEVICE.startswith('cuda') else nullcontext():\n",
    "                # Process sub-batch\n",
    "                sub_batch_paths = [os.path.join(tomo_dir, slice_file) for slice_file in sub_batch]\n",
    "                sub_batch_slice_nums = [int(slice_file.split('_')[1].split('.')[0]) for slice_file in sub_batch]\n",
    "                \n",
    "                # Run inference with profiling\n",
    "                with GPUProfiler(f\"Inference batch {i+1}/{len(sub_batches)}\"):\n",
    "                    sub_results = model(sub_batch_paths, verbose=False)\n",
    "                \n",
    "                # Process each result in this sub-batch\n",
    "                for j, result in enumerate(sub_results):\n",
    "                    if len(result.boxes) > 0:\n",
    "                        boxes = result.boxes\n",
    "                        for box_idx, confidence in enumerate(boxes.conf):\n",
    "                            if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                                # Get bounding box coordinates\n",
    "                                x1, y1, x2, y2 = boxes.xyxy[box_idx].cpu().numpy()\n",
    "                                \n",
    "                                # Calculate center coordinates\n",
    "                                x_center = (x1 + x2) / 2\n",
    "                                y_center = (y1 + y2) / 2\n",
    "                                \n",
    "                                # Store detection with 3D coordinates\n",
    "                                all_detections.append({\n",
    "                                    'z': round(sub_batch_slice_nums[j]),\n",
    "                                    'y': round(y_center),\n",
    "                                    'x': round(x_center),\n",
    "                                    'confidence': float(confidence)\n",
    "                                })\n",
    "        \n",
    "        # Synchronize streams\n",
    "        if DEVICE.startswith('cuda'):\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    # Clean up thread if still running\n",
    "    if next_batch_thread is not None:\n",
    "        next_batch_thread.join()\n",
    "    \n",
    "    # 3D Non-Maximum Suppression to merge nearby detections across slices\n",
    "    final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n",
    "    \n",
    "    # Sort detections by confidence (highest first)\n",
    "    final_detections.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # If there are no detections, return NA values\n",
    "    if not final_detections:\n",
    "        return {\n",
    "            'tomo_id': tomo_id,\n",
    "            'Motor axis 0': -1,\n",
    "            'Motor axis 1': -1,\n",
    "            'Motor axis 2': -1\n",
    "        }\n",
    "    \n",
    "    # Take the detection with highest confidence\n",
    "    best_detection = final_detections[0]\n",
    "    \n",
    "    # Return result with integer coordinates\n",
    "    return {\n",
    "        'tomo_id': tomo_id,\n",
    "        'Motor axis 0': round(best_detection['z']),\n",
    "        'Motor axis 1': round(best_detection['y']),\n",
    "        'Motor axis 2': round(best_detection['x'])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "130a89bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T10:24:17.786189Z",
     "iopub.status.busy": "2025-04-27T10:24:17.785580Z",
     "iopub.status.idle": "2025-04-27T10:24:17.795005Z",
     "shell.execute_reply": "2025-04-27T10:24:17.794505Z"
    },
    "papermill": {
     "duration": 1.115799,
     "end_time": "2025-04-27T10:24:17.796050",
     "exception": false,
     "start_time": "2025-04-27T10:24:16.680251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_submission():\n",
    "    \"\"\"\n",
    "    Main function to generate the submission file\n",
    "    \"\"\"\n",
    "    # Get list of test tomograms\n",
    "    test_tomos = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\n",
    "    total_tomos = len(test_tomos)\n",
    "    \n",
    "    print(f\"Found {total_tomos} tomograms in test directory\")\n",
    "    \n",
    "    # Debug image loading for the first tomogram\n",
    "    if test_tomos:\n",
    "        debug_image_loading(test_tomos[0])\n",
    "    \n",
    "    # Clear GPU cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize model once outside the processing loop\n",
    "    print(f\"Loading YOLO model from {MODEL_PATH}\")\n",
    "    model = YOLO(MODEL_PATH)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Additional optimizations for inference\n",
    "    if DEVICE.startswith('cuda'):\n",
    "        # Fuse conv and bn layers for faster inference\n",
    "        model.fuse()\n",
    "        \n",
    "        # Enable model half precision (FP16) if on compatible GPU\n",
    "        if torch.cuda.get_device_capability(0)[0] >= 7:  # Volta or newer\n",
    "            model.model.half()\n",
    "            print(\"Using half precision (FP16) for inference\")\n",
    "    \n",
    "    # Process tomograms with parallelization\n",
    "    results = []\n",
    "    motors_found = 0\n",
    "    \n",
    "    # Using ThreadPoolExecutor with max_workers=1 since each worker uses the GPU already\n",
    "    # and we're parallelizing within each tomogram processing\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        future_to_tomo = {}\n",
    "        \n",
    "        # Submit all tomograms for processing\n",
    "        for i, tomo_id in enumerate(test_tomos, 1):\n",
    "            future = executor.submit(process_tomogram, tomo_id, model, i, total_tomos)\n",
    "            future_to_tomo[future] = tomo_id\n",
    "        \n",
    "        # Process completed futures as they complete\n",
    "        for future in future_to_tomo:\n",
    "            tomo_id = future_to_tomo[future]\n",
    "            try:\n",
    "                # Clear CUDA cache between tomograms\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update motors found count\n",
    "                has_motor = not pd.isna(result['Motor axis 0'])\n",
    "                if has_motor:\n",
    "                    motors_found += 1\n",
    "                    print(f\"Motor found in {tomo_id} at position: \"\n",
    "                          f\"z={result['Motor axis 0']}, y={result['Motor axis 1']}, x={result['Motor axis 2']}\")\n",
    "                else:\n",
    "                    print(f\"No motor detected in {tomo_id}\")\n",
    "                    \n",
    "                print(f\"Current detection rate: {motors_found}/{len(results)} ({motors_found/len(results)*100:.1f}%)\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {tomo_id}: {e}\")\n",
    "                # Create a default entry for failed tomograms\n",
    "                results.append({\n",
    "                    'tomo_id': tomo_id,\n",
    "                    'Motor axis 0': -1,\n",
    "                    'Motor axis 1': -1,\n",
    "                    'Motor axis 2': -1\n",
    "                })\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure proper column order\n",
    "    submission_df = submission_df[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\n",
    "    \n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    \n",
    "    print(f\"\\nSubmission complete!\")\n",
    "    print(f\"Motors detected: {motors_found}/{total_tomos} ({motors_found/total_tomos*100:.1f}%)\")\n",
    "    print(f\"Submission saved to: {SUBMISSION_PATH}\")\n",
    "    \n",
    "    # Display first few rows of submission\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission_df.head())\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc243dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T10:24:20.011721Z",
     "iopub.status.busy": "2025-04-27T10:24:20.011077Z",
     "iopub.status.idle": "2025-04-27T10:25:04.173199Z",
     "shell.execute_reply": "2025-04-27T10:25:04.172588Z"
    },
    "papermill": {
     "duration": 45.277993,
     "end_time": "2025-04-27T10:25:04.174447",
     "exception": false,
     "start_time": "2025-04-27T10:24:18.896454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 tomograms in test directory\n",
      "Found 500 image files in /kaggle/input/byu-locating-bacterial-flagellar-motors-2025/test/tomo_003acc\n",
      "PIL Image shape: (1912, 1847), dtype: uint8\n",
      "OpenCV Image shape: (1912, 1847), dtype: uint8\n",
      "OpenCV RGB Image shape: (1912, 1847, 3), dtype: uint8\n",
      "Image loading successful!\n",
      "YOLO model successfully processed the test image\n",
      "Loading YOLO model from /kaggle/working/yolo_weights/motor_detector/weights/best.pt\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients\n",
      "Processing tomogram tomo_003acc (1/3)\n",
      "Processing 500 out of 500 slices based on CONCENTRATION=1\n",
      "[PROFILE] Inference batch 1/4: 0.260s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.134s\n",
      "[PROFILE] Inference batch 2/4: 0.129s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.117s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.114s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.116s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.118s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.124s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.116s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.116s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.111s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.118s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.116s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.120s\n",
      "[PROFILE] Inference batch 2/4: 0.124s\n",
      "[PROFILE] Inference batch 3/4: 0.116s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.148s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.111s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.115s\n",
      "[PROFILE] Inference batch 2/4: 0.125s\n",
      "[PROFILE] Inference batch 3/4: 0.114s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.119s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.110s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.117s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.115s\n",
      "[PROFILE] Inference batch 2/4: 0.114s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.114s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.117s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.114s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.117s\n",
      "[PROFILE] Inference batch 2/4: 0.122s\n",
      "[PROFILE] Inference batch 3/4: 0.116s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.116s\n",
      "[PROFILE] Inference batch 2/4: 0.113s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.133s\n",
      "[PROFILE] Inference batch 2/4: 0.118s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.133s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.118s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.121s\n",
      "[PROFILE] Inference batch 2/4: 0.135s\n",
      "[PROFILE] Inference batch 3/4: 0.115s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.119s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.126s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.115s\n",
      "[PROFILE] Inference batch 2/4: 0.132s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.117s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.119s\n",
      "[PROFILE] Inference batch 1/4: 0.120s\n",
      "[PROFILE] Inference batch 2/4: 0.136s\n",
      "[PROFILE] Inference batch 3/4: 0.114s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.139s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.136s\n",
      "[PROFILE] Inference batch 2/4: 0.114s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.119s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.127s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.137s\n",
      "[PROFILE] Inference batch 2/4: 0.118s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.114s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.146s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.116s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.117s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.114s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.118s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.111s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.121s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.120s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.117s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.115s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.121s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.119s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.117s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.126s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.119s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.121s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.118s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.118s\n",
      "[PROFILE] Inference batch 2/4: 0.119s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.118s\n",
      "[PROFILE] Inference batch 2/4: 0.118s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.118s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.121s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.121s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.122s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.123s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.118s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.118s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.114s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.127s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.116s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.118s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.115s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.122s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.117s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.129s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.117s\n",
      "[PROFILE] Inference batch 2/4: 0.119s\n",
      "[PROFILE] Inference batch 3/4: 0.136s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.143s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.122s\n",
      "[PROFILE] Inference batch 2/4: 0.114s\n",
      "[PROFILE] Inference batch 3/4: 0.119s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.128s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.129s\n",
      "[PROFILE] Inference batch 3/4: 0.112s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.145s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.113s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.126s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.122s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.064s\n",
      "[PROFILE] Inference batch 2/4: 0.065s\n",
      "[PROFILE] Inference batch 3/4: 0.064s\n",
      "[PROFILE] Inference batch 4/4: 0.064s\n",
      "Processing tomogram tomo_00e047 (2/3)\n",
      "Motor found in tomo_003acc at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Processing 300 out of 300 slices based on CONCENTRATION=1\n",
      "[PROFILE] Inference batch 1/4: 0.062s\n",
      "[PROFILE] Inference batch 2/4: 0.076s\n",
      "[PROFILE] Inference batch 3/4: 0.052s\n",
      "[PROFILE] Inference batch 4/4: 0.058s\n",
      "[PROFILE] Inference batch 1/4: 0.052s\n",
      "[PROFILE] Inference batch 2/4: 0.051s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.043s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.054s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.051s\n",
      "[PROFILE] Inference batch 2/4: 0.045s\n",
      "[PROFILE] Inference batch 3/4: 0.044s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.047s\n",
      "[PROFILE] Inference batch 3/4: 0.051s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.045s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.050s\n",
      "[PROFILE] Inference batch 2/4: 0.047s\n",
      "[PROFILE] Inference batch 3/4: 0.051s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.046s\n",
      "[PROFILE] Inference batch 2/4: 0.046s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.043s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "[PROFILE] Inference batch 1/4: 0.058s\n",
      "[PROFILE] Inference batch 2/4: 0.050s\n",
      "[PROFILE] Inference batch 3/4: 0.044s\n",
      "[PROFILE] Inference batch 4/4: 0.046s\n",
      "[PROFILE] Inference batch 1/4: 0.048s\n",
      "[PROFILE] Inference batch 2/4: 0.056s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "[PROFILE] Inference batch 1/4: 0.042s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.048s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.051s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.050s\n",
      "[PROFILE] Inference batch 2/4: 0.050s\n",
      "[PROFILE] Inference batch 3/4: 0.049s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "[PROFILE] Inference batch 1/4: 0.050s\n",
      "[PROFILE] Inference batch 2/4: 0.051s\n",
      "[PROFILE] Inference batch 3/4: 0.050s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.043s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.042s\n",
      "[PROFILE] Inference batch 2/4: 0.053s\n",
      "[PROFILE] Inference batch 3/4: 0.051s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.049s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.045s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.050s\n",
      "[PROFILE] Inference batch 3/4: 0.047s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.054s\n",
      "[PROFILE] Inference batch 2/4: 0.051s\n",
      "[PROFILE] Inference batch 3/4: 0.058s\n",
      "[PROFILE] Inference batch 4/4: 0.045s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.043s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "[PROFILE] Inference batch 1/4: 0.052s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.042s\n",
      "[PROFILE] Inference batch 2/4: 0.043s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.043s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.049s\n",
      "[PROFILE] Inference batch 3/4: 0.052s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.051s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.044s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.046s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.049s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.051s\n",
      "[PROFILE] Inference batch 2/4: 0.046s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.023s\n",
      "[PROFILE] Inference batch 2/4: 0.022s\n",
      "[PROFILE] Inference batch 3/4: 0.022s\n",
      "[PROFILE] Inference batch 4/4: 0.023s\n",
      "Processing tomogram tomo_01a877 (3/3)\n",
      "Motor found in tomo_00e047 at position: z=165, y=543, x=602\n",
      "Current detection rate: 2/2 (100.0%)\n",
      "Processing 300 out of 300 slices based on CONCENTRATION=1\n",
      "[PROFILE] Inference batch 1/4: 0.054s\n",
      "[PROFILE] Inference batch 2/4: 0.061s\n",
      "[PROFILE] Inference batch 3/4: 0.050s\n",
      "[PROFILE] Inference batch 4/4: 0.048s\n",
      "[PROFILE] Inference batch 1/4: 0.039s\n",
      "[PROFILE] Inference batch 2/4: 0.049s\n",
      "[PROFILE] Inference batch 3/4: 0.045s\n",
      "[PROFILE] Inference batch 4/4: 0.036s\n",
      "[PROFILE] Inference batch 1/4: 0.048s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.043s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.038s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.039s\n",
      "[PROFILE] Inference batch 2/4: 0.048s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.050s\n",
      "[PROFILE] Inference batch 2/4: 0.046s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.045s\n",
      "[PROFILE] Inference batch 2/4: 0.050s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.048s\n",
      "[PROFILE] Inference batch 2/4: 0.048s\n",
      "[PROFILE] Inference batch 3/4: 0.049s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.050s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.038s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.038s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.047s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.048s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.044s\n",
      "[PROFILE] Inference batch 2/4: 0.046s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.050s\n",
      "[PROFILE] Inference batch 2/4: 0.064s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "[PROFILE] Inference batch 1/4: 0.048s\n",
      "[PROFILE] Inference batch 2/4: 0.047s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.047s\n",
      "[PROFILE] Inference batch 1/4: 0.051s\n",
      "[PROFILE] Inference batch 2/4: 0.051s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.047s\n",
      "[PROFILE] Inference batch 2/4: 0.048s\n",
      "[PROFILE] Inference batch 3/4: 0.044s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.038s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.048s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "[PROFILE] Inference batch 1/4: 0.052s\n",
      "[PROFILE] Inference batch 2/4: 0.055s\n",
      "[PROFILE] Inference batch 3/4: 0.047s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.040s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.051s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.050s\n",
      "[PROFILE] Inference batch 2/4: 0.051s\n",
      "[PROFILE] Inference batch 3/4: 0.045s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "[PROFILE] Inference batch 1/4: 0.049s\n",
      "[PROFILE] Inference batch 2/4: 0.046s\n",
      "[PROFILE] Inference batch 3/4: 0.049s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.039s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.041s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.046s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "[PROFILE] Inference batch 1/4: 0.039s\n",
      "[PROFILE] Inference batch 2/4: 0.038s\n",
      "[PROFILE] Inference batch 3/4: 0.038s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "[PROFILE] Inference batch 1/4: 0.049s\n",
      "[PROFILE] Inference batch 2/4: 0.046s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "[PROFILE] Inference batch 1/4: 0.038s\n",
      "[PROFILE] Inference batch 2/4: 0.038s\n",
      "[PROFILE] Inference batch 3/4: 0.037s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.038s\n",
      "[PROFILE] Inference batch 2/4: 0.038s\n",
      "[PROFILE] Inference batch 3/4: 0.045s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "[PROFILE] Inference batch 1/4: 0.039s\n",
      "[PROFILE] Inference batch 2/4: 0.037s\n",
      "[PROFILE] Inference batch 3/4: 0.036s\n",
      "[PROFILE] Inference batch 4/4: 0.036s\n",
      "[PROFILE] Inference batch 1/4: 0.023s\n",
      "[PROFILE] Inference batch 2/4: 0.021s\n",
      "[PROFILE] Inference batch 3/4: 0.022s\n",
      "[PROFILE] Inference batch 4/4: 0.021s\n",
      "Motor found in tomo_01a877 at position: z=145, y=638, x=285\n",
      "Current detection rate: 3/3 (100.0%)\n",
      "\n",
      "Submission complete!\n",
      "Motors detected: 3/3 (100.0%)\n",
      "Submission saved to: /kaggle/working/submission.csv\n",
      "\n",
      "Submission preview:\n",
      "       tomo_id  Motor axis 0  Motor axis 1  Motor axis 2\n",
      "0  tomo_003acc            -1            -1            -1\n",
      "1  tomo_00e047           165           543           602\n",
      "2  tomo_01a877           145           638           285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tomo_id</th>\n",
       "      <th>Motor axis 0</th>\n",
       "      <th>Motor axis 1</th>\n",
       "      <th>Motor axis 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tomo_003acc</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tomo_00e047</td>\n",
       "      <td>165</td>\n",
       "      <td>543</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tomo_01a877</td>\n",
       "      <td>145</td>\n",
       "      <td>638</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tomo_id  Motor axis 0  Motor axis 1  Motor axis 2\n",
       "0  tomo_003acc            -1            -1            -1\n",
       "1  tomo_00e047           165           543           602\n",
       "2  tomo_01a877           145           638           285"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c5094",
   "metadata": {
    "papermill": {
     "duration": 1.154394,
     "end_time": "2025-04-27T10:25:06.431624",
     "exception": false,
     "start_time": "2025-04-27T10:25:05.277230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1defa7c",
   "metadata": {
    "papermill": {
     "duration": 1.103763,
     "end_time": "2025-04-27T10:25:08.632919",
     "exception": false,
     "start_time": "2025-04-27T10:25:07.529156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b19e1",
   "metadata": {
    "papermill": {
     "duration": 1.10643,
     "end_time": "2025-04-27T10:25:10.843454",
     "exception": false,
     "start_time": "2025-04-27T10:25:09.737024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43115f",
   "metadata": {
    "papermill": {
     "duration": 1.10584,
     "end_time": "2025-04-27T10:25:13.049238",
     "exception": false,
     "start_time": "2025-04-27T10:25:11.943398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9382d9",
   "metadata": {
    "papermill": {
     "duration": 1.216583,
     "end_time": "2025-04-27T10:25:15.370702",
     "exception": false,
     "start_time": "2025-04-27T10:25:14.154119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7b997",
   "metadata": {
    "papermill": {
     "duration": 1.106218,
     "end_time": "2025-04-27T10:25:17.568347",
     "exception": false,
     "start_time": "2025-04-27T10:25:16.462129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca64a0",
   "metadata": {
    "papermill": {
     "duration": 1.104775,
     "end_time": "2025-04-27T10:25:19.770026",
     "exception": false,
     "start_time": "2025-04-27T10:25:18.665251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d9336",
   "metadata": {
    "papermill": {
     "duration": 1.116419,
     "end_time": "2025-04-27T10:25:21.986474",
     "exception": false,
     "start_time": "2025-04-27T10:25:20.870055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646864df",
   "metadata": {
    "papermill": {
     "duration": 1.104785,
     "end_time": "2025-04-27T10:25:24.197292",
     "exception": false,
     "start_time": "2025-04-27T10:25:23.092507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11294684,
     "sourceId": 91249,
     "sourceType": "competition"
    },
    {
     "datasetId": 7206260,
     "sourceId": 11495557,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7206328,
     "sourceId": 11495647,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3859.152679,
   "end_time": "2025-04-27T10:25:28.256597",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-27T09:21:09.103918",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
