{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0b0641",
   "metadata": {
    "papermill": {
     "duration": 0.003193,
     "end_time": "2025-08-26T12:18:38.005986",
     "exception": false,
     "start_time": "2025-08-26T12:18:38.002793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Change Summary**\n",
    "\n",
    "1. Added function: remove_extra_digit in post_filter\n",
    "\n",
    "Purpose: To clean DOI-like values in a dataset by removing rows where the DOI is simply a duplicate with one or two extra digits appended at the end.\n",
    "\n",
    "2. Remove GPL\\d+ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d00b06",
   "metadata": {
    "_cell_guid": "bb51b9e3-a3cf-459b-b461-290687c6196c",
    "_uuid": "68ddb9cd-125c-47c3-898e-b4133ce5de9c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-26T12:18:38.012288Z",
     "iopub.status.busy": "2025-08-26T12:18:38.012060Z",
     "iopub.status.idle": "2025-08-26T12:18:53.856816Z",
     "shell.execute_reply": "2025-08-26T12:18:53.855735Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.850254,
     "end_time": "2025-08-26T12:18:53.859086",
     "exception": false,
     "start_time": "2025-08-26T12:18:38.008832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 2.50s\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.18.0\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m157 packages\u001b[0m \u001b[2min 206ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m52 packages\u001b[0m \u001b[2min 12.20s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m14 packages\u001b[0m \u001b[2min 144ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m52 packages\u001b[0m \u001b[2min 140ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250622\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.9.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.7\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogits-processor-zoo\u001b[0m\u001b[2m==0.1.12\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.47b0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions-ai\u001b[0m\u001b[2m==0.4.9\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==24.0.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.7\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.8.5.post1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.18\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! uv pip uninstall --system 'tensorflow'\n",
    "! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'\n",
    "! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0741f3ff",
   "metadata": {
    "_cell_guid": "d83ceec5-2fa5-4e2c-aab0-2b543305470f",
    "_uuid": "0d592448-86ce-43c2-8a60-d64375fe7946",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-26T12:18:53.887712Z",
     "iopub.status.busy": "2025-08-26T12:18:53.887368Z",
     "iopub.status.idle": "2025-08-26T12:18:53.895293Z",
     "shell.execute_reply": "2025-08-26T12:18:53.894362Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023286,
     "end_time": "2025-08-26T12:18:53.896460",
     "exception": false,
     "start_time": "2025-08-26T12:18:53.873174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/helpers.py\n",
    "import logging, os, kagglehub, inspect\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "IS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\n",
    "IS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "COMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\n",
    "PDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\n",
    "WORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n",
    "\n",
    "DOI_LINK = 'https://doi.org/'\n",
    "\n",
    "DEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\n",
    "LOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\n",
    "LOG_DIR = Path(LOG_FILE_PATH).parent\n",
    "\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\n",
    "LOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def get_logger(name=None):\n",
    "    if name is None:\n",
    "        frame = inspect.currentframe()\n",
    "        if frame is None or frame.f_back is None:\n",
    "            name = \"__main__\"\n",
    "        else:\n",
    "            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        ch.setFormatter(formatter)\n",
    "        fh = logging.FileHandler(LOG_FILE_PATH)\n",
    "        fh.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "        logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def is_doi_link(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.starts_with(DOI_LINK)\n",
    "\n",
    "def string_normalization(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n",
    "\n",
    "def get_df(parse_dir: str):\n",
    "    records = []\n",
    "    txt_files = list(Path(parse_dir).glob('*.txt'))\n",
    "    for txt_file in txt_files:\n",
    "        id_ = txt_file.stem\n",
    "        with open(txt_file, 'r') as f:\n",
    "            text = f.read()\n",
    "        records.append({'article_id': id_, 'text': text})\n",
    "    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n",
    "\n",
    "def assume_type(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n",
    "    )\n",
    "\n",
    "def score(df, gt, on, tag='all'):\n",
    "    hits = gt.join(df, on=on)\n",
    "    tp = hits.height\n",
    "    fp = df.height - tp\n",
    "    fn = gt.height - tp\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n",
    "\n",
    "def evaluate(df, on=['article_id', 'dataset_id']):\n",
    "    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n",
    "    return (\n",
    "        score(df, gt, on),\n",
    "        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n",
    "        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e8c649",
   "metadata": {
    "_cell_guid": "c7060a6d-ef30-4916-b3fc-08b5b4a3d6d9",
    "_uuid": "eb6f0483-05f0-4031-b027-c3cb4129306d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-26T12:18:53.908608Z",
     "iopub.status.busy": "2025-08-26T12:18:53.908406Z",
     "iopub.status.idle": "2025-08-26T12:18:53.912858Z",
     "shell.execute_reply": "2025-08-26T12:18:53.912265Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011666,
     "end_time": "2025-08-26T12:18:53.913860",
     "exception": false,
     "start_time": "2025-08-26T12:18:53.902194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/parse.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from helpers import get_logger, PDF_DIR\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def pdf_to_txt(output_dir: Path):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_files = list(PDF_DIR.glob(\"*.pdf\")) + list(PDF_DIR.glob(\"*.PDF\"))\n",
    "    existing_txt_files = {f.stem for f in output_dir.glob(\"*.txt\")}\n",
    "    for pdf_file in pdf_files:\n",
    "        txt_file = output_dir / f\"{pdf_file.stem}.txt\"\n",
    "        if pdf_file.stem in existing_txt_files:\n",
    "            continue\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pymupdf.open(pdf_file) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            txt_file.write_text(text, encoding='utf-8')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('output_dir', type=Path, help='Directory to save text files')\n",
    "    args = parser.parse_args()\n",
    "    pdf_to_txt(args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e6f93e",
   "metadata": {
    "_cell_guid": "573a2751-a0a6-4d64-8786-da86453a2fda",
    "_uuid": "165f2fe5-dd88-475a-b589-cf17d1ca97d7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-26T12:18:53.925717Z",
     "iopub.status.busy": "2025-08-26T12:18:53.925514Z",
     "iopub.status.idle": "2025-08-26T12:18:53.930081Z",
     "shell.execute_reply": "2025-08-26T12:18:53.929295Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01195,
     "end_time": "2025-08-26T12:18:53.931404",
     "exception": false,
     "start_time": "2025-08-26T12:18:53.919454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/check_parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/check_parse.py\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from helpers import *\n",
    "\n",
    "l=get_logger()\n",
    "\n",
    "def gt_dataset_id_normalization(name:str) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(is_doi_link(name))\n",
    "        .then(pl.col(name).str.split(DOI_LINK).list.last())\n",
    "        .otherwise(name)\n",
    "        .str.to_lowercase()\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    if IS_KAGGLE_SUBMISSION:\n",
    "        l.debug('skipping check_parse for submission')\n",
    "        return\n",
    "    df = (\n",
    "        get_df('/tmp/train_parse')\n",
    "        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n",
    "    )\n",
    "\n",
    "    gt = (\n",
    "        pl.read_csv(COMP_DIR/'train_labels.csv')\n",
    "        .filter(pl.col('article_id').is_in(df['article_id']))\n",
    "        .filter(pl.col('type')!='Missing')\n",
    "        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n",
    "    )\n",
    "\n",
    "    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b16c76",
   "metadata": {
    "_cell_guid": "af0b3ba0-38a0-49e6-9d58-2c7ba1618484",
    "_uuid": "8eea8c4d-a332-4622-817c-b4949f6d8cac",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-26T12:18:53.943422Z",
     "iopub.status.busy": "2025-08-26T12:18:53.943230Z",
     "iopub.status.idle": "2025-08-26T12:18:53.951001Z",
     "shell.execute_reply": "2025-08-26T12:18:53.950445Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015231,
     "end_time": "2025-08-26T12:18:53.952166",
     "exception": false,
     "start_time": "2025-08-26T12:18:53.936935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/getid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/getid.py\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "COMPILED_PATTERNS = {\n",
    "    'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],    \n",
    "    'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n",
    "    'first_citation_patterns': [\n",
    "        re.compile(r'^\\s*\\[1\\]\\s*'),\n",
    "        re.compile(r'^\\s*\\(1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1\\.\\s*'),\n",
    "        re.compile(r'^\\s*1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1(?=\\s|$)'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n",
    "    last_match_idx = None\n",
    "    for pattern in header_patterns:\n",
    "        matches = list(pattern.finditer(text))\n",
    "        if matches:\n",
    "            last_match_idx = matches[-1].start()\n",
    "    return last_match_idx\n",
    "\n",
    "def find_last_first_citation(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_match_line = None\n",
    "    for line_num, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n",
    "            if pattern.match(line):\n",
    "                next_lines = lines[line_num:line_num+3]\n",
    "                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n",
    "                    last_match_line = line_num\n",
    "                break\n",
    "    return last_match_line\n",
    "\n",
    "def find_reference_start(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_first_citation = find_last_first_citation(text)\n",
    "    if last_first_citation is not None:\n",
    "        return last_first_citation\n",
    "    start_search_idx = int(len(lines) * 0.5)\n",
    "    for i in range(start_search_idx, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if COMPILED_PATTERNS['citation_pattern'].match(line):\n",
    "            next_lines = lines[i:i+3]\n",
    "            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n",
    "                for j in range(i, max(-1, i-10), -1):\n",
    "                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()):\n",
    "                        return j + 1\n",
    "                return max(0, i-10)\n",
    "    return None\n",
    "\n",
    "def split_text_and_references(text: str) -> Tuple[str, str]:\n",
    "    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n",
    "    if header_idx is not None:\n",
    "        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "        if header_idx2 is not None:\n",
    "            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "            if header_idx3 is not None:\n",
    "                return text[:header_idx3].strip(), text[header_idx3:].strip()\n",
    "            return text[:header_idx2].strip(), text[header_idx2:].strip()\n",
    "        return text[:header_idx].strip(), text[header_idx:].strip()\n",
    "    ref_start_line = find_reference_start(text)\n",
    "    if ref_start_line is not None:\n",
    "        lines = text.splitlines()\n",
    "        body = '\\n'.join(lines[:ref_start_line])\n",
    "        refs = '\\n'.join(lines[ref_start_line:])\n",
    "        return body.strip(), refs.strip()\n",
    "    return text.strip(), ''\n",
    "\n",
    "def get_splits(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    bodies, refs = [], []\n",
    "    for raw_text in df['text']:\n",
    "        main, ref = split_text_and_references(raw_text)\n",
    "        bodies.append(main)\n",
    "        refs.append(ref)\n",
    "    return df.with_columns(pl.Series('body', bodies), pl.Series('ref', refs))\n",
    "\n",
    "def tidy_extraction(df) -> pl.DataFrame:\n",
    "    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n",
    "\n",
    "    doi_df = (\n",
    "        df.with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match'))\n",
    "          .explode('match')\n",
    "          .drop_nulls('match')\n",
    "          .with_columns(\n",
    "              pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                             .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                             .str.to_lowercase()\n",
    "                             .alias('dataset_id')\n",
    "          )\n",
    "          .group_by('article_id', 'dataset_id')\n",
    "          .agg('match')\n",
    "          .with_columns((DOI_LINK + pl.col('dataset_id')).alias('dataset_id'))\n",
    "    )\n",
    "\n",
    "    # REGEX_IDS = (\n",
    "    #     r\"(?i)\\b(?:\"\n",
    "    #     r\"CHEMBL\\d+|\"\n",
    "    #     r\"E-GEOD-\\d+|E-PROT-\\d+|E-MTAB-\\d+|E-MEXP-\\d+|EMPIAR-\\d+|\"\n",
    "    #     r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n",
    "    #     r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n",
    "    #     r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|BX\\d{6}|KX\\d{6}|K0\\d{4}|CAB\\d{6}|\"\n",
    "    #     r\"NC_\\d{6}\\.\\d{1}|NM_\\d{9}|\"\n",
    "    #     r\"PRJNA\\d+|PRJEB\\d+|PRJDB\\d+|PXD\\d+|SAMN\\d+|\"\n",
    "    #     r\"GSE\\d+|GSM\\d+|GPL\\d+|\"\n",
    "    #     r\"PDB\\s?[1-9][A-Z0-9]{3}|HMDB\\d+|\"\n",
    "    #     r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n",
    "    #     r\"(?:SR[PRX]|STH|ERR|DRR|DRX|DRP|ERP|ERX)\\d+\"\n",
    "    #     r\")\"\n",
    "    # )  \n",
    "\n",
    "    REGEX_IDS = (\n",
    "        r\"(?i)\\b(?:\"\n",
    "        r\"CHEMBL\\d+|\"\n",
    "        r\"E-GEOD-\\d+|E-PROT-\\d+|E-MTAB-\\d+|E-MEXP-\\d+|EMPIAR-\\d+|\"\n",
    "        r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n",
    "        r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n",
    "        r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|BX\\d{6}|KX\\d{6}|K0\\d{4}|CAB\\d{6}|\"\n",
    "        r\"NC_\\d{6}\\.\\d{1}|NM_\\d{9}|\"\n",
    "        r\"PRJNA\\d+|PRJEB\\d+|PRJDB\\d+|PXD\\d+|SAMN\\d+|\"\n",
    "        r\"GSE\\d+|GSM\\d+|\"\n",
    "        r\"PDB\\s?[1-9][A-Z0-9]{3}|HMDB\\d+|\"\n",
    "        r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n",
    "        r\"(?:SR[RPAX]|STH|ERR|DRR|DRX|DRP|ERP|ERX)\\d+|\"\n",
    "        r\"phs\\d{6}(?:\\.v\\d{1,2}\\.p\\d{1,2})?|\"\n",
    "        r\"CVCL_[A-Z0-9]{4}\"\n",
    "        r\")\"\n",
    "    )\n",
    "    \n",
    "    acc_df = (\n",
    "        df.with_columns(\n",
    "            pl.col('text').str.extract_all(REGEX_IDS).alias('match')\n",
    "        )\n",
    "        .explode('match')\n",
    "        .drop_nulls('match')\n",
    "        .with_columns(\n",
    "            pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                           .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                           .str.replace(r'(?i)^PDB', '')\n",
    "                           .alias('dataset_id')\n",
    "        )\n",
    "        .group_by('article_id', 'dataset_id')\n",
    "        .agg('match')\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('dryad.'))\n",
    "              .then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('pasta/'))\n",
    "              .then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = pl.concat([doi_df, acc_df])\n",
    "\n",
    "    df = (\n",
    "        df.unique(['article_id', 'dataset_id'])  # CHANGED\n",
    "          .filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains('figshare', literal=True))\n",
    "          .filter(~pl.col('dataset_id').is_in(bad_ids))\n",
    "          .filter(\n",
    "              pl.when(is_doi_link('dataset_id') &\n",
    "                      (pl.col('dataset_id').str.split('/').list.last().str.len_chars() < 5))\n",
    "               .then(False)\n",
    "               .otherwise(True)\n",
    "          )\n",
    "          .with_columns(pl.col('match').list.unique())\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 100) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        raise ValueError\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end]\n",
    "\n",
    "def get_window_df(text_df, ids_df):\n",
    "    df = ids_df.join(text_df, on='article_id')\n",
    "    windows = []\n",
    "    for text, match_ids in df.select('text', 'match').rows():\n",
    "        windows.append(get_context_window(text, match_ids[0]))\n",
    "    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n",
    "\n",
    "def main():\n",
    "    text_df = get_df('/tmp/train_parse')\n",
    "    df = get_splits(text_df)\n",
    "    df = tidy_extraction(df)\n",
    "    df = get_window_df(text_df, df)\n",
    "    df.write_parquet('/tmp/extracted.parquet')\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r)\n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f70040",
   "metadata": {
    "_cell_guid": "378df5d8-fad7-4982-9948-ab64de94454d",
    "_uuid": "bdaa185a-0977-417c-b783-32b1b556214a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-26T12:18:53.965464Z",
     "iopub.status.busy": "2025-08-26T12:18:53.965290Z",
     "iopub.status.idle": "2025-08-26T12:18:53.972591Z",
     "shell.execute_reply": "2025-08-26T12:18:53.971880Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014958,
     "end_time": "2025-08-26T12:18:53.973593",
     "exception": false,
     "start_time": "2025-08-26T12:18:53.958635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/llm_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/llm_validate.py\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n",
    "1.1 Rules supporting output A (Primary):\n",
    "If DOI prefix in the text matches a known data repository:\n",
    "    Dryad: 10.5061\n",
    "    Zenodo: 10.5281\n",
    "    Figshare: 10.6084\n",
    "    Mendeley Data: 10.24433\n",
    "    Mendeley Data: 10.17632\n",
    "    Dataverse: 10.7910/DVN\n",
    "    OpenNeuro: 10.18112/openneuro\n",
    "    PANGAEA: 10.1594/PANGAEA\n",
    "    Neotoma Paleoecology: 10.21233\n",
    "    ICPSR: 10.3886\n",
    "    NOAA NCEI: 10.7289\n",
    "    UK Data Service: 10.5255\n",
    "    EMPIAR: 10.6019\n",
    "    Non-DOI dataset accession prefixes:\n",
    "    NCBI SRA / ENA: SRP, SRA, ERP, ERX\n",
    "    BioProject: PRJNA, PRJEB, PRJDB\n",
    "    ProteomeXchange / PRIDE: PXD\n",
    "    ArrayExpress / EMBL-EBI: E-MTAB, E-  (context needed)\n",
    "    MetaboLights: MTBLS\n",
    "    GEO Series: GSE\n",
    "    GenBank: MN, NC_, CP, MT  (context needed)\n",
    "    EMDB: EMD-  (context needed)\n",
    "    EMPIAR: EMPIAR-  (context needed)\n",
    "    \n",
    "If the text explicitly uses phrases like \"we generated\", \"we created\", \"we collected and processed\", or \"our team developed\" to describe the dataset/database, directly attributing its creation to the authors of the paper.\n",
    "If the text states that the dataset/database was \"specifically generated for this study\" or \"produced as part of the current research\".\n",
    "If the text details the authors’ direct involvement in data generation/processing (e.g., describing their own data collection methods, experimental procedures to generate raw data, or unique processing steps tailored for the study).\n",
    "If the text indicates that the dataset/database \"did not exist prior to this research\" and was created to address the study’s objectives.\n",
    "If the text refers to the dataset as \"our study’s dataset\", \"the database developed in this work\", or similar phrases that explicitly link it to the current paper’s original efforts.\n",
    "If the text specifies that the data is \"raw data collected by our team\" or \"processed data derived from our own experiments\" without referencing external sources.\n",
    "If the text mentions that the dataset/database is \"made publicly available for the first time through this paper\" and is identified as the authors’ own creation.\n",
    "If the text describes the dataset’s structure, variables, or parameters as \"designed and implemented by our research group\" for the study.\n",
    "If the text includes statements like \"we conducted surveys/experiments to gather data for this dataset\" or \"our fieldwork generated the raw data in this database\".\n",
    "If the text claims the dataset/database is \"an original contribution of this study\" or \"a key output of our research\".\n",
    "If the text notes that the dataset is \"expanded or refined from the authors’ previously unpublished data\" (not derived from external sources).\n",
    "\n",
    "1.2 Rules supporting output B (Secondary):\n",
    "If the text explicitly cites a reference when mentioning the dataset/database (e.g., \"using the dataset from [Author et al., Year]\" or \"as reported in [Reference X], the database...\").\n",
    "If the text uses phrases like \"we reused the existing dataset\", \"the database was obtained from prior studies\", or \"we adopted a published dataset\".\n",
    "If the dataset/database is referred to by a name widely recognized as existing in the field (e.g., \"MNIST\", \"PubMed Central\") without the authors claiming creation.\n",
    "If the text states that the dataset/database was \"retrieved from [external source]\", \"downloaded from [public repository]\", or \"extracted from [existing records]\".\n",
    "If the text identifies the creator/owner of the dataset/database as a third party (e.g., \"the database was developed by [Institution/Author] in 20XX\").\n",
    "If the text only describes applying the dataset/database in the study (e.g., \"we analyzed data from [Dataset Name]\") without any mention of creating or generating it.\n",
    "If the text indicates the dataset/database \"has been used in previous studies\" or \"is a well-established resource in the field\".\n",
    "If the text provides a link, DOI, or access path to the dataset/database that points to an external platform (not the paper’s supplementary materials or the authors’ institutional repository for newly created data).\n",
    "If the text notes that the dataset/database was \"first published in [Reference Y]\" or \"originally described in [earlier work]\".\n",
    "If the text refers to the dataset as \"secondary data\" or \"publicly available data\" without claiming original generation.\n",
    "If the text mentions the dataset/database was \"modified from an existing source\" (even with adjustments, the core data is derived from external records).\n",
    "If the text states that the data was \"obtained through collaboration with [external organization]\" where the data pre-existed.\n",
    "If the text describes the dataset as \"a benchmark dataset widely used in the field\" (implying prior existence).\n",
    "\n",
    "2. Output\n",
    "Only output:\n",
    "\n",
    "A → data repository / dataset\n",
    "\n",
    "B → literature / non-data resource\n",
    "\n",
    "\n",
    "Few-shot examples\n",
    "\n",
    "“Raw images are stored on Figshare (DOI 10.6084/m9.figshare.1234567).” → A\n",
    "\n",
    "“Sequence reads available under BioProject accession PRJNA765432.” → A\n",
    "\n",
    "“As described in Nature Methods (DOI 10.1038/s41592-020-0793-2).” → B\n",
    "\n",
    "“See Supplementary Data at Zenodo (10.5281/zenodo.987654).” → A\n",
    "\n",
    "“Method details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.” → B\n",
    "\n",
    "“Data uploaded to Dryad (10.5061/dryad.x1y2z3).” → A\n",
    "\n",
    "“Referenced paper: DOI 10.1101/2020.01.01.123456 (bioRxiv preprint).” → B\n",
    "\n",
    "“Metabolomics data in MetaboLights MTBLS1234.” → A\n",
    "\n",
    "“The MRI scans are deposited at OpenNeuro (DOI 10.18112/openneuro.ds000001.v1.0.0).” → A\n",
    "\n",
    "“Protein structure described in Science (DOI 10.1126/science.abc1234).” → B\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_df():\n",
    "    df = pl.read_parquet('/tmp/extracted.parquet')\n",
    "    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n",
    "    return df.filter(is_doi_link('dataset_id'))\n",
    "\n",
    "def build_prompt(tokenizer, df):\n",
    "    prompts = []\n",
    "    for doi, text in df.select('dataset_id', 'window').rows():\n",
    "        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI}, {'role':'user', 'content': text}]\n",
    "        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False))\n",
    "    return df.with_columns(pl.Series('prompt', prompts))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    llm = vllm.LLM(model_path, quantization='awq', tensor_parallel_size=2, gpu_memory_utilization=0.9, trust_remote_code=True, dtype=\"half\", enforce_eager=True, max_model_len=2048, disable_log_stats=True, disable_custom_all_reduce=True, enable_prefix_caching=True, task='generate')\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    df = build_df()\n",
    "    df = build_prompt(tokenizer, df)\n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b68cd5e",
   "metadata": {
    "_cell_guid": "27022685-0893-460a-8ea4-5c62fd964ae8",
    "_uuid": "24048569-bb55-4720-8252-1bfdeca91805",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-26T12:18:53.985970Z",
     "iopub.status.busy": "2025-08-26T12:18:53.985768Z",
     "iopub.status.idle": "2025-08-26T12:18:53.990664Z",
     "shell.execute_reply": "2025-08-26T12:18:53.989976Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012471,
     "end_time": "2025-08-26T12:18:53.991769",
     "exception": false,
     "start_time": "2025-08-26T12:18:53.979298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/post_filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/post_filter.py\n",
    "import polars as pl\n",
    "from helpers import *\n",
    "\n",
    "\"\"\"\n",
    "Fourth essence: Post-filter to cut FP DOIs that look like literature.\n",
    "- Read /kaggle/working/submission.csv (output of llm_validate.py)\n",
    "- Join with /tmp/extracted.parquet to get context window\n",
    "- Drop DOI rows that (1) start with typical publisher prefixes AND (2) have no data-ish words nearby\n",
    "- Keep accessions untouched\n",
    "\"\"\"\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "PAPER_PREFIXES = [\n",
    "    \"10.1038\",\"10.1007\",\"10.1126\",\"10.1016\",\"10.1101\",\"10.1021\",\"10.1145\",\"10.1177\",\n",
    "    \"10.1093\",\"10.1080\",\"10.1111\",\"10.1098\",\"10.1103\",\"10.1186\",\"10.1371\",\"10.7554\",\n",
    "    \"10.1039\",\"10.1002\",\"10.3390\",\"10.1073\",\"10.1097\",\"10.15252\",\"10.1136\",\"10.1091\",\n",
    "    \"10.1523\", \"10.1152\", \"10.1128\", \"10.1155\", \"10.1242\", \"10.1182\", \"10.1012\"\n",
    "]\n",
    "\n",
    "CONTEXT_RE = r\"(?i)\\b(data(?: ?set)?|database|repository|archive|deposited|available|supplementary|raw(?:\\s+data)?|uploaded|hosted|stored|accession(?: number| code)?|files|retrieved from|novel)\\b\"\n",
    "\n",
    "\n",
    "def remove_extra_digit(df: pl.DataFrame, column: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows where the value in `column` is just the same DOI with one extra digit at the end.\n",
    "    Keeps all other columns.\n",
    "    \"\"\"\n",
    "    items_set = set(df[column].to_list())\n",
    "\n",
    "    def keep_row(value):\n",
    "        if (value[-1].isdigit() and value[:-1] in items_set) or \\\n",
    "           (len(value) > 2 and value[-2:].isdigit() and value[:-2] in items_set):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    return df.filter(pl.col(column).map_elements(keep_row, return_dtype=pl.Boolean))\n",
    "\n",
    "\n",
    "def is_paper_prefix(col: str = \"dataset_id\") -> pl.Expr:\n",
    "    expr = pl.lit(False)\n",
    "    for p in PAPER_PREFIXES:\n",
    "        expr = expr | pl.col(col).str.starts_with(f\"{DOI_LINK}{p}\")\n",
    "    return expr\n",
    "\n",
    "def main():\n",
    "    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "    # Normalize columns: drop row_id if present so concat widths match\n",
    "    if \"row_id\" in sub.columns:\n",
    "        sub = sub.drop(\"row_id\")\n",
    "\n",
    "    # Context windows\n",
    "    win = pl.read_parquet(\"/tmp/extracted.parquet\").select(\"article_id\", \"dataset_id\", \"window\")\n",
    "\n",
    "    # DOI & ACC split\n",
    "    doi_rows = sub.filter(is_doi_link(\"dataset_id\")).join(win, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "    acc_rows = sub.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "    keep_mask = (\n",
    "        (~is_paper_prefix(\"dataset_id\"))  # not a known paper prefix\n",
    "        | doi_rows[\"window\"].fill_null(\"\").str.contains(CONTEXT_RE)\n",
    "    )\n",
    "\n",
    "    kept_doi = doi_rows.filter(keep_mask).select(\"article_id\", \"dataset_id\", \"type\")\n",
    "    ## Remove extra digits\n",
    "    doi_df = remove_extra_digit(kept_doi, \"dataset_id\")\n",
    "    final = pl.concat([doi_df, acc_rows.select(\"article_id\", \"dataset_id\", \"type\")])\n",
    "\n",
    "    # Re-eval & save\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        for r in evaluate(final): l.info(r)\n",
    "        for r in evaluate(final, on=[\"article_id\", \"dataset_id\", \"type\"]): l.info(r)\n",
    "\n",
    "    final.with_row_index(\"row_id\").write_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1d9c532",
   "metadata": {
    "_cell_guid": "819a5328-e369-4aec-aa6d-a1090924dc14",
    "_kg_hide-output": true,
    "_uuid": "d453e324-769e-4364-883e-36dc9f68cab8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-26T12:18:54.003861Z",
     "iopub.status.busy": "2025-08-26T12:18:54.003656Z",
     "iopub.status.idle": "2025-08-26T12:26:17.374440Z",
     "shell.execute_reply": "2025-08-26T12:26:17.373481Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 443.378013,
     "end_time": "2025-08-26T12:26:17.375684",
     "exception": false,
     "start_time": "2025-08-26T12:18:53.997671",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "INFO 2025-08-26 12:20:17  [check_parse.py:31 - main()] pymupdf misses: 42 dataset_ids\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:208 - main()] all - f1: 0.5508 [507/615/212]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:208 - main()] doi - f1: 0.4344 [164/266/161]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:208 - main()] acc - f1: 0.6317 [343/349/51]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:210 - main()] all - f1: 0.4606 [424/698/295]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:210 - main()] doi - f1: 0.3338 [126/304/199]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:210 - main()] acc - f1: 0.5488 [298/394/96]\r\n",
      "INFO 08-26 12:20:37 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "WARNING 08-26 12:20:54 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 08-26 12:20:54 [config.py:1770] Defaulting to use mp for distributed inference\r\n",
      "WARNING 08-26 12:20:54 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 08-26 12:20:54 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \r\n",
      "WARNING 08-26 12:20:54 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:20:54 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\r\n",
      "INFO 08-26 12:20:54 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 08-26 12:20:54 [cuda.py:289] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:20:54 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:20:54 [cuda.py:289] Using XFormers backend.\r\n",
      "[W826 12:21:05.763693185 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W826 12:21:06.105853710 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W826 12:21:15.773261401 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W826 12:21:25.783761224 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:21:26 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "INFO 08-26 12:21:26 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:21:26 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 08-26 12:21:26 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 08-26 12:21:26 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_b6a223a3'), local_subscribe_addr='ipc:///tmp/e61bdf59-992d-4a2a-b6e4-ddbba409c58a', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "INFO 08-26 12:21:26 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:21:26 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\r\n",
      "INFO 08-26 12:21:26 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:21:26 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:22<01:31, 22.87s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:50<01:16, 25.65s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:25<00:59, 29.86s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:58<00:31, 31.15s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:32<00:00, 32.29s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:32<00:00, 30.56s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:23:59 [loader.py:458] Loading weights took 152.87 seconds\r\n",
      "INFO 08-26 12:23:59 [loader.py:458] Loading weights took 153.13 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:23:59 [model_runner.py:1140] Model loading took 9.0935 GiB and 153.240191 seconds\r\n",
      "INFO 08-26 12:24:00 [model_runner.py:1140] Model loading took 9.0935 GiB and 153.491232 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=152)\u001b[0;0m INFO 08-26 12:24:11 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 3.63GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 08-26 12:24:11 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 2.66GiB.\r\n",
      "INFO 08-26 12:24:11 [executor_base.py:112] # cuda blocks: 1363, # CPU blocks: 2048\r\n",
      "INFO 08-26 12:24:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 10.65x\r\n",
      "INFO 08-26 12:24:15 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 15.21 seconds\r\n",
      "Processed prompts: 100%|█| 430/430 [01:52<00:00,  3.82it/s, est. speed input: 60\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:128 - <module>()] all - f1: 0.5775 [447/382/272]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:128 - <module>()] doi - f1: 0.4502 [104/33/221]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:128 - <module>()] acc - f1: 0.6317 [343/349/51]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:130 - <module>()] all - f1: 0.5090 [394/435/325]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:130 - <module>()] doi - f1: 0.4156 [96/41/229]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:130 - <module>()] acc - f1: 0.5488 [298/394/96]\r\n",
      "INFO 08-26 12:26:11 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\r\n",
      "[rank0]:[W826 12:26:12.320604296 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Exception ignored in: <Finalize object, dead>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 206, in _finalize_close\r\n",
      "TypeError: 'NoneType' object is not callable\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:72 - main()] all - f1: 0.5798 [447/376/272]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:72 - main()] doi - f1: 0.4561 [104/27/221]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:72 - main()] acc - f1: 0.6317 [343/349/51]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:73 - main()] all - f1: 0.5110 [394/429/325]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:73 - main()] doi - f1: 0.4211 [96/35/229]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:73 - main()] acc - f1: 0.5488 [298/394/96]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:208 - main()] all - f1: 0.5508 [507/615/212]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:208 - main()] doi - f1: 0.4344 [164/266/161]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:208 - main()] acc - f1: 0.6317 [343/349/51]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:210 - main()] all - f1: 0.4606 [424/698/295]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:210 - main()] doi - f1: 0.3338 [126/304/199]\r\n",
      "INFO 2025-08-26 12:20:22  [getid.py:210 - main()] acc - f1: 0.5488 [298/394/96]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:128 - <module>()] all - f1: 0.5775 [447/382/272]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:128 - <module>()] doi - f1: 0.4502 [104/33/221]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:128 - <module>()] acc - f1: 0.6317 [343/349/51]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:130 - <module>()] all - f1: 0.5090 [394/435/325]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:130 - <module>()] doi - f1: 0.4156 [96/41/229]\r\n",
      "INFO 2025-08-26 12:26:10  [llm_validate.py:130 - <module>()] acc - f1: 0.5488 [298/394/96]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:72 - main()] all - f1: 0.5798 [447/376/272]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:72 - main()] doi - f1: 0.4561 [104/27/221]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:72 - main()] acc - f1: 0.6317 [343/349/51]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:73 - main()] all - f1: 0.5110 [394/429/325]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:73 - main()] doi - f1: 0.4211 [96/35/229]\r\n",
      "INFO 2025-08-26 12:26:17  [post_filter.py:73 - main()] acc - f1: 0.5488 [298/394/96]\r\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp\n",
    "!LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n",
    "! python src/check_parse.py\n",
    "! python src/getid.py\n",
    "! python src/llm_validate.py\n",
    "! python src/post_filter.py\n",
    "! grep \"f1:\" /tmp/logs/project.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fb07d",
   "metadata": {
    "_cell_guid": "104fd444-60c0-437d-9d83-8f169b59ecab",
    "_uuid": "3f69cbb6-58cf-4c41-b511-d7bbcad754d3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007689,
     "end_time": "2025-08-26T12:26:17.392154",
     "exception": false,
     "start_time": "2025-08-26T12:26:17.384465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "sourceId": 248118764,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141565,
     "sourceId": 166368,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 463.782102,
   "end_time": "2025-08-26T12:26:17.717751",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-26T12:18:33.935649",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
